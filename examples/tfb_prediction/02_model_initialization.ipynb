{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b9c718",
   "metadata": {},
   "source": [
    "## \u001f TFB Prediction Tutorial 2/4: Model Initialization - From Task to Architecture\n",
    "\n",
    "In the previous tutorial, [01_data_preparation.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/01_data_preparation.ipynb), we defined our biological task, predicting transcription factor binding sites, and prepared our data accordingly. We framed it as a **multi-label sequence classification** problem. This crucial step of defining the task and data format dictates our next decision: **choosing the right model architecture**.\n",
    "\n",
    "\n",
    "Welcome to the second part of our **streamlined** tutorial series. In the previous tutorial, we prepared our data with just 3 lines of code using the enhanced OmniDataset framework. Now, we will focus on **Model Initialization** with equal simplicity.\n",
    "\n",
    "> \u001f Learning Objectives: Master the universal model initialization pattern, understand foundation model concepts, and leverage OmniGenBench's intelligent defaults\n",
    "\n",
    "---\n",
    "\n",
    "## The Power of Pre-trained Models \u001f\n",
    "\n",
    "The core idea behind fine-tuning is to leverage **pre-trained foundation models**. These models have already learned the fundamental \\\"language\\\" of the genome from vast amounts of unlabeled sequence data. This pre-training endows them with powerful, general-purpose understanding of genomic patterns.\n",
    "\n",
    "Our task is to take this general knowledge and specialize it for our specific problem: translation efficiency prediction. With the enhanced OmniGenBench framework, this process is now **effortless**.\n",
    "\n",
    "### \u001f Why Use Foundation Models Instead of Traditional Methods?\n",
    "\n",
    "| Traditional Methods | Foundation Model Approach |\n",
    "|---------|------------|\n",
    "| \u001f Requires hand-crafted features | \u001f Automatically learns feature representations |\n",
    "| \u001f Relies on prior biological knowledge | \u001f Discovers patterns from data |\n",
    "| \u001f Limited generalization ability | \u001f Strong cross-task generalization |\n",
    "| \u001f Needs large amounts of task-specific data | \u001f Can fine-tune with small amounts of data |\n",
    "\n",
    "\n",
    "## Key Components: PlantRNA-FM Model and Tokenizer \n",
    "\n",
    "The enhanced OmniGenBench framework has revolutionized model initialization. What once required complex configuration is now **automatic**, especially when using PlantRNA-FM for plant genomics tasks.\n",
    "\n",
    "This tutorial will guide you through the process of selecting and initializing PlantRNA-FM from the `OmniGenBench` framework. We will cover:\n",
    "\n",
    "1.  **The OmniGenBench Model Zoo**: An overview of available model architectures, with focus on PlantRNA-FM for plant genomics.\n",
    "\n",
    "2.  **The Principle of Model Selection**: How to choose PlantRNA-FM and configure it for plant transcription factor binding prediction.\n",
    "\n",
    "3.  **Model Architecture**: Understanding the \"PlantRNA-FM base + task head\" design.\n",
    "\n",
    "4.  **Inputs and Outputs**: What PlantRNA-FM expects as input and produces as output for plant regulatory analysis.\n",
    "\n",
    "5.  **Practical Implementation**: Initializing PlantRNA-FM for our plant TFB prediction task.\n",
    "\n",
    "By the end of this tutorial, you will understand how to leverage PlantRNA-FM for plant regulatory genomics problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30995b4d",
   "metadata": {},
   "source": [
    "### 1. The OmniGenBench Model Zoo: Plant-Specialized Models\n",
    "\n",
    "`OmniGenBench` provides a comprehensive framework with various model architectures. For plant genomics, **PlantRNA-FM** serves as the foundation, tailored for plant-specific tasks. These are often referred to as \"task heads.\" When you use PlantRNA-FM, you combine a powerful, plant-specific **base model** with a smaller, task-specific **head**.\n",
    "\n",
    "Here is a summary of the main model classes available in `OmniGenBench` and their plant genomics applications:\n",
    "\n",
    "| Model Class                                       | Task Type                    | Plant Genomics Example                                       |\n",
    "| ------------------------------------------------- | ---------------------------- | -------------------------------------------------------- |\n",
    "| `OmniModelForSequenceClassification`              | Sequence Classification      | Classifying plant promoters as active/inactive, tissue-specific expression   |\n",
    "| `OmniModelForMultiLabelSequenceClassification`    | Multi-Label Classification   | Predicting multiple TFB sites in plant regulatory regions (our task)       |\n",
    "| `OmniModelForTokenClassification`                 | Token Classification         | Identifying splice sites in plant genes, m6A modification sites |\n",
    "| `OmniModelForSequenceRegression`                  | Sequence Regression          | Predicting plant mRNA translation efficiency scores |\n",
    "| `OmniModelForTokenRegression`                     | Token Regression             | Predicting per-base chromatin accessibility in plants     |\n",
    "| `OmniModelForSeq2Seq`                             | Sequence-to-Sequence         | RNA secondary structure prediction in plants        |\n",
    "| `OmniModelForRNADesign`                           | Sequence Generation          | Designing synthetic plant regulatory elements |\n",
    "| `OmniModelForMLM` (Masked Language Model)         | Self-Supervised Pre-training | Learning representations from unlabeled plant DNA/RNA     |\n",
    "\n",
    "**PlantRNA-FM** (published in *Nature Machine Intelligence*, 35M parameters) is particularly powerful for plant genomics because it was pre-trained on extensive plant transcriptome and genome data, making it excel at tasks involving plant regulatory elements, codon usage patterns, and RNA structures‚Äîall while being remarkably efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2c986",
   "metadata": {},
   "source": [
    "### 2. The Principle of Model Selection: PlantRNA-FM for Plant TFB Prediction\n",
    "\n",
    "The selection principle is straightforward: **match the model architecture to the machine learning task you defined, and choose PlantRNA-FM for plant-specific genomics**.\n",
    "\n",
    "In our case:\n",
    "-   **Biological Problem**: Predicting if any of 919 transcription factors bind to a given plant DNA sequence.\n",
    "-   **Data Format**: A DNA sequence of length 1000 from plant genomes.\n",
    "-   **Label Format**: A binary vector of length 919, where each element indicates binding (1) or no binding (0) for a specific TF.\n",
    "-   **ML Task**: Since a single sequence can have multiple TFs binding to it, this is a **Multi-Label Sequence Classification** problem.\n",
    "-   **Model Choice**: `OmniModelForMultiLabelSequenceClassification` with **PlantRNA-FM** base model.\n",
    "\n",
    "**Why PlantRNA-FM for this task?**\n",
    "- Pre-trained on plant transcriptomes and regulatory regions\n",
    "- Understands plant-specific transcription factor binding motifs\n",
    "- Captures plant chromatin structure patterns\n",
    "- Generalizes well across plant species (Arabidopsis, rice, maize, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd77b9",
   "metadata": {},
   "source": [
    "### 3. Model Architecture: PlantRNA-FM Base + Task Head\n",
    "\n",
    "Let's visualize the architecture. At its core, our model consists of two parts:\n",
    "\n",
    "1.  **PlantRNA-FM Base Model**: This is a large, pre-trained transformer model specifically trained on plant genomic and transcriptomic data. Its job is to read a plant DNA sequence (as a series of tokens) and convert it into a rich numerical representation (an embedding) that captures plant-specific regulatory patterns, motifs, and chromatin contexts.\n",
    "2.  **The Multi-Label Classification Head**: This is a smaller neural network (usually one or two linear layers) that sits on top of PlantRNA-FM. It takes the plant sequence embedding and transforms it into 919 independent binary predictions for transcription factor binding.\n",
    "\n",
    "Here is a diagram illustrating this plant-specific architecture:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Input\"\n",
    "        A[DNA Sequence<br/>\"GATTACA...\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"Tokenization\"\n",
    "        B[Input Tokens<br/>\"[CLS], G, A, T, T, A, A, [SEP]\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"OmniModelForMultiLabelSequenceClassification\"\n",
    "        C(Base Model<br/>OmniGenome-186M)\n",
    "        D(Classification Head<br/>Linear Layer + Sigmoid)\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output\"\n",
    "        E[Prediction Vector<br/>\"[0.9, 0.1, 0.05, ..., 0.8]\"]\n",
    "    end\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "    C -- Sequence Embedding --> D\n",
    "    D --> E\n",
    "```\n",
    "\n",
    "The base model does the heavy lifting of understanding the sequence, while the head adapts that understanding to our specific predictive goal. During fine-tuning, we will update the weights of both the head and (to a lesser extent) the base model to optimize for our TFB prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc513e73",
   "metadata": {},
   "source": [
    "### 4. Inputs and Outputs: A Look at the Data Flow\n",
    "\n",
    "Understanding what the model expects and what it returns is critical for debugging and interpretation.\n",
    "\n",
    "-   **Model Input**: The model expects a batch of tokenized sequences. When you use the `AccelerateTrainer` (which we will see in the next tutorial), the data loader will automatically prepare a dictionary-like object for each batch. This object contains:\n",
    "    -   `input_ids`: A tensor of token IDs representing the DNA sequences.\n",
    "    -   `attention_mask`: A tensor indicating which tokens are real and which are padding.\n",
    "    -   `labels`: A tensor containing the ground-truth labels for each sequence.\n",
    "\n",
    "-   **Model Output**: During training, the model returns a dictionary containing:\n",
    "    -   `loss`: The calculated loss value, which the trainer uses to update the model weights.\n",
    "    -   `logits`: The raw, unnormalized output scores from the final linear layer. For our task, this will be a tensor of shape `(batch_size, 919)`.\n",
    "\n",
    "During inference (prediction), the model simply returns the `logits`. To get probabilities, we typically apply a Sigmoid function to the logits. To get binary predictions, we can then threshold these probabilities (e.g., predict 1 if probability > 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e58fd0",
   "metadata": {},
   "source": [
    "### 5. Practical Implementation: Initializing the Model\n",
    "\n",
    "Now, let's translate this theory into code. We will perform the following steps:\n",
    "1.  Import necessary libraries.\n",
    "2.  Define a configuration object to hold all our parameters.\n",
    "3.  Write a function to initialize the tokenizer and the `OmniModelForMultiLabelSequenceClassification` model.\n",
    "\n",
    "First, let's set up our environment by importing the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bee4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer, \n",
    "    OmniModelForMultiLabelSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c976d1",
   "metadata": {},
   "source": [
    "Next, we define a configuration dictionary. This is a best practice that centralizes all important parameters, making the code cleaner and easier to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - matches complete tutorial exactly\n",
    "config = {\n",
    "    \"model_name_or_path\": \"yangheng/OmniGenome-52M\",\n",
    "    \"num_labels\": 919,\n",
    "    \"max_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"  üß¨ Model: {config['model_name_or_path']}\")\n",
    "print(f\"  üè∑Ô∏è Labels: {config['num_labels']} TF binding sites\")\n",
    "print(f\"  üìè Max length: {config['max_length']} tokens\")\n",
    "print(f\"  üì± Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497f225",
   "metadata": {},
   "source": [
    "Finally, we create the function that loads our model. This function encapsulates the logic for:\n",
    "-   Loading the pre-trained tokenizer.\n",
    "-   Loading the pre-trained base model.\n",
    "-   Initializing our chosen task-specific model, `OmniModelForMultiLabelSequenceClassification`, which wraps the base model and adds the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer - matches complete tutorial\n",
    "print(\"üîÑ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config[\"model_name_or_path\"])\n",
    "print(f\"‚úÖ Tokenizer loaded: {config['model_name_or_path']}\")\n",
    "\n",
    "# Load model - matches complete tutorial exactly\n",
    "print(\"üîÑ Loading model...\")\n",
    "model = OmniModelForMultiLabelSequenceClassification(\n",
    "    config[\"model_name_or_path\"],\n",
    "    tokenizer,\n",
    "    num_labels=config[\"num_labels\"],\n",
    ")\n",
    "\n",
    "# Move model to the specified device\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully.\")\n",
    "print(f\"üéØ Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Show model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "print(f\"üß¨ Ready for {config['num_labels']}-label TF binding prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae5fa",
   "metadata": {},
   "source": [
    "### Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully initialized a powerful Genomic Foundation Model tailored for your specific biological task.\n",
    "\n",
    "To recap, we have:\n",
    "-   Surveyed the different model architectures available in `OmniGenBench`.\n",
    "-   Learned the principle of matching the model architecture to the ML task.\n",
    "-   Understood the \"base model + task head\" design.\n",
    "-   Initialized the `OmniModelForMultiLabelSequenceClassification` model and its tokenizer.\n",
    "\n",
    "We now have the two key components ready: a prepared dataset and an initialized model. The next logical step is to bring them together and train the model to make accurate predictions.\n",
    "\n",
    "In the next tutorial, **[3/4: Model Training](./03_model_training.ipynb)**, we will take this model and the data we prepared in the first tutorial and begin the fine-tuning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
