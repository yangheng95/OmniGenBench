{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b9c718",
   "metadata": {},
   "source": [
    "## ü§ñ TFB Prediction Tutorial 2/4: Model Initialization - From Task to Architecture\n",
    "\n",
    "In the previous tutorial, [01_data_preparation.ipynb](./01_data_preparation.ipynb), we defined our biological task, predicting transcription factor binding sites, and prepared our data accordingly. We framed it as a **multi-label sequence classification** problem. This crucial step of defining the task and data format dictates our next decision: **choosing the right model architecture**.\n",
    "\n",
    "\n",
    "Welcome to the second part of our **streamlined** tutorial series. In the previous tutorial, we prepared our data with just 3 lines of code using the enhanced OmniDataset framework. Now, we will focus on **Model Initialization** with equal simplicity.\n",
    "\n",
    "> üí° **Learning Objectives**: Master the universal model initialization pattern, understand foundation model concepts, and leverage OmniGenBench's intelligent defaults\n",
    "\n",
    "---\n",
    "\n",
    "## The Power of Pre-trained Models üöÄ\n",
    "\n",
    "The core idea behind fine-tuning is to leverage **pre-trained foundation models**. These models have already learned the fundamental \"language\" of the genome from vast amounts of unlabeled sequence data. This pre-training endows them with powerful, general-purpose understanding of genomic patterns.\n",
    "\n",
    "Our task is to take this general knowledge and specialize it for our specific problem: translation efficiency prediction. With the enhanced OmniGenBench framework, this process is now **effortless**.\n",
    "\n",
    "### üß† Why Use Foundation Models Instead of Traditional Methods?\n",
    "\n",
    "| Traditional Methods | Foundation Model Approach |\n",
    "|---------|------------|\n",
    "| üî¥ Requires hand-crafted features | üü¢ Automatically learns feature representations |\n",
    "| üî¥ Relies on prior biological knowledge | üü¢ Discovers patterns from data |\n",
    "| üî¥ Limited generalization ability | üü¢ Strong cross-task generalization |\n",
    "| üî¥ Needs large amounts of task-specific data | üü¢ Can fine-tune with small amounts of data |\n",
    "\n",
    "## Key Components: Model and Tokenizer üîß\n",
    "\n",
    "The enhanced OmniGenBench framework has revolutionized model initialization. What once required complex configuration is now **automatic**.\n",
    "\n",
    "\n",
    "This tutorial will guide you through the process of selecting and initializing a model from the `OmniGenBench` framework. We will cover:\n",
    "\n",
    "1.  **The OmniGenBench Model Zoo**: An overview of the various pre-packaged model architectures available in `OmniGenBench`.\n",
    "2.  **The Principle of Model Selection**: How to choose the appropriate model based on the task you defined.\n",
    "3.  **Model Architecture**: Understanding the \"base model + task head\" design.\n",
    "4.  **Inputs and Outputs**: What the model expects as input and what it produces as output.\n",
    "5.  **Practical Implementation**: Initializing a model for our TFB prediction task.\n",
    "\n",
    "By the end of this tutorial, you will understand how to translate a biological problem into a concrete model architecture ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30995b4d",
   "metadata": {},
   "source": [
    "### 1. The OmniGenBench Model Zoo: A Task-Oriented Collection\n",
    "\n",
    "`OmniGenBench` is not just a single model; it's a comprehensive framework that provides a variety of model architectures, each tailored for a specific type of biological task. These are often referred to as \"task heads.\" When you use a foundation model, you are combining a powerful, pre-trained **base model** (which has learned fundamental patterns in genomic data) with a smaller, task-specific **head**.\n",
    "\n",
    "Here is a summary of the main model classes available in `OmniGenBench` and the tasks they are designed for:\n",
    "\n",
    "| Model Class                                       | Task Type                    | Biological Example                                       |\n",
    "| ------------------------------------------------- | ---------------------------- | -------------------------------------------------------- |\n",
    "| `OmniModelForSequenceClassification`              | Sequence Classification      | Classifying a promoter sequence as active or inactive.   |\n",
    "| `OmniModelForMultiLabelSequenceClassification`    | Multi-Label Classification   | Predicting multiple TFB sites in one DNA sequence.       |\n",
    "| `OmniModelForTokenClassification`                 | Token Classification         | Identifying splice sites at specific positions in a gene. |\n",
    "| `OmniModelForSequenceRegression`                  | Sequence Regression          | Predicting the overall translation efficiency of an mRNA. |\n",
    "| `OmniModelForTokenRegression`                     | Token Regression             | Predicting per-base modification status (e.g., m6A).     |\n",
    "| `OmniModelForSeq2Seq`                             | Sequence-to-Sequence         | Translating a DNA sequence to a protein sequence.        |\n",
    "| `OmniModelForRNADesign`                           | Sequence Generation          | Designing an RNA sequence that folds into a target shape. |\n",
    "| `OmniModelForMLM` (Masked Language Model)         | Self-Supervised Pre-training | Learning general representations from unlabeled DNA.     |\n",
    "\n",
    "This modular design is powerful because it allows you to leverage the same foundational knowledge for many different problems. You simply swap out the \"head\" to match your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2c986",
   "metadata": {},
   "source": [
    "### 2. The Principle of Model Selection: Matching Task to Architecture\n",
    "\n",
    "The selection principle is straightforward: **match the model architecture to the machine learning task you defined in the data preparation phase.**\n",
    "\n",
    "In our case:\n",
    "-   **Biological Problem**: Predicting if any of 919 transcription factors bind to a given DNA sequence.\n",
    "-   **Data Format**: A DNA sequence of length 1000.\n",
    "-   **Label Format**: A binary vector of length 919, where each element indicates binding (1) or no binding (0) for a specific TF.\n",
    "-   **ML Task**: Since a single sequence can have multiple TFs binding to it, this is a **Multi-Label Sequence Classification** problem.\n",
    "\n",
    "Based on the table above, the clear choice for this task is `OmniModelForMultiLabelSequenceClassification`. This model is specifically designed to take a single sequence as input and produce multiple independent binary classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd77b9",
   "metadata": {},
   "source": [
    "### 3. Model Architecture: Base Model + Task Head\n",
    "\n",
    "Let's visualize the architecture. At its core, our model consists of two parts:\n",
    "\n",
    "1.  **The Base Model**: This is a large, pre-trained transformer model (like OmniGenome-186M) that has been trained on a massive amount of unlabeled genomic data. Its job is to read a DNA sequence (as a series of tokens) and convert it into a rich numerical representation (an embedding). This embedding captures the complex syntax and semantics of the genomic language.\n",
    "2.  **The Task Head**: This is a smaller, simple neural network (usually just one or two linear layers) that sits on top of the base model. It takes the sequence embedding from the base model and transforms it into the desired output format for our specific task. For our TFB problem, this is a **multi-label classification head**.\n",
    "\n",
    "Here is a diagram illustrating this concept:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Input\"\n",
    "        A[DNA Sequence<br/>\"GATTACA...\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"Tokenization\"\n",
    "        B[Input Tokens<br/>\"[CLS], G, A, T, T, A, A, [SEP]\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"OmniModelForMultiLabelSequenceClassification\"\n",
    "        C(Base Model<br/>OmniGenome-186M)\n",
    "        D(Classification Head<br/>Linear Layer + Sigmoid)\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output\"\n",
    "        E[Prediction Vector<br/>\"[0.9, 0.1, 0.05, ..., 0.8]\"]\n",
    "    end\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "    C -- Sequence Embedding --> D\n",
    "    D --> E\n",
    "```\n",
    "\n",
    "The base model does the heavy lifting of understanding the sequence, while the head adapts that understanding to our specific predictive goal. During fine-tuning, we will update the weights of both the head and (to a lesser extent) the base model to optimize for our TFB prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc513e73",
   "metadata": {},
   "source": [
    "### 4. Inputs and Outputs: A Look at the Data Flow\n",
    "\n",
    "Understanding what the model expects and what it returns is critical for debugging and interpretation.\n",
    "\n",
    "-   **Model Input**: The model expects a batch of tokenized sequences. When you use the `AccelerateTrainer` (which we will see in the next tutorial), the data loader will automatically prepare a dictionary-like object for each batch. This object contains:\n",
    "    -   `input_ids`: A tensor of token IDs representing the DNA sequences.\n",
    "    -   `attention_mask`: A tensor indicating which tokens are real and which are padding.\n",
    "    -   `labels`: A tensor containing the ground-truth labels for each sequence.\n",
    "\n",
    "-   **Model Output**: During training, the model returns a dictionary containing:\n",
    "    -   `loss`: The calculated loss value, which the trainer uses to update the model weights.\n",
    "    -   `logits`: The raw, unnormalized output scores from the final linear layer. For our task, this will be a tensor of shape `(batch_size, 919)`.\n",
    "\n",
    "During inference (prediction), the model simply returns the `logits`. To get probabilities, we typically apply a Sigmoid function to the logits. To get binary predictions, we can then threshold these probabilities (e.g., predict 1 if probability > 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e58fd0",
   "metadata": {},
   "source": [
    "### 5. Practical Implementation: Initializing the Model\n",
    "\n",
    "Now, let's translate this theory into code. We will perform the following steps:\n",
    "1.  Import necessary libraries.\n",
    "2.  Define a configuration object to hold all our parameters.\n",
    "3.  Write a function to initialize the tokenizer and the `OmniModelForMultiLabelSequenceClassification` model.\n",
    "\n",
    "First, let's set up our environment by importing the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bee4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer, \n",
    "    OmniModelForMultiLabelSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c976d1",
   "metadata": {},
   "source": [
    "Next, we define a configuration dictionary. This is a best practice that centralizes all important parameters, making the code cleaner and easier to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - matches complete tutorial exactly\n",
    "config = {\n",
    "    \"model_name_or_path\": \"yangheng/OmniGenome-52M\",\n",
    "    \"num_labels\": 919,\n",
    "    \"max_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"  üß¨ Model: {config['model_name_or_path']}\")\n",
    "print(f\"  üè∑Ô∏è Labels: {config['num_labels']} TF binding sites\")\n",
    "print(f\"  üìè Max length: {config['max_length']} tokens\")\n",
    "print(f\"  üì± Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497f225",
   "metadata": {},
   "source": [
    "Finally, we create the function that loads our model. This function encapsulates the logic for:\n",
    "-   Loading the pre-trained tokenizer.\n",
    "-   Loading the pre-trained base model.\n",
    "-   Initializing our chosen task-specific model, `OmniModelForMultiLabelSequenceClassification`, which wraps the base model and adds the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer - matches complete tutorial\n",
    "print(\"üîÑ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config[\"model_name_or_path\"])\n",
    "print(f\"‚úÖ Tokenizer loaded: {config['model_name_or_path']}\")\n",
    "\n",
    "# Load model - matches complete tutorial exactly\n",
    "print(\"üîÑ Loading model...\")\n",
    "model = OmniModelForMultiLabelSequenceClassification(\n",
    "    config[\"model_name_or_path\"],\n",
    "    tokenizer,\n",
    "    num_labels=config[\"num_labels\"],\n",
    ")\n",
    "\n",
    "# Move model to the specified device\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully.\")\n",
    "print(f\"üéØ Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Show model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "print(f\"üß¨ Ready for {config['num_labels']}-label TF binding prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae5fa",
   "metadata": {},
   "source": [
    "### Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully initialized a powerful Genomic Foundation Model tailored for your specific biological task.\n",
    "\n",
    "To recap, we have:\n",
    "-   Surveyed the different model architectures available in `OmniGenBench`.\n",
    "-   Learned the principle of matching the model architecture to the ML task.\n",
    "-   Understood the \"base model + task head\" design.\n",
    "-   Initialized the `OmniModelForMultiLabelSequenceClassification` model and its tokenizer.\n",
    "\n",
    "We now have the two key components ready: a prepared dataset and an initialized model. The next logical step is to bring them together and train the model to make accurate predictions.\n",
    "\n",
    "In the next tutorial, **[3/4: Model Training](./03_model_training.ipynb)**, we will take this model and the data we prepared in the first tutorial and begin the fine-tuning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
