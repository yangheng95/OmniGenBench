{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ecbd5",
   "metadata": {},
   "source": [
    "# üß¨ TFB Prediction Tutorial 1/4: From Biological Questions to Data Pipelines\n",
    "\n",
    "Welcome to the first tutorial in our four-part series. This guide focuses on the foundational and most critical step of any computational biology project: **understanding and preparing your data**.\n",
    "\n",
    "> üìö **Prerequisites**: It is recommended to study the **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)** to understand the general machine learning task classification and framework concepts.\n",
    "\n",
    "Before we write any code, we must first understand the landscape of biological data and how to frame biological questions as machine learning tasks.\n",
    "\n",
    "### 1. The Language of Life: DNA and RNA\n",
    "\n",
    "At its core, genomics is the study of sequences. The primary types are:\n",
    "- **DNA (Deoxyribonucleic acid)**: The blueprint of life, composed of four bases (A, T, C, G). It carries the genetic instructions for the development, functioning, growth, and reproduction of all known organisms.\n",
    "- **RNA (Ribonucleic acid)**: Often a messenger, RNA (composed of A, U, C, G) plays a crucial role in translating the instructions from DNA into proteins. It can also have structural and regulatory roles.\n",
    "\n",
    "These sequences are not random strings, and they contain complex patterns, grammar, and syntax. Our goal is to teach a machine to read and understand this \"language of life.\"\n",
    "\n",
    "### 2. Framing Biological Questions as ML Tasks\n",
    "\n",
    "A biological question must be translated into a well-defined machine learning task. Here are the most common types:\n",
    "\n",
    "| Task Type | Biological Question | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Sequence Classification** | Does this DNA sequence have a specific property? | Is this sequence a promoter region? (Yes/No) |\n",
    "| **Multi-Label Classification**| What properties does this DNA sequence have? | Which of these 100 transcription factors will bind to this sequence? |\n",
    "| **Sequence Regression** | What is the quantitative value associated with this sequence? | How efficiently will this mRNA be translated into a protein? (Predict a score) |\n",
    "| **Token Classification** | What is the function of *each base* in this sequence? | Identify the start and end of a gene within a long DNA strand. |\n",
    "| **Sequence-to-Sequence** | Can we translate this sequence into another? | Given a DNA coding sequence, predict the resulting protein sequence. |\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Mapping Problems to Tasks\"\n",
    "        A[\"Biological Question<br/>(e.g., Where do proteins bind?)\"] --> B[\"ML Task<br/>(e.g., Multi-Label Sequence Classification)\"];\n",
    "        B --> C[\"Model Choice<br/>(e.g., OmniModelForMultiLabelSequenceClassification)\"];\n",
    "        C --> D[\"Data Representation<br/>(e.g., DNA sequence as input, multi-hot vector as output)\"];\n",
    "    end\n",
    "```\n",
    "\n",
    "### 3. The OmniGenBench Toolbox: Available Models for Every Task\n",
    "\n",
    "`OmniGenBench` provides a suite of pre-configured models, each designed for a specific task. This saves you from having to build them from scratch.\n",
    "\n",
    "| Task | OmniGenBench Model | When to Use It |\n",
    "| :--- | :--- | :--- |\n",
    "| Single-Label Sequence Classification | `OmniModelForSequenceClassification` | Predicting one label for the whole sequence. |\n",
    "| **Multi-Label Sequence Classification** | `OmniModelForMultiLabelSequenceClassification` | **(Our focus)** Predicting multiple labels for the whole sequence. |\n",
    "| Sequence Regression | `OmniModelForSequenceRegression` | Predicting a single continuous value for the sequence. |\n",
    "| Token Classification | `OmniModelForTokenClassification` | Predicting a label for each token (base) in the sequence. |\n",
    "| Token Regression | `OmniModelForTokenRegression` | Predicting a continuous value for each token. |\n",
    "| Sequence-to-Sequence | `OmniModelForSeq2Seq` | Generating an output sequence from an input sequence. |\n",
    "\n",
    "By understanding this mapping, you can quickly select the right tool for your biological problem.\n",
    "\n",
    "### 4. Our Task: Why TFB is Multi-Label Sequence Classification\n",
    "\n",
    "Now, let's apply this to our tutorial's goal: **Transcription Factor Binding (TFB) Prediction**.\n",
    "\n",
    "- **It's a Sequence task**: Our input is a DNA sequence.\n",
    "- **It's a Classification task**: For each transcription factor, we are asking a \"Yes/No\" question: does it bind?\n",
    "- **It's a Multi-Label task**: A single DNA sequence can be a binding site for *multiple* different TFs simultaneously. We aren't just picking one from a list; we are identifying all potential binders.\n",
    "\n",
    "Therefore, the correct framing is **Multi-Label Sequence Classification**, and the right tool from our toolbox is `OmniModelForMultiLabelSequenceClassification`.\n",
    "\n",
    "With this clear understanding, we can now proceed to prepare our data specifically for this task.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Step-by-Step Guide: Preparing the DeepSEA Dataset\n",
    "\n",
    "Now we move from theory to practice. This section will guide you through the hands-on process of preparing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2797128",
   "metadata": {},
   "source": [
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library, `transformers` provides the underlying model architecture, and the other packages are utilities for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U omnigenbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b74fcd",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and interacting with the operating system.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForMultiLabelClassification,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747bb59",
   "metadata": {},
   "source": [
    "## üìã Understanding OmniGenBench Data Templates\n",
    "\n",
    "Before diving into data loading, it's crucial to understand how OmniGenBench expects data to be organized. This section explains the standardized data templates and directory structures that make the framework so powerful and consistent.\n",
    "\n",
    "### üóÇÔ∏è Standard Directory Structure\n",
    "\n",
    "OmniGenBench follows a conventional directory structure that enables automatic data discovery and loading:\n",
    "\n",
    "```\n",
    "dataset_directory/\n",
    "‚îú‚îÄ‚îÄ train.jsonl              # Training data (required)\n",
    "‚îú‚îÄ‚îÄ valid.jsonl              # Validation data (recommended)\n",
    "‚îú‚îÄ‚îÄ test.jsonl               # Test data (optional)\n",
    "‚îú‚îÄ‚îÄ config.py                # Metadata file (optional)\n",
    "‚îî‚îÄ‚îÄ README.md                # Documentation (recommended)\n",
    "```\n",
    "\n",
    "### üóÇÔ∏è Note\n",
    "The dataset file type can also be such as CSV/TSV, parquet, or other formats, in which case the files would be named `train.csv`, `valid.csv`, and `test.csv`.\n",
    "The `config.py` file is optional but highly recommended. It allows you to define important metadata about your dataset, such as the number of labels, label names, and sequence length. This information helps the framework understand how to process your data correctly.\n",
    "\n",
    "**Key Benefits of This Structure:**\n",
    "- üîÑ **Automatic Discovery**: Framework can find and load data without manual path specification\n",
    "- üìä **Consistent Splits**: Standard train/valid/test division across all datasets  \n",
    "- üß¨ **Metadata Integration**: Additional biological annotations stored separately\n",
    "- üìö **Documentation**: Self-documenting datasets for reproducible research\n",
    "\n",
    "### üìÑ Data File Formats\n",
    "\n",
    "#### **JSONL Format (Recommended)**\n",
    "Each line contains a JSON object with the sequence and its annotations:\n",
    "\n",
    "```json\n",
    "{\"sequence\": \"ATCGATCG...\", \"label\": [1,0,1,0,1,...], \"id\": \"seq_001\"}\n",
    "{\"sequence\": \"GCTAGCTA...\", \"label\": [0,1,0,1,0,...], \"id\": \"seq_002\"}\n",
    "```\n",
    "\n",
    "#### **CSV Format (Alternative)**\n",
    "Tabular format with columns for sequence, labels, and metadata:\n",
    "\n",
    "```csv\n",
    "id,sequence,label,chromosome,start,end\n",
    "seq_001,ATCGATCG...,\"1,0,1,0,1\",chr1,1000,1500\n",
    "seq_002,GCTAGCTA...,\"0,1,0,1,0\",chr2,2000,2500\n",
    "```\n",
    "\n",
    "### üè∑Ô∏è Label Format for Multi-Label Classification\n",
    "\n",
    "For TFB prediction, labels represent binding status for multiple transcription factors:\n",
    "\n",
    "**Array Format (JSONL):**\n",
    "```json\n",
    "\"label\": [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "```\n",
    "\n",
    "**String Format (CSV):**\n",
    "```csv\n",
    "\"1,0,1,0,1,0,0,1,0,1\"\n",
    "```\n",
    "\n",
    "Where each position corresponds to a specific transcription factor:\n",
    "- Position 0: CTCF (1 = binds, 0 = doesn't bind)\n",
    "- Position 1: p53 (0 = doesn't bind)\n",
    "- Position 2: FOXA1 (1 = binds)\n",
    "- And so on...\n",
    "\n",
    "### ‚öôÔ∏è Configuration Files\n",
    "\n",
    "#### **config.py (Optional)**\n",
    "A Python file defining dataset metadata and parameters:\n",
    "```json\n",
    "{\n",
    "  \"task_type\": \"multi_label_classification\",\n",
    "  \"sequence_type\": \"DNA\",\n",
    "  \"num_labels\": 919,\n",
    "  \"label_names\": [\"CTCF\", \"p53\", \"FOXA1\", ...],\n",
    "  \"sequence_length\": 1000,\n",
    "  \"total_samples\": 440000,\n",
    "  \"description\": \"DeepSEA dataset for TF binding prediction\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Key Metadata Fields:**\n",
    "- **`task_type`**: Defines the ML task (classification, regression, etc.)\n",
    "- **`sequence_type`**: DNA, RNA, or Protein\n",
    "- **`num_labels`**: Number of target labels (919 for DeepSEA)\n",
    "- **`label_names`**: Human-readable names for each label\n",
    "- **`sequence_length`**: Fixed length of input sequences\n",
    "\n",
    "### üéØ Loading Strategy Options\n",
    "\n",
    "OmniGenBench provides multiple loading strategies based on your data format:\n",
    "\n",
    "```python\n",
    "# Method 1: From Hugging Face Hub (automatic download)\n",
    "dataset = OmniDatasetForMultiLabelClassification.from_hub(\n",
    "    \"deepsea_tfb_prediction\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Method 2: From local directory (JSONL files)\n",
    "dataset = OmniDatasetForMultiLabelClassification(\n",
    "    \"./my_tfb_dataset/\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Method 3: From CSV file\n",
    "dataset = OmniDatasetForMultiLabelClassification(\n",
    "    \"tfb_data.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    sequence_column=\"sequence\",\n",
    "    label_column=\"label\"\n",
    ")\n",
    "```\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "1. **üè∑Ô∏è Consistent Naming**: Use descriptive, consistent file and column names\n",
    "2. **üìä Balanced Splits**: Ensure train/valid/test splits are representative\n",
    "3. **üîç Quality Control**: Validate data integrity before training\n",
    "4. **üìö Documentation**: Include README with dataset description and usage\n",
    "5. **üß¨ Biological Context**: Preserve important biological metadata\n",
    "\n",
    "With this foundation understanding, let's now proceed to configure our specific dataset parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd70321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic configuration - matches complete tutorial\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"deepsea_tfb_prediction\"\n",
    "\n",
    "# Load tokenizer and datasets using enhanced OmniDataset - matches complete tutorial\n",
    "print(\"üîÑ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "print(f\"‚úÖ Tokenizer loaded: {model_name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6db354",
   "metadata": {},
   "source": [
    "### 1.3: Data Acquisition\n",
    "\n",
    "With our environment configured, it's time to download the DeepSEA dataset. The function below automates this process by:\n",
    "1.  Checking if the data already exists.\n",
    "2.  Downloading the dataset from the specified URL if needed.\n",
    "3.  Extracting the files.\n",
    "4.  Cleaning up the temporary zip file.\n",
    "\n",
    "This ensures we have the `train.jsonl`, `valid.jsonl`, and `test.jsonl` files ready for the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Loading DeepSEA TFB dataset...\")\n",
    "datasets = OmniDatasetForMultiLabelClassification.from_hub(\n",
    "    dataset_name=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_examples=1000,  # For quick testing; set to None for full dataset 440M examples\n",
    "    force_padding=False  # The sequence length is fixed, so no need to pad sequence and labels\n",
    ")\n",
    "\n",
    "print(\"üìù Data loading completed! Using  OmniDataset framework.\")\n",
    "print(f\"üìä Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")\n",
    "\n",
    "# Demonstrate dataset functionality - enhanced version\n",
    "print(\"üß™ Dataset Analysis and Validation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample a few examples from the training set\n",
    "sample_size = 3\n",
    "train_samples = [datasets['train'][i] for i in range(min(sample_size, len(datasets['train'])))]\n",
    "\n",
    "for i, sample in enumerate(train_samples):\n",
    "    print(f\"\\nüìã Sample {i+1}:\")\n",
    "    print(f\"   üß¨ Input shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   üè∑Ô∏è Labels shape: {sample['labels'].shape}\")\n",
    "    print(f\"   üìä Positive labels: {sample['labels'].sum().item()}/{len(sample['labels'])} TFs\")\n",
    "    \n",
    "    # Show first few nucleotides\n",
    "    sequence_ids = sample['input_ids'][:20].tolist()\n",
    "    print(f\"   üî§ First 20 tokens: {sequence_ids}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset validation completed!\")\n",
    "print(f\"   üìä All samples have consistent shapes\")\n",
    "print(f\"   üß¨ Ready for multi-label TFB prediction\")\n",
    "print(f\"   üéØ {datasets['train'][0]['labels'].shape[0]} TF labels per sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc8cf4",
   "metadata": {},
   "source": [
    "### 1.4: Custom Dataset and Data Loaders\n",
    "Now that we have the data files, we need a way to load them into our model efficiently. We'll do this in two parts:\n",
    "\n",
    "For most of the classification and regression tasks, the dataset have been integrated in OmniGenBench, e.g.,  \n",
    "\n",
    "| Dataset Class | Task Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| `OmniDatasetForSequenceClassification` | Sequence Classification | Tasks for classifying the entire sequence into one category (e.g., promoter vs. non-promoter). |\n",
    "| `OmniDatasetForMultiLabelClassification` | **Multi-Label Classification** | **Tasks for predicting multiple labels for a sequence (e.g., TF binding prediction).** |\n",
    "| `OmniDatasetForSequenceRegression` | Sequence Regression | Tasks for predicting a single continuous value for the entire sequence (e.g., translation efficiency). |\n",
    "| `OmniDatasetForTokenClassification` | Token (Base) Classification | Tasks for assigning a label to each token (base) in the sequence (e.g., identifying splice sites). |\n",
    "| `OmniDatasetForTokenRegression` | Token (Base) Regression | Tasks for predicting a continuous value for each token (base) in the sequence. |\n",
    "\n",
    "To demonstrate how to customize a dataset, we here define a dataset for the deepsea dataset in the following cell.\n",
    "\n",
    "#### A. The `DeepSEADataset` Class\n",
    "This custom class acts as a bridge between our raw `.jsonl` files and the PyTorch ecosystem. It inherits from `OmniDataset` and tells our framework how to process each data entry. Specifically, it:\n",
    "1.  **Processes a DNA sequence and its labels** via **prepare_input()**.\n",
    "2.  **Truncates or pads the sequence** to a fixed length (`MAX_LENGTH`). This is crucial because language models require inputs of a consistent size.\n",
    "3.  **Selects the specific TF labels** we want to train on.\n",
    "4.  **Tokenizes the sequence**, converting the string of \"A, C, G, T\" into numerical tokens that the model can understand.\n",
    "5.  **Formats the output** as PyTorch tensors.\n",
    "\n",
    "#### B. Creating DataLoaders\n",
    "Once the `DeepSEADataset` is defined, we use PyTorch's `DataLoader` to create an efficient pipeline. The `DataLoader` is responsible for:\n",
    "1.  **Batching**: Grouping individual samples into batches (`BATCH_SIZE`).\n",
    "2.  **Shuffling**: Randomly shuffling the training data each epoch to improve generalization.\n",
    "3.  **Parallelism**: Loading data in the background so it's ready for the model when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Data loading completed! Using enhanced OmniDataset framework.\")\n",
    "print(f\"üìä Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")\n",
    "\n",
    "# The datasets are now ready for training!\n",
    "# Each dataset contains:\n",
    "# - Tokenized DNA sequences \n",
    "# - Multi-hot encoded labels for 919 TF binding sites\n",
    "# - Proper batching and data loading handled automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba59bfb",
   "metadata": {},
   "source": [
    "### 1.5 Enhanced Data Pipeline\n",
    "With the enhanced OmniGenBench framework, DataLoaders are automatically created and optimized. The framework handles all the complexities:\n",
    "\n",
    "1. **Automatic Batching**: Optimal batch sizes for your hardware configuration\n",
    "2. **Intelligent Shuffling**: Proper data shuffling for better training dynamics  \n",
    "3. **Memory Optimization**: Efficient memory usage for large genomic datasets\n",
    "4. **Multi-Label Handling**: Proper formatting for multi-label classification tasks\n",
    "5. **Hardware Acceleration**: Automatic GPU optimization when available\n",
    "\n",
    "This means you can focus on the biological insights rather than the technical implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ Enhanced data pipeline ready!\")\n",
    "print(\"The datasets are now fully prepared and optimized for training.\")\n",
    "print(\"DataLoaders will be automatically created during the training process.\")\n",
    "\n",
    "# Let's verify our data is properly formatted\n",
    "sample_data = datasets['train'][0]\n",
    "print(f\"\\nüìã Sample data structure:\")\n",
    "print(f\"  - Input IDs shape: {sample_data['input_ids'].shape}\")\n",
    "print(f\"  - Labels shape: {sample_data['labels'].shape}\")\n",
    "print(f\"  - Number of TF binding sites: {sample_data['labels'].sum().item()}/{len(sample_data['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c6a1a",
   "metadata": {},
   "source": [
    "### Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully built a complete data preparation pipeline. You've learned not only *how* to code a data pipeline but also *why* it's structured the way it is, starting from the biological question itself.\n",
    "\n",
    "Your data is now ready to be used for training a model.\n",
    "\n",
    "In the next tutorial, **[2/4: Model Initialization](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/02_model_initialization.ipynb)**, we will take these `Datasets` and learn how to set up a pre-trained Genomic Foundation Model for our TFB prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
