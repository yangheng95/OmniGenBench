{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947dcdbbe7f7d4b5",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Transcription Factor Binding Prediction with Foundation Model\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to predict Transcription Factor Binding (TFB) sites on plant DNA sequences using **OmniGenBench** and **PlantRNA-FM** (Plant RNA Foundation Model). This guide demonstrates the practical application of plant-specialized genomic deep learning for regulatory genomics.\n",
    "\n",
    "> ðŸ“š **Prerequisites**: If you're new to OmniGenBench, we strongly recommend starting with the **[Fundamental Concepts Tutorial](https://github.com/yangheng95/OmniGenBench/blob/master/examples/00_fundamental_concepts.ipynb)**. It covers essential topics including language model concepts, machine learning task classification, foundation model principles (particularly PlantRNA-FM), and other foundational knowledge.\n",
    "\n",
    "### 1. The Biological Challenge: Plant Transcription Factor Binding\n",
    "\n",
    "**Transcription Factors (TFs)** are essential proteins that regulate gene expression in plants, controlling processes from development to stress responses. They bind to specific DNA sequences in regulatory regions. Identifying these binding sites is critical for understanding plant gene regulation, which is fundamental to crop improvement, stress tolerance, and developmental biology.\n",
    "\n",
    "However, experimentally identifying TF binding sites across plant genomes is slow and expensive. This is where **PlantRNA-FM** (published in *Nature Machine Intelligence*, 35M parameters), a plant-specialized foundation model, can make a significant impact by learning plant-specific regulatory patterns efficiently.\n",
    "\n",
    "### 2. The Data: Plant Regulatory Genomics Dataset\n",
    "\n",
    "To train a model for this task, we use a curated dataset of plant regulatory regions, adapted from the **DeepSEA dataset** for plant genomics applications.\n",
    "\n",
    "- **What it contains**: 1000-base-pair DNA sequences from plant genomes (Arabidopsis, rice, maize).\n",
    "- **What it labels**: Each sequence is associated with 919 binary labels indicating the presence or absence of various chromatin features, including plant TF binding, DNase I sensitivity, and histone marks specific to plant genomes.\n",
    "- **Our Goal**: Train PlantRNA-FM to predict which of the 919 features are present for any given plant DNA sequence.\n",
    "\n",
    "| sequence (1000 bases) | label (919) |\n",
    "|---------|-------|\n",
    "| AUGGCCAUUGUAAUUGGCCGACUUGA... | [0,1,1,0...,1,0,0,1] |\n",
    "| AUGGCUACUAGCUAGCUAGCUAGC... | [1,1,1,0...,0,0,1,1] |\n",
    "| ... | ... |\n",
    "\n",
    "Find the dataset template in **[Dataset Template](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/05_advanced_dataset_creation.ipynb)** and customize it as needed for your experiments.\n",
    "\n",
    "### 3. Quick Start: Transcription Factor Binding Prediction Workflow\n",
    "\n",
    "This tutorial demonstrates the practical application of the **[Fundamental Concepts Tutorial](https://github.com/yangheng95/OmniGenBench/blob/master/00_fundamental_concepts.ipynb)** to a specific biological problem. We'll use the standard 4-step OmniGenBench workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "320c321e",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"4-step workflow.png\", width=300))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c23193d7",
   "metadata": {},
   "source": [
    "**Transcription Factor Binding Prediction** is a **multi-label classification** task where we predict which transcription factors will bind to plant DNA sequences. We'll use `OmniModelForMultiLabelSequenceClassification` with **PlantRNA-FM** to leverage plant-specific regulatory patterns learned during pre-training.\n",
    "\n",
    "### 4. Tutorial Structure\n",
    "\n",
    "1. **[Data Preparation](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/01_data_preparation.ipynb)**: Download and preprocess plant regulatory genomics data for multi-label classification\n",
    "2. **[Model Initialization](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/02_model_initialization.ipynb)**: Load PlantRNA-FM and configure it for multi-label plant TFB prediction\n",
    "3. **[Model Training](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/03_model_training.ipynb)**: Fine-tune PlantRNA-FM using plant genomic data and validate its performance\n",
    "4. **[Model Inference](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/04_model_inference.ipynb)**: Use the trained model to predict transcription factor binding sites on new plant DNA sequences\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c6b9-cb43-4b97-bbe4-e136ccd84bf1",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Data Preparation\n",
    "\n",
    "This first step is all about getting our data ready for in-silico analysis. It involves four key parts:\n",
    "1.  **Environment Setup**: Installing and importing the necessary libraries.\n",
    "2.  **Configuration**: Defining all our important parameters in one place.\n",
    "3.  **Data Acquisition**: Downloading and preparing the raw dataset.\n",
    "4.  **Data Loading**: Creating a pipeline to efficiently feed data to the model.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library, `transformers` provides the underlying model architecture, and the other packages are utilities for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "id": "183876efc5e7cc96",
   "metadata": {},
   "source": [
    "!pip install -U omnigenbench"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa5dfb3779432687",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and interacting with the operating system.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU. This makes a huge difference when working with large models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "id": "7a6639117f94fa80",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForMultiLabelClassification,\n",
    "    OmniModelForMultiLabelSequenceClassification,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1adc406c9352bd7f",
   "metadata": {},
   "source": [
    "### 1.2: Global Configuration\n",
    "\n",
    "To make our tutorial easy to modify and understand, we'll centralize all important parameters in this section. This is a best practice in software development that makes experiments more reproducible.\n",
    "\n",
    "#### Key Parameters\n",
    "-   **Dataset**: We define the local path and download URL for our dataset.\n",
    "-   **Model**: We select which pre-trained OmniGenome model to use. For this tutorial, we'll start with `OmniGenome-52M` because it's fast and efficient, making it perfect for learning and prototyping.\n",
    "\n",
    "This centralized approach allows you to easily experiment with different settings (e.g., a larger model, a different learning rate) without having to hunt through the code.\n",
    "\n",
    "#### Note\n",
    "Almost all the parameters here are standard in machine learning workflows and have a default value that works well if you don't set them. Don't worry if some of these terms are unfamiliar right now. We'll explain each one as we go along."
   ]
  },
  {
   "cell_type": "code",
   "id": "4d91bd4216de4618",
   "metadata": {},
   "source": [
    "config_or_model = \"yangheng/PlantRNA-FM\"  # Plant RNA Foundation Model\n",
    "dataset_name = \"deepsea_tfb_prediction\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9be6b70c5fa67ba",
   "metadata": {},
   "source": [
    "### 1.3: Data Acquisition\n",
    "\n",
    "With our environment configured, it's time to download the DeepSEA dataset. The function below automates this process by:\n",
    "1.  Checking if the data already exists.\n",
    "2.  Downloading the dataset from the specified URL if needed.\n",
    "3.  Extracting the files.\n",
    "4.  Cleaning up the temporary zip file.\n",
    "\n",
    "This ensures we have the `train.jsonl`, `valid.jsonl`, and `test.jsonl` files ready for the next stage. These files represent the standard splits for training, validating, and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "id": "d61ca36fdb228ad",
   "metadata": {},
   "source": [
    "# Load tokenizer and datasets using enhanced OmniDataset\n",
    "tokenizer = OmniTokenizer.from_pretrained(config_or_model)\n",
    "\n",
    "datasets = OmniDatasetForMultiLabelClassification.from_hub(\n",
    "    dataset_name_or_path=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_examples=1000,  # For quick testing; set to None for full dataset 440M examples\n",
    "    force_padding=False  # The sequence length is fixed, so no need to pad sequence and labels\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74598a7903d9d652",
   "metadata": {},
   "source": [
    "### 1.4: Dataset Loading with OmniGenBench\n",
    "\n",
    "With OmniGenBench, data loading is significantly simplified! The framework automatically handles:\n",
    "\n",
    "#### Automatic Data Processing\n",
    "The `OmniDatasetForMultiLabelClassification` class automatically:\n",
    "1.  **Downloads and processes** the dataset from Hugging Face Hub\n",
    "2.  **Handles sequence preprocessing** including truncation, padding, and tokenization\n",
    "3.  **Manages multi-label formatting** for TF binding prediction\n",
    "4.  **Creates train/validation/test splits** ready for training\n",
    "\n",
    "This streamlined approach eliminates the need for custom dataset classes while maintaining full flexibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "c7b65e673a660320",
   "metadata": {},
   "source": [
    "print(\"ðŸ“ Data loading completed! Using  OmniDataset framework.\")\n",
    "print(f\"ðŸ“Š Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "216243b2191d93ea",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Initialization with PlantRNA-FM\n",
    "\n",
    "With our data pipeline in place, it's time to set up **PlantRNA-FM**. Instead of building a model from scratch, we'll load the pre-trained PlantRNA-FM and adapt it for our plant transcription factor binding prediction task, leveraging its plant-specific pre-training on regulatory regions.\n",
    "\n",
    "The `OmniModelForMultiLabelSequenceClassification` class from our library handles this process for us, seamlessly combining the base model and the new classification head.\n",
    "In adddition, most of the models and datasets have been integrated into the OmniGenBench package, making it easy to load them with just a few lines of code. Please refer to the sub-tutorials of [data preparation](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/02_data_preparation.ipynb) and [model initialization](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/03_model_initialization.ipynb) for more details.\n",
    "\n",
    "...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "id": "7090afa4b979ebf5",
   "metadata": {},
   "source": [
    "# === Model Initialization ===\n",
    "# We almost support all genomic foundation models from Hugging Face Hub.\n",
    "\n",
    "model = OmniModelForMultiLabelSequenceClassification(\n",
    "    config_or_model,\n",
    "    tokenizer,\n",
    "    num_labels=919,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84c39c79fe05fe78",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Model Training\n",
    "\n",
    "This is the most exciting part! With our data and model ready, we can now begin the **fine-tuning** process. During training, the model will learn to associate specific patterns in the DNA sequences with the presence or absence of TF binding sites.\n",
    "\n",
    "The `AccelerateTrainer` from `omnigenbench` wraps all this logic into a simple interface, allowing us to launch the training process with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "id": "3c58afa6eb5bf95d",
   "metadata": {},
   "source": [
    "metric_functions = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    "\n",
    ")\n",
    "print(\"ðŸŽ“ Starting training...\")\n",
    "\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_tfb_finetuned\")\n",
    "\n",
    "print('Metrics:', metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "344edb6b83677903",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Model Inference and Interpretation\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting TF binding sites on new DNA sequences. This process is called **inference**.\n",
    "\n",
    "### The Inference Pipeline\n",
    "\n",
    "Our inference pipeline consists of a few key steps:\n",
    "1.  **Load the Model**: We load the best-performing model that was saved during training.\n",
    "2.  **Process the Input**: We take a new DNA sequence and apply the same preprocessing steps we used for our training data (truncating/padding and tokenizing).\n",
    "3.  **Run Prediction**: We feed the processed sequence to the model and get its predictions. We use `torch.no_grad()` to disable gradient calculations, which makes inference faster and uses less memory.\n",
    "4.  **Interpret the Results**: The model's raw output is a set of probabilities. We'll interpret these to make them more understandable, identifying which TFs are predicted to bind and with what level of confidence.\n",
    "\n",
    "To demonstrate, we'll test our model on a few sample sequences and print out a user-friendly summary of the results. This shows how the model can be used in a real-world application to analyze sequences of interest."
   ]
  },
  {
   "cell_type": "code",
   "id": "86b586a10274d9aa",
   "metadata": {},
   "source": [
    "\n",
    "inference_model = ModelHub.load(\"yangheng/ogb_tfb_finetuned\")\n",
    "\n",
    "sample_sequences = {\n",
    "    \"Random sequence\": \"AGCT\" * (128 // 4),\n",
    "    \"AT-rich sequence\": \"AATT\" * (128 // 4),\n",
    "    \"GC-rich sequence\": \"GCGC\" * (128 // 4),\n",
    "}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq_name, sequence in sample_sequences.items():\n",
    "        outputs = inference_model.inference(sequence)\n",
    "        print(\"âœ… Prediction completed!\")\n",
    "\n",
    "        # â€”â€” ç»“æžœè§£é‡Š â€”â€”\n",
    "        predictions = outputs.get('predictions', None)\n",
    "        logits = outputs.get('logits', None)  # logits are probabilities after Sigmoid for multi-label\n",
    "        predictions = np.array(predictions)\n",
    "        probabilities = np.array(logits) if logits is not None else None\n",
    "\n",
    "        positive_count = np.sum(predictions == 1)\n",
    "        total_count = len(predictions)\n",
    "\n",
    "        print(f\"ðŸ“Š Prediction summary:\")\n",
    "        print(f\"  ðŸŽ¯ Total TF binding sites analyzed: {total_count}\")\n",
    "        print(f\"  âœ… Predicted binding sites: {positive_count}\")\n",
    "        print(f\"  ðŸ“ˆ Binding rate: {positive_count/total_count:.1%}\")\n",
    "\n",
    "        if probabilities is not None:\n",
    "            print(f\"\\nðŸ† Top 5 highest confidence predictions:\")\n",
    "            sorted_indices = np.argsort(probabilities)[::-1]\n",
    "\n",
    "            for i, idx in enumerate(sorted_indices[:5]):\n",
    "                tf_id = idx + 1\n",
    "                prediction = \"Binding\" if predictions[idx] == 1 else \"No binding\"\n",
    "                confidence = probabilities[idx]\n",
    "\n",
    "                if confidence > 0.8:\n",
    "                    emoji = \"ðŸ”¥\"\n",
    "                elif confidence > 0.6:\n",
    "                    emoji = \"â­\"\n",
    "                elif confidence > 0.4:\n",
    "                    emoji = \"ðŸ’«\"\n",
    "                else:\n",
    "                    emoji = \"ðŸ’­\"\n",
    "\n",
    "                print(f\"  {emoji} TF-{tf_id:03d}: {prediction} (confidence: {confidence:.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bf11d72370e8e16",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on transcription factor binding prediction with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end MLOps workflow, a critical skill in modern computational biology. Specifically, you have:\n",
    "\n",
    "1.  **Understood the \"Why\"**: Gained an appreciation for the biological problem of TFB prediction and how Genomic Foundation Models provide a powerful solution.\n",
    "\n",
    "2.  **Mastered the 4-Step Workflow**:\n",
    "    -   **Step 1: Data Preparation**: You learned how to acquire, process, and efficiently load a large-scale genomic dataset using the enhanced OmniDataset framework.\n",
    "    -   **Step 2: Model Initialization**: You saw how to leverage a powerful pre-trained model and adapt it for multi-label sequence classification tasks.\n",
    "    -   **Step 3: Model Training**: You implemented a robust training strategy using AccelerateTrainer with proper evaluation metrics and model persistence.\n",
    "    -   **Step 4: Model Inference**: You used your fine-tuned model to make predictions on new DNA sequences and interpreted the results for biological insights.\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. The future of computational biology is in your hands! \n",
    "\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "Now that you've mastered translation efficiency prediction, you can:\n",
    "\n",
    "#### ðŸ§¬ **Explore Other Sequence Classification Tasks**\n",
    "- **Promoter Recognition**: Identify regulatory sequences\n",
    "- **Subcellular Localization**: Predict protein cellular destinations  \n",
    "- **Functional Annotation**: Classify protein or RNA functions\n",
    "\n",
    "#### ðŸ“Š **Try Different Task Types**\n",
    "- **Sequence Regression**: Gene expression level prediction\n",
    "- **Token Classification**: Binding site identification\n",
    "- **Multi-label Classification**: Multi-functional sequence prediction\n",
    "\n",
    "#### ðŸ”¬ **Advanced Techniques**\n",
    "- **Custom Dataset Creation**: Use the [Advanced Dataset Creation Tutorial](https://github.com/yangheng95/OmniGenBench/blob/main/examples/tfb_prediction/05_advanced_dataset_creation.ipynb)\n",
    "- **Model Comparison**: Benchmark different foundation models\n",
    "- **Hyperparameter Optimization**: Fine-tune model performance\n",
    "- **Biological Validation**: Compare predictions with experimental data\n",
    "\n",
    "### ðŸ“š Resources\n",
    "\n",
    "- **[Fundamental Concepts Tutorial](https://github.com/yangheng95/OmniGenBench/blob/main/00_fundamental_concepts.ipynb)**: Review core concepts anytime\n",
    "- **[OmniGenBench Documentation](https://omnigenbench.readthedocs.io/)**: Complete API reference\n",
    "- **[GitHub Repository](https://github.com/yangheng95/OmniGenBench)**: Source code and community discussions\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. \n",
    "\n",
    "**Happy coding and discovering! ðŸ§¬âœ¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090a3b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
