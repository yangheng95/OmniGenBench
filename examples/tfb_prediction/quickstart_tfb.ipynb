{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947dcdbbe7f7d4b5",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Transcription Factor Binding Prediction with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to predict Transcription Factor Binding (TFB) sites on DNA sequences using **OmniGenBench**. This guide focuses on the specific biological problem of transcription factor binding prediction and demonstrates the practical application of genomic deep learning.\n",
    "\n",
    "> ðŸ“š **Prerequisites**: If you're new to OmniGenBench, we strongly recommend starting with the **[Fundamental Concepts Tutorial](../../00_fundamental_concepts.ipynb)**. It covers essential topics including language model concepts, machine learning task classification, genomic foundation model principles, and other foundational knowledge.\n",
    "\n",
    "### 1. The Biological Challenge: What is Transcription Factor Binding?\n",
    "\n",
    "**Transcription Factors (TFs)** are essential proteins that regulate gene expression, the process of turning genes 'on' or 'off'. They do this by binding to specific DNA sequences. Identifying these binding sites is a critical step in understanding gene regulation, which is fundamental to comprehending cellular function, development, and disease.\n",
    "\n",
    "However, experimentally identifying TF binding sites across the entire genome is a slow and expensive process. This is where computational methods, particularly deep learning, can make a significant impact.\n",
    "\n",
    "### 2. The Data: An Introduction to the DeepSEA Dataset\n",
    "\n",
    "To train a model for this task, we need a labeled dataset. We will use the **DeepSEA dataset**, a well-known benchmark in genomics.\n",
    "\n",
    "- **What it contains**: It consists of 1000-base-pair DNA sequences.\n",
    "- **What it labels**: Each sequence is associated with 919 binary labels. These labels indicate the presence or absence of various chromatin features, including TF binding, DNase I sensitivity, and histone marks.\n",
    "- **Our Goal**: We will use these sequences and labels to train a model that can predict which of the 919 features are present for any given DNA sequence.\n",
    "\n",
    "| sequence (1000 bases) | label (919) |\n",
    "|---------|-------|\n",
    "| AUGGCCAUUGUAAUUGGCCGACUUGA... | [0,1,1,0...,1,0,0,1] |\n",
    "| AUGGCUACUAGCUAGCUAGCUAGC... | [1,1,1,0...,0,0,1,1] |\n",
    "| ... | ... |\n",
    "\n",
    "Find the dataset template in **[Dataset Template](./05_advanced_dataset_creation.ipynb)** and customize it as needed for your experiments.\n",
    "\n",
    "### 3. Quick Start: Transcription Factor Binding Prediction Workflow\n",
    "\n",
    "This tutorial demonstrates the practical application of the **[Fundamental Concepts Tutorial](../../00_fundamental_concepts.ipynb)** to a specific biological problem. We'll use the standard 4-step OmniGenBench workflow:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for TFB Prediction\"\n",
    "        A[\"ðŸ“¥ Step 1: Data Preparation<br/>Download and process the DeepSEA dataset\"] --> B[\"ðŸ”§ Step 2: Model Initialization<br/>Load the pre-trained OmniGenome model\"]\n",
    "        B --> C[\"ðŸŽ“ Step 3: Model Training<br/>Fine-tune the model on the TFB dataset\"]\n",
    "        C --> D[\"ðŸ”® Step 4: Model Inference<br/>Use the trained model to predict TFB on new sequences\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "**Transcription Factor Binding Prediction** is a **multi-label classification** task where we predict which transcription factors will bind to DNA sequences. We'll use `OmniModelForMultiLabelSequenceClassification` as demonstrated in the fundamental concepts tutorial.\n",
    "\n",
    "### 4. Tutorial Structure\n",
    "\n",
    "1. **[Data Preparation](./01_data_preparation.ipynb)**: Download and preprocess the DeepSEA dataset for multi-label classification\n",
    "2. **[Model Initialization](./02_model_initialization.ipynb)**: Load the pre-trained OmniGenome model and set it up for multi-label TFB prediction\n",
    "3. **[Model Training](./03_model_training.ipynb)**: Fine-tune the model using our dataset and validate its performance\n",
    "4. **[Model Inference](./04_model_inference.ipynb)**: Use the trained model to predict transcription factor binding sites on new DNA sequences\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c6b9-cb43-4b97-bbe4-e136ccd84bf1",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Data Preparation\n",
    "\n",
    "This first step is all about getting our data ready for in-silico analysis. It involves four key parts:\n",
    "1.  **Environment Setup**: Installing and importing the necessary libraries.\n",
    "2.  **Configuration**: Defining all our important parameters in one place.\n",
    "3.  **Data Acquisition**: Downloading and preparing the raw dataset.\n",
    "4.  **Data Loading**: Creating a pipeline to efficiently feed data to the model.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library, `transformers` provides the underlying model architecture, and the other packages are utilities for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183876efc5e7cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U omnigenbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5dfb3779432687",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and interacting with the operating system.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU. This makes a huge difference when working with large models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6639117f94fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForMultiLabelClassification,\n",
    "    OmniModelForMultiLabelSequenceClassification,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc406c9352bd7f",
   "metadata": {},
   "source": [
    "### 1.2: Global Configuration\n",
    "\n",
    "To make our tutorial easy to modify and understand, we'll centralize all important parameters in this section. This is a best practice in software development that makes experiments more reproducible.\n",
    "\n",
    "#### Key Parameters\n",
    "-   **Dataset**: We define the local path and download URL for our dataset.\n",
    "-   **Model**: We select which pre-trained OmniGenome model to use. For this tutorial, we'll start with `OmniGenome-52M` because it's fast and efficient, making it perfect for learning and prototyping.\n",
    "\n",
    "This centralized approach allows you to easily experiment with different settings (e.g., a larger model, a different learning rate) without having to hunt through the code.\n",
    "\n",
    "#### Note\n",
    "Almost all the parameters here are standard in machine learning workflows and have a default value that works well if you don't set them. Don't worry if some of these terms are unfamiliar right now. We'll explain each one as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91bd4216de4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"deepsea_tfb_prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be6b70c5fa67ba",
   "metadata": {},
   "source": [
    "### 1.3: Data Acquisition\n",
    "\n",
    "With our environment configured, it's time to download the DeepSEA dataset. The function below automates this process by:\n",
    "1.  Checking if the data already exists.\n",
    "2.  Downloading the dataset from the specified URL if needed.\n",
    "3.  Extracting the files.\n",
    "4.  Cleaning up the temporary zip file.\n",
    "\n",
    "This ensures we have the `train.jsonl`, `valid.jsonl`, and `test.jsonl` files ready for the next stage. These files represent the standard splits for training, validating, and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ca36fdb228ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and datasets using enhanced OmniDataset\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "datasets = OmniDatasetForMultiLabelClassification.from_huggingface(\n",
    "    dataset_name=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_examples=1000,  # For quick testing; set to None for full dataset 440M examples\n",
    "    force_padding=False  # The sequence length is fixed, so no need to pad sequence and labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74598a7903d9d652",
   "metadata": {},
   "source": [
    "### 1.4: Dataset Loading with OmniGenBench\n",
    "\n",
    "With OmniGenBench, data loading is significantly simplified! The framework automatically handles:\n",
    "\n",
    "#### Automatic Data Processing\n",
    "The `OmniDatasetForMultiLabelClassification` class automatically:\n",
    "1.  **Downloads and processes** the dataset from Hugging Face Hub\n",
    "2.  **Handles sequence preprocessing** including truncation, padding, and tokenization\n",
    "3.  **Manages multi-label formatting** for TF binding prediction\n",
    "4.  **Creates train/validation/test splits** ready for training\n",
    "\n",
    "This streamlined approach eliminates the need for custom dataset classes while maintaining full flexibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b65e673a660320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Data loading completed! Using  OmniDataset framework.\")\n",
    "print(f\"ðŸ“Š Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216243b2191d93ea",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Initialization\n",
    "\n",
    "With our data pipeline in place, it's time to set up the model. This is where the power of Genomic Foundation Models (GFMs) comes into play. Instead of building a model from scratch, we will load the pre-trained **OmniGenome** model and adapt it for our specific task.\n",
    "\n",
    "The `OmniModelForMultiLabelSequenceClassification` class from our library handles this process for us, seamlessly combining the base model and the new classification head.\n",
    "In adddition, most of the models and datasets have been integrated into the OmniGenBench package, making it easy to load them with just a few lines of code. Please refer to the sub-tutorials of [data preparation](./02_data_preparation.ipynb) and [model initialization](./03_model_initialization.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090afa4b979ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Initialization ===\n",
    "# We almost support all genomic foundation models from Hugging Face Hub.\n",
    "\n",
    "model = OmniModelForMultiLabelSequenceClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer,\n",
    "    num_labels=919,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c39c79fe05fe78",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Model Training\n",
    "\n",
    "This is the most exciting part! With our data and model ready, we can now begin the **fine-tuning** process. During training, the model will learn to associate specific patterns in the DNA sequences with the presence or absence of TF binding sites.\n",
    "\n",
    "The `AccelerateTrainer` from `omnigenbench` wraps all this logic into a simple interface, allowing us to launch the training process with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58afa6eb5bf95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_functions = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    "\n",
    ")\n",
    "print(\"ðŸŽ“ Starting training...\")\n",
    "\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_tfb_finetuned\")\n",
    "\n",
    "print('Metrics:', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344edb6b83677903",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Model Inference and Interpretation\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting TF binding sites on new DNA sequences. This process is called **inference**.\n",
    "\n",
    "### The Inference Pipeline\n",
    "\n",
    "Our inference pipeline consists of a few key steps:\n",
    "1.  **Load the Model**: We load the best-performing model that was saved during training.\n",
    "2.  **Process the Input**: We take a new DNA sequence and apply the same preprocessing steps we used for our training data (truncating/padding and tokenizing).\n",
    "3.  **Run Prediction**: We feed the processed sequence to the model and get its predictions. We use `torch.no_grad()` to disable gradient calculations, which makes inference faster and uses less memory.\n",
    "4.  **Interpret the Results**: The model's raw output is a set of probabilities. We'll interpret these to make them more understandable, identifying which TFs are predicted to bind and with what level of confidence.\n",
    "\n",
    "To demonstrate, we'll test our model on a few sample sequences and print out a user-friendly summary of the results. This shows how the model can be used in a real-world application to analyze sequences of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b586a10274d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_model = ModelHub.load(\"yangheng/ogb_tfb_finetuned\")\n",
    "\n",
    "sample_sequences = {\n",
    "    \"Random sequence\": \"AGCT\" * (128 // 4),\n",
    "    \"AT-rich sequence\": \"AATT\" * (128 // 4),\n",
    "    \"GC-rich sequence\": \"GCGC\" * (128 // 4),\n",
    "}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq_name, sequence in sample_sequences.items():\n",
    "        outputs = inference_model.inference(sequence)\n",
    "        print(\"âœ… Prediction completed!\")\n",
    "\n",
    "        # â€”â€” ç»“æžœè§£é‡Š â€”â€”\n",
    "        predictions = outputs.get('predictions', None)\n",
    "        probabilities = outputs.get('probabilities', None)\n",
    "        predictions = np.array(predictions)\n",
    "        probabilities = np.array(probabilities) if probabilities is not None else None\n",
    "\n",
    "        positive_count = np.sum(predictions == 1)\n",
    "        total_count = len(predictions)\n",
    "\n",
    "        print(f\"ðŸ“Š Prediction summary:\")\n",
    "        print(f\"  ðŸŽ¯ Total TF binding sites analyzed: {total_count}\")\n",
    "        print(f\"  âœ… Predicted binding sites: {positive_count}\")\n",
    "        print(f\"  ðŸ“ˆ Binding rate: {positive_count/total_count:.1%}\")\n",
    "\n",
    "        if probabilities is not None:\n",
    "            print(f\"\\nðŸ† Top 5 highest confidence predictions:\")\n",
    "            sorted_indices = np.argsort(probabilities)[::-1]\n",
    "\n",
    "            for i, idx in enumerate(sorted_indices[:5]):\n",
    "                tf_id = idx + 1\n",
    "                prediction = \"Binding\" if predictions[idx] == 1 else \"No binding\"\n",
    "                confidence = probabilities[idx]\n",
    "\n",
    "                if confidence > 0.8:\n",
    "                    emoji = \"ðŸ”¥\"\n",
    "                elif confidence > 0.6:\n",
    "                    emoji = \"â­\"\n",
    "                elif confidence > 0.4:\n",
    "                    emoji = \"ðŸ’«\"\n",
    "                else:\n",
    "                    emoji = \"ðŸ’­\"\n",
    "\n",
    "                print(f\"  {emoji} TF-{tf_id:03d}: {prediction} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf11d72370e8e16",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on transcription factor binding prediction with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end MLOps workflow, a critical skill in modern computational biology. Specifically, you have:\n",
    "\n",
    "1.  **Understood the \"Why\"**: Gained an appreciation for the biological problem of TFB prediction and how Genomic Foundation Models provide a powerful solution.\n",
    "\n",
    "2.  **Mastered the 4-Step Workflow**:\n",
    "    -   **Step 1: Data Preparation**: You learned how to acquire, process, and efficiently load a large-scale genomic dataset using the enhanced OmniDataset framework.\n",
    "    -   **Step 2: Model Initialization**: You saw how to leverage a powerful pre-trained model and adapt it for multi-label sequence classification tasks.\n",
    "    -   **Step 3: Model Training**: You implemented a robust training strategy using AccelerateTrainer with proper evaluation metrics and model persistence.\n",
    "    -   **Step 4: Model Inference**: You used your fine-tuned model to make predictions on new DNA sequences and interpreted the results for biological insights.\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. The future of computational biology is in your hands! \n",
    "\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "Now that you've mastered translation efficiency prediction, you can:\n",
    "\n",
    "#### ðŸ§¬ **Explore Other Sequence Classification Tasks**\n",
    "- **Promoter Recognition**: Identify regulatory sequences\n",
    "- **Subcellular Localization**: Predict protein cellular destinations  \n",
    "- **Functional Annotation**: Classify protein or RNA functions\n",
    "\n",
    "#### ðŸ“Š **Try Different Task Types**\n",
    "- **Sequence Regression**: Gene expression level prediction\n",
    "- **Token Classification**: Binding site identification\n",
    "- **Multi-label Classification**: Multi-functional sequence prediction\n",
    "\n",
    "#### ðŸ”¬ **Advanced Techniques**\n",
    "- **Custom Dataset Creation**: Use the [Advanced Dataset Creation Tutorial](./05_advanced_dataset_creation.ipynb)\n",
    "- **Model Comparison**: Benchmark different foundation models\n",
    "- **Hyperparameter Optimization**: Fine-tune model performance\n",
    "- **Biological Validation**: Compare predictions with experimental data\n",
    "\n",
    "### ðŸ“š Resources\n",
    "\n",
    "- **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)**: Review core concepts anytime\n",
    "- **[OmniGenBench Documentation](https://omnigenbench.readthedocs.io/)**: Complete API reference\n",
    "- **[GitHub Repository](https://github.com/yangheng95/OmniGenBench)**: Source code and community discussions\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. \n",
    "\n",
    "**Happy coding and discovering! ðŸ§¬âœ¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090a3b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
