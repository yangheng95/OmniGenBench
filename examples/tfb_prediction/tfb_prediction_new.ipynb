{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning OmniGenome for Transcription Factor Binding Prediction\n",
    "\n",
    "This tutorial provides a comprehensive guide to fine-tuning a pre-trained genomic foundation model, [OmniGenome-52M](https://huggingface.co/yangheng/OmniGenome-52M), for a Transcription Factor Binding (TFB) prediction task. We will use the [DeepSEA](https://www.nature.com/articles/nmeth.3547) dataset and the OmniGenBench library to build a multi-label classifier that predicts the binding sites of 919 different transcription factors from a raw DNA sequence.\n",
    "\n",
    "## Task Overview\n",
    "The goal is to perform multi-label binary classification. Given a DNA sequence, the model must predict for each of the 919 possible chromatin features whether the sequence is a binding site (label 1) or not (label 0).\n",
    "\n",
    "##  Dataset Description\n",
    "The data for this tutorial is a preprocessed version of the DeepSEA dataset, which was originally designed for studying the effects of non-coding genetic variants. It consists of:\n",
    "\n",
    "- Inputs: DNA sequences of 1000 base pairs (bp).\n",
    "\n",
    "- Labels: 919 binary labels corresponding to various chromatin features, including transcription factor binding, DNase I sensitivity, histone marks, etc.\n",
    "\n",
    "We will use a version of this dataset hosted on Hugging Face at [yangheng/tfb_prediction](https://huggingface.co/datasets/yangheng/tfb_prediction).\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "**The total runtime depends on your hardware.**\n",
    "\n",
    "- **Full Run**: On a single NVIDIA RTX 4090 GPU, training with the default settings (MAX_EXAMPLES=100000, EPOCHS=10) takes approximately 1-2 hours.\n",
    "\n",
    "- **Quick Test**: For a quick test run with MAX_EXAMPLES=1000, it should take about 5-10 minutes.\n",
    "\n",
    "## Notebook Structure\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. **Setup & Installation**: Prepares the environment by installing necessary libraries.\n",
    "\n",
    "2. **Import Libraries**: Loads all required Python packages.\n",
    "\n",
    "3. **Configuration**: Centralizes all hyperparameters and paths for easy modification.\n",
    "\n",
    "4. **Model Definition**: Implements a custom PyTorch model that integrates the OmniGenome backbone with a classification head.\n",
    "\n",
    "5. **Data Loading and Preprocessing**: Defines the dataset class and logic for loading and tokenizing the DNA sequences.\n",
    "\n",
    "6. **Initialization**: Instantiates the tokenizer, model, and datasets, preparing them for training.\n",
    "\n",
    "7. **Training the Model**: Fine-tunes the model using the efficient AccelerateTrainer.\n",
    "\n",
    "8. **Evaluation**: Assesses the final model's performance on the test set using the area under the ROC curve (AUC-ROC) metric.\n",
    "\n",
    "9. **Inference Example**: Shows how to use the trained model for predictions on a new DNA sequence.\n",
    "\n",
    "10. **Conclusion**: Summarizes the tutorial and suggests next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Installation\n",
    "\n",
    "Before we begin, you need to prepare your environment. This involves installing Git LFS to download the large dataset files from Hugging Face and then installing the required Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Install and Initialize Git LFS\n",
    "If you don't have Git LFS, you must install it to download the data. Run the appropriate command for your system in a **terminal**, not in this notebook.\n",
    "\n",
    "- **On Debian/Ubuntu**: `sudo apt-get install git-lfs`\n",
    "\n",
    "- **On macOS (with Homebrew)**:`brew install git-lfs`\n",
    "\n",
    "- **On Windows**: Download and run the installer from the [official Git LFS website](https://git-lfs.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Install Python Libraries\n",
    "\n",
    "Now, run the following cell to install the necessary Python packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the necessary packages\n",
    "# !pip install torch numpy transformers omnigenbench autocuda findfile accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Libraries\n",
    "Next, we import the necessary libraries for data manipulation, model building, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import autocuda\n",
    "import findfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from omnigenbench import (\n",
    "    AccelerateTrainer,\n",
    "    ClassificationMetric,\n",
    "    OmniDataset,\n",
    "    OmniLoraModel,\n",
    "    OmniModel,\n",
    "    OmniPooling,\n",
    ")\n",
    "from transformers import AutoModel, AutoTokenizer, BatchEncoding\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Configuration\n",
    "In this section, we will set up all the necessary parameters for our experiment. This includes downloading the data, selecting a model, defining training hyperparameters, and configuring the hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Download and Verify the Dataset\n",
    "First, we define the location of our dataset. The following code will download the `tfb_prediction` dataset from Hugging Face using `git`, extract it if necessary, and then verify that the required data files (`.npy`) are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already found in tfb_prediction_dataset.\n",
      "All data files found successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Data Source and Local Directory ---\n",
    "DATASET_URL = \"https://huggingface.co/datasets/yangheng/tfb_prediction\"\n",
    "LOCAL_DIR = \"tfb_prediction_dataset\"\n",
    "\n",
    "# --- Download and Extract ---\n",
    "if not os.path.isdir(LOCAL_DIR):\n",
    "    print(f\"Cloning dataset from {DATASET_URL} into {LOCAL_DIR}...\")\n",
    "    os.system(f\"git clone {DATASET_URL} {LOCAL_DIR}\")\n",
    "    \n",
    "    zip_path = os.path.join(LOCAL_DIR, \"tfb_dataset.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(LOCAL_DIR)\n",
    "        os.remove(zip_path) # Clean up the zip file\n",
    "else:\n",
    "    print(f\"Dataset already found in {LOCAL_DIR}.\")\n",
    "\n",
    "# --- Define and Verify File Paths ---\n",
    "TRAIN_FILE = os.path.join(LOCAL_DIR, \"train_tfb.npy\")\n",
    "TEST_FILE = os.path.join(LOCAL_DIR, \"test_tfb.npy\")\n",
    "VALID_FILE = os.path.join(LOCAL_DIR, \"valid_tfb.npy\")\n",
    "\n",
    "# Verify that the files exist to prevent errors later\n",
    "if not os.path.exists(TRAIN_FILE):\n",
    "    raise FileNotFoundError(f\"Training file not found at {TRAIN_FILE}. Please check the download step.\")\n",
    "if not os.path.exists(TEST_FILE):\n",
    "    raise FileNotFoundError(f\"Test file not found at {TEST_FILE}.\")\n",
    "if not os.path.exists(VALID_FILE):\n",
    "    print(f\"Warning: Validation file not found at {VALID_FILE}. Skipping validation.\")\n",
    "else:\n",
    "    print(\"All data files found successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Select the Foundation Model\n",
    "Here, you can choose which pre-trained genomic foundation model to fine-tune. We have provided a list of compatible models. To switch models, simply change the index for `AVAILABLE_MODELS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: yangheng/OmniGenome-52M\n"
     ]
    }
   ],
   "source": [
    "# --- Model Configuration ---\n",
    "AVAILABLE_MODELS = [\n",
    "    'yangheng/OmniGenome-52M',      # Default model for this tutorial\n",
    "    'yangheng/OmniGenome-186M',     # A larger OmniGenome model\n",
    "    'DNABERT-2-117M',               # DNABERT-2 by ailab\n",
    "    'InstaDeepAI/nucleotide-transformer-500m-human-ref', # Nucleotide Transformer\n",
    "    # 'DNABERT-2-117M',  # You can add more models here as needed,\n",
    "    # 'LongSafari/hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-500m-human-ref',\n",
    "    # 'multimolecule/rnafm', # RNA-specific models\n",
    "    # 'multimolecule/rnamsm',\n",
    "    # 'multimolecule/rnabert',\n",
    "    # 'SpliceBERT-510nt', # Splice-specific model\n",
    "]\n",
    "\n",
    "# Select the model by its index in the list (0 = OmniGenome-52M)\n",
    "MODEL_NAME_OR_PATH = AVAILABLE_MODELS[0]\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME_OR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Set Training Hyperparameters\n",
    "\n",
    "This step defines the key parameters that control the training process, such as learning rate, batch size, and the number of epochs. You can also set a limit on the number of training examples for a quicker test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 100000 examples for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 10                  # Number of training epochs\n",
    "LEARNING_RATE = 2e-5         # Optimizer learning rate\n",
    "WEIGHT_DECAY = 1e-5          # Weight decay for regularization\n",
    "BATCH_SIZE = 16              # Number of samples per batch\n",
    "PATIENCE = 3                 # Number of epochs with no improvement to wait before stopping\n",
    "MAX_LENGTH = 200             # Sequence length to process (the central 200bp of the 1000bp sequence)\n",
    "SEED = 42                    # Random seed for reproducibility\n",
    "MAX_EXAMPLES = 100000        # Max examples for training/testing. Use a small number (e.g., 1000) for a quick test run.\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 # Accumulates gradients over multiple steps for a larger effective batch size\n",
    "\n",
    "print(f\"Training with {MAX_EXAMPLES} examples for {EPOCHS} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: Configure Hardware Device\n",
    "Finally, we'll set the device for training. The `autocuda` library will automatically select an available NVIDIA GPU if possible, otherwise it will fall back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup ---\n",
    "DEVICE = autocuda.auto_cuda()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Definition\n",
    "Here, we define our custom model, `OmniModelForMultiLabelClassification`. This class inherits from `OmniModel` and wraps a pre-trained Transformer (like OmniGenome) with a classification head suitable for our 919-label task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniModelForMultiLabelClassification defined.\n"
     ]
    }
   ],
   "source": [
    "class OmniModelForMultiLabelClassification(OmniModel):\n",
    "    \"\"\"\n",
    "    Multi-label sequence classification model based on OmniGenome-52M.\n",
    "\n",
    "    This model replaces the original DeepSEA CNN architecture with a pretrained\n",
    "    Transformer encoder from the OmniGenome family. Optionally, convolutional\n",
    "    layers can be stacked on top of the Transformer outputs for additional\n",
    "    feature extraction.\n",
    "\n",
    "    Parameters:\n",
    "        config_or_model (PretrainedConfig or nn.Module):\n",
    "            Configuration or instance of the pretrained Transformer model,\n",
    "            typically obtained via AutoModel.from_pretrained().\n",
    "        tokenizer (PreTrainedTokenizer):\n",
    "            Tokenizer compatible with the Transformer encoder, used to convert\n",
    "            DNA sequences into model inputs.\n",
    "        threshold (float, optional):\n",
    "            Probability threshold for binary decisions in predict(), defaults to 0.5.\n",
    "        use_conv (bool, optional):\n",
    "            If True, apply convolutional layers after the Transformer encoder\n",
    "            for enhanced feature extraction, defaults to False.\n",
    "        *args, **kwargs: Additional arguments passed to the base OmniModel class.\n",
    "\n",
    "    Attributes:\n",
    "        threshold (float):\n",
    "            Probability cutoff for generating binary predictions.\n",
    "        deepsea_classifier (nn.Sequential):\n",
    "            Classification head consisting of a Tanh activation followed by\n",
    "            a linear layer mapping to the number of labels.\n",
    "        loss_fn (nn.BCEWithLogitsLoss):\n",
    "            Binary cross-entropy loss with logits, using pos_weight to balance\n",
    "            positive and negative samples.\n",
    "        pooler (OmniPooling):\n",
    "            Utility for pooling token-level outputs into a sequence-level vector.\n",
    "        sigmoid (nn.Sigmoid):\n",
    "            Activation used to convert logits to probabilities during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        self.threshold = kwargs.pop(\"threshold\", 0.5)\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = \"OmniModelForMultiLabelClassification\"\n",
    "\n",
    "        # Classification head based directly on Transformer outputs\n",
    "        conv_output_dim = self.config.hidden_size\n",
    "        self.deepsea_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(conv_output_dim, self.config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Use pos_weight to address class imbalance in BCEWithLogitsLoss\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0]))\n",
    "        self.pooler = OmniPooling(self.config)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model: encode, pool, classify, and optionally compute loss.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict):\n",
    "                Must contain 'input_ids', 'attention_mask', etc., and optionally 'labels'.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'logits': Tensor of shape (batch_size, num_labels), raw scores before\n",
    "                          sigmoid activation.\n",
    "                'last_hidden_state': Tensor of shape (batch_size, seq_len, hidden_size),\n",
    "                                     the last layer hidden states from the Transformer.\n",
    "                'loss' (optional): Computed BCEWithLogitsLoss if 'labels' provided.\n",
    "            }\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If all provided labels are zero or if logits and labels shapes mismatch.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "\n",
    "        # Encode inputs\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "\n",
    "        # Pooling strategy\n",
    "        if self.pooler._is_causal_lm():\n",
    "            pad_token_id = getattr(self.config, \"pad_token_id\", -100)\n",
    "            sequence_lengths = inputs[\"input_ids\"].ne(pad_token_id).sum(dim=1) - 1\n",
    "            pooled_output = last_hidden_state[\n",
    "                torch.arange(inputs[\"input_ids\"].size(0), device=last_hidden_state.device),\n",
    "                sequence_lengths,\n",
    "            ]\n",
    "        else:\n",
    "            pooled_output = self.pooler(inputs, last_hidden_state)\n",
    "\n",
    "        logits = self.deepsea_classifier(pooled_output)\n",
    "        outputs = {\"logits\": logits, \"last_hidden_state\": last_hidden_state}\n",
    "\n",
    "        if labels is not None:\n",
    "            if torch.sum(labels[labels != -100]) == 0:\n",
    "                raise ValueError(\"Labels cannot be all zeros.\")\n",
    "            labels = labels[labels != -100]\n",
    "            loss = self.loss_fn(logits.view(-1), labels.view(-1).to(torch.float32))\n",
    "            outputs[\"loss\"] = loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform inference on raw sequences or tokenized inputs, returning probabilities and predictions.\n",
    "\n",
    "        Args:\n",
    "            sequence_or_inputs (str, BatchEncoding, or dict):\n",
    "                Raw DNA string or pre-tokenized inputs.\n",
    "            padding (str, optional): Padding strategy for tokenizer, defaults to 'max_length'.\n",
    "            max_length (int, optional): Maximum sequence length, defaults to 1024.\n",
    "            **kwargs: Additional tokenizer arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'predictions': Tensor of binary labels,\n",
    "                'probabilities': Tensor of positive class probabilities,\n",
    "                'logits': Tensor of raw scores,\n",
    "                'last_hidden_state': Transformer outputs.\n",
    "            }\n",
    "        \"\"\"\n",
    "        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(\n",
    "                sequence_or_inputs, dict\n",
    "        ):\n",
    "            inputs = self.tokenizer(\n",
    "                sequence_or_inputs,\n",
    "                padding=kwargs.pop(\"padding\", \"max_length\"),\n",
    "                max_length=kwargs.pop(\"max_length\", 1024),\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            inputs = sequence_or_inputs\n",
    "        inputs = inputs.to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(**inputs)\n",
    "        logits = outputs[\"logits\"]\n",
    "        last_hidden_state = outputs[\"last_hidden_state\"]\n",
    "\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        predictions = (probabilities >= self.threshold).to(torch.int)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": probabilities,\n",
    "            \"logits\": logits,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Compute BCEWithLogitsLoss for multi-label classification.\n",
    "\n",
    "        Args:\n",
    "            logits (Tensor): Raw output scores, shape (batch_size, num_labels).\n",
    "            labels (Tensor): Ground-truth labels as floats, same shape as logits.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If logits and labels shapes do not match.\n",
    "        \"\"\"\n",
    "        valid_labels = labels.to(torch.float32)\n",
    "        if logits.shape != valid_labels.shape:\n",
    "            raise ValueError(f\"Shape mismatch between logits {logits.shape} and labels {valid_labels.shape}\")\n",
    "        return self.loss_fn(logits, valid_labels)\n",
    "\n",
    "\n",
    "print(\"OmniModelForMultiLabelClassification defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Loading and Preprocessing\n",
    "This section defines our `DeepSEADataset` class, which handles loading the `.npy` files and preparing the DNA sequences for the model.\n",
    "\n",
    "**Important Note on Tokenization**: Most genomic foundation models (like DNABERT and OmniGenome) are trained on sequences where each nucleotide is separated by a space (e.g., `\"A T C G\"` instead of `\"ATCG\"`). Our dataset class must perform this conversion before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSEADataset(OmniDataset):\n",
    "    \"\"\"\n",
    "    Dataset designed for the DeepSEA task, handling the conversion from DNA sequences to tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metadata[key] = value\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        \"\"\"\n",
    "        Prepare input data for DeepSEA\n",
    "\n",
    "        Expected instance format:\n",
    "        {\n",
    "            'sequence': DNA sequence string (e.g., \"ATCGATCG...\")\n",
    "            'labels': binary labels as numpy array of shape (919,)\n",
    "        }\n",
    "        \"\"\"\n",
    "        labels = None\n",
    "        if isinstance(instance, str):\n",
    "            sequence = instance\n",
    "        elif isinstance(instance, dict):\n",
    "            sequence = (\n",
    "                instance.get(\"seq\", None)\n",
    "                if \"seq\" in instance\n",
    "                else instance.get(\"sequence\", None)\n",
    "            )\n",
    "            label = instance.get(\"label\", None)\n",
    "            labels = instance.get(\"labels\", None)\n",
    "            labels = labels if labels is not None else label\n",
    "        else:\n",
    "            raise Exception(\"Unknown instance format.\")\n",
    "\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Sequence is required\")\n",
    "\n",
    "        # Convert DNA sequence to space-separated format for tokenizer\n",
    "        # e.g., \"ATCG\" -> \"A T C G\"\n",
    "        if isinstance(sequence, str):\n",
    "            spaced_sequence = ' '.join(list(sequence))\n",
    "        else:\n",
    "            # If sequence is one-hot encoded, convert to string first\n",
    "            if isinstance(sequence, np.ndarray) and sequence.shape[1] == 4:\n",
    "                # one-hot to sequence string\n",
    "                base_map = {0: 'A', 1: 'T', 2: 'C', 3: 'G'}\n",
    "                sequence_str = ''.join([base_map[np.argmax(sequence[i])] for i in range(len(sequence))])\n",
    "                spaced_sequence = ' '.join(list(sequence_str))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported sequence format: {type(sequence)}\")\n",
    "\n",
    "        # Use tokenizer to process the sequence\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            spaced_sequence[500-self.max_length//2:500+self.max_length//2],  # DeepSEA usually processes 200bp sequences\n",
    "            # spaced_sequence,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Squeeze dimensions\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "\n",
    "        if labels is not None:\n",
    "            # For sequence classification, labels should be a fixed-length vector\n",
    "            if isinstance(labels, np.ndarray):\n",
    "                labels = torch.from_numpy(labels).float()\n",
    "            elif not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "        return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Initialization and Data Inspection\n",
    "Now, we instantiate the tokenizer, model, and datasets, bringing all the components together. We will also inspect a sample to see what our processed data looks like before feeding it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Initialize Tokenizer, Model, and Datasets\n",
    "This step loads the pre-trained model and its corresponding tokenizer from Hugging Face, wraps it in our custom `OmniModelForMultiLabelClassification` class, and then creates the `Dataset` objects for our training, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Tokenizer and Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OmniGenomeModel were not initialized from the model checkpoint at yangheng/OmniGenome-52M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: OmniModelForMultiLabelClassification\n",
      "Model Metadata: {'library_name': 'omnigenbench', 'omnigenbench_version': '0.3.7alpha', 'torch_version': '2.7.1+cu126+cu12.6+gite2d141dbde55c2a4370fac5165b0561b6af4798b', 'transformers_version': '4.53.0', 'model_cls': 'OmniModelForMultiLabelClassification', 'tokenizer_cls': 'EsmTokenizer', 'model_name': 'OmniModelForMultiLabelClassification'}\n",
      "Base Model Name: yangheng/OmniGenome-52M\n",
      "Model Type: omnigenome\n",
      "Model Architecture: None\n",
      "Model Parameters: 52.453345 M\n",
      "Model Config: OmniGenomeConfig {\n",
      "  \"OmniGenomefold_config\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_omnigenome.OmniGenomeConfig\",\n",
      "    \"AutoModel\": \"modeling_omnigenome.OmniGenomeModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modeling_omnigenome.OmniGenomeForMaskedLM\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_omnigenome.OmniGenomeForSeq2SeqLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_omnigenome.OmniGenomeForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modeling_omnigenome.OmniGenomeForTokenClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 480,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\",\n",
      "    \"20\": \"20\",\n",
      "    \"21\": \"21\",\n",
      "    \"22\": \"22\",\n",
      "    \"23\": \"23\",\n",
      "    \"24\": \"24\",\n",
      "    \"25\": \"25\",\n",
      "    \"26\": \"26\",\n",
      "    \"27\": \"27\",\n",
      "    \"28\": \"28\",\n",
      "    \"29\": \"29\",\n",
      "    \"30\": \"30\",\n",
      "    \"31\": \"31\",\n",
      "    \"32\": \"32\",\n",
      "    \"33\": \"33\",\n",
      "    \"34\": \"34\",\n",
      "    \"35\": \"35\",\n",
      "    \"36\": \"36\",\n",
      "    \"37\": \"37\",\n",
      "    \"38\": \"38\",\n",
      "    \"39\": \"39\",\n",
      "    \"40\": \"40\",\n",
      "    \"41\": \"41\",\n",
      "    \"42\": \"42\",\n",
      "    \"43\": \"43\",\n",
      "    \"44\": \"44\",\n",
      "    \"45\": \"45\",\n",
      "    \"46\": \"46\",\n",
      "    \"47\": \"47\",\n",
      "    \"48\": \"48\",\n",
      "    \"49\": \"49\",\n",
      "    \"50\": \"50\",\n",
      "    \"51\": \"51\",\n",
      "    \"52\": \"52\",\n",
      "    \"53\": \"53\",\n",
      "    \"54\": \"54\",\n",
      "    \"55\": \"55\",\n",
      "    \"56\": \"56\",\n",
      "    \"57\": \"57\",\n",
      "    \"58\": \"58\",\n",
      "    \"59\": \"59\",\n",
      "    \"60\": \"60\",\n",
      "    \"61\": \"61\",\n",
      "    \"62\": \"62\",\n",
      "    \"63\": \"63\",\n",
      "    \"64\": \"64\",\n",
      "    \"65\": \"65\",\n",
      "    \"66\": \"66\",\n",
      "    \"67\": \"67\",\n",
      "    \"68\": \"68\",\n",
      "    \"69\": \"69\",\n",
      "    \"70\": \"70\",\n",
      "    \"71\": \"71\",\n",
      "    \"72\": \"72\",\n",
      "    \"73\": \"73\",\n",
      "    \"74\": \"74\",\n",
      "    \"75\": \"75\",\n",
      "    \"76\": \"76\",\n",
      "    \"77\": \"77\",\n",
      "    \"78\": \"78\",\n",
      "    \"79\": \"79\",\n",
      "    \"80\": \"80\",\n",
      "    \"81\": \"81\",\n",
      "    \"82\": \"82\",\n",
      "    \"83\": \"83\",\n",
      "    \"84\": \"84\",\n",
      "    \"85\": \"85\",\n",
      "    \"86\": \"86\",\n",
      "    \"87\": \"87\",\n",
      "    \"88\": \"88\",\n",
      "    \"89\": \"89\",\n",
      "    \"90\": \"90\",\n",
      "    \"91\": \"91\",\n",
      "    \"92\": \"92\",\n",
      "    \"93\": \"93\",\n",
      "    \"94\": \"94\",\n",
      "    \"95\": \"95\",\n",
      "    \"96\": \"96\",\n",
      "    \"97\": \"97\",\n",
      "    \"98\": \"98\",\n",
      "    \"99\": \"99\",\n",
      "    \"100\": \"100\",\n",
      "    \"101\": \"101\",\n",
      "    \"102\": \"102\",\n",
      "    \"103\": \"103\",\n",
      "    \"104\": \"104\",\n",
      "    \"105\": \"105\",\n",
      "    \"106\": \"106\",\n",
      "    \"107\": \"107\",\n",
      "    \"108\": \"108\",\n",
      "    \"109\": \"109\",\n",
      "    \"110\": \"110\",\n",
      "    \"111\": \"111\",\n",
      "    \"112\": \"112\",\n",
      "    \"113\": \"113\",\n",
      "    \"114\": \"114\",\n",
      "    \"115\": \"115\",\n",
      "    \"116\": \"116\",\n",
      "    \"117\": \"117\",\n",
      "    \"118\": \"118\",\n",
      "    \"119\": \"119\",\n",
      "    \"120\": \"120\",\n",
      "    \"121\": \"121\",\n",
      "    \"122\": \"122\",\n",
      "    \"123\": \"123\",\n",
      "    \"124\": \"124\",\n",
      "    \"125\": \"125\",\n",
      "    \"126\": \"126\",\n",
      "    \"127\": \"127\",\n",
      "    \"128\": \"128\",\n",
      "    \"129\": \"129\",\n",
      "    \"130\": \"130\",\n",
      "    \"131\": \"131\",\n",
      "    \"132\": \"132\",\n",
      "    \"133\": \"133\",\n",
      "    \"134\": \"134\",\n",
      "    \"135\": \"135\",\n",
      "    \"136\": \"136\",\n",
      "    \"137\": \"137\",\n",
      "    \"138\": \"138\",\n",
      "    \"139\": \"139\",\n",
      "    \"140\": \"140\",\n",
      "    \"141\": \"141\",\n",
      "    \"142\": \"142\",\n",
      "    \"143\": \"143\",\n",
      "    \"144\": \"144\",\n",
      "    \"145\": \"145\",\n",
      "    \"146\": \"146\",\n",
      "    \"147\": \"147\",\n",
      "    \"148\": \"148\",\n",
      "    \"149\": \"149\",\n",
      "    \"150\": \"150\",\n",
      "    \"151\": \"151\",\n",
      "    \"152\": \"152\",\n",
      "    \"153\": \"153\",\n",
      "    \"154\": \"154\",\n",
      "    \"155\": \"155\",\n",
      "    \"156\": \"156\",\n",
      "    \"157\": \"157\",\n",
      "    \"158\": \"158\",\n",
      "    \"159\": \"159\",\n",
      "    \"160\": \"160\",\n",
      "    \"161\": \"161\",\n",
      "    \"162\": \"162\",\n",
      "    \"163\": \"163\",\n",
      "    \"164\": \"164\",\n",
      "    \"165\": \"165\",\n",
      "    \"166\": \"166\",\n",
      "    \"167\": \"167\",\n",
      "    \"168\": \"168\",\n",
      "    \"169\": \"169\",\n",
      "    \"170\": \"170\",\n",
      "    \"171\": \"171\",\n",
      "    \"172\": \"172\",\n",
      "    \"173\": \"173\",\n",
      "    \"174\": \"174\",\n",
      "    \"175\": \"175\",\n",
      "    \"176\": \"176\",\n",
      "    \"177\": \"177\",\n",
      "    \"178\": \"178\",\n",
      "    \"179\": \"179\",\n",
      "    \"180\": \"180\",\n",
      "    \"181\": \"181\",\n",
      "    \"182\": \"182\",\n",
      "    \"183\": \"183\",\n",
      "    \"184\": \"184\",\n",
      "    \"185\": \"185\",\n",
      "    \"186\": \"186\",\n",
      "    \"187\": \"187\",\n",
      "    \"188\": \"188\",\n",
      "    \"189\": \"189\",\n",
      "    \"190\": \"190\",\n",
      "    \"191\": \"191\",\n",
      "    \"192\": \"192\",\n",
      "    \"193\": \"193\",\n",
      "    \"194\": \"194\",\n",
      "    \"195\": \"195\",\n",
      "    \"196\": \"196\",\n",
      "    \"197\": \"197\",\n",
      "    \"198\": \"198\",\n",
      "    \"199\": \"199\",\n",
      "    \"200\": \"200\",\n",
      "    \"201\": \"201\",\n",
      "    \"202\": \"202\",\n",
      "    \"203\": \"203\",\n",
      "    \"204\": \"204\",\n",
      "    \"205\": \"205\",\n",
      "    \"206\": \"206\",\n",
      "    \"207\": \"207\",\n",
      "    \"208\": \"208\",\n",
      "    \"209\": \"209\",\n",
      "    \"210\": \"210\",\n",
      "    \"211\": \"211\",\n",
      "    \"212\": \"212\",\n",
      "    \"213\": \"213\",\n",
      "    \"214\": \"214\",\n",
      "    \"215\": \"215\",\n",
      "    \"216\": \"216\",\n",
      "    \"217\": \"217\",\n",
      "    \"218\": \"218\",\n",
      "    \"219\": \"219\",\n",
      "    \"220\": \"220\",\n",
      "    \"221\": \"221\",\n",
      "    \"222\": \"222\",\n",
      "    \"223\": \"223\",\n",
      "    \"224\": \"224\",\n",
      "    \"225\": \"225\",\n",
      "    \"226\": \"226\",\n",
      "    \"227\": \"227\",\n",
      "    \"228\": \"228\",\n",
      "    \"229\": \"229\",\n",
      "    \"230\": \"230\",\n",
      "    \"231\": \"231\",\n",
      "    \"232\": \"232\",\n",
      "    \"233\": \"233\",\n",
      "    \"234\": \"234\",\n",
      "    \"235\": \"235\",\n",
      "    \"236\": \"236\",\n",
      "    \"237\": \"237\",\n",
      "    \"238\": \"238\",\n",
      "    \"239\": \"239\",\n",
      "    \"240\": \"240\",\n",
      "    \"241\": \"241\",\n",
      "    \"242\": \"242\",\n",
      "    \"243\": \"243\",\n",
      "    \"244\": \"244\",\n",
      "    \"245\": \"245\",\n",
      "    \"246\": \"246\",\n",
      "    \"247\": \"247\",\n",
      "    \"248\": \"248\",\n",
      "    \"249\": \"249\",\n",
      "    \"250\": \"250\",\n",
      "    \"251\": \"251\",\n",
      "    \"252\": \"252\",\n",
      "    \"253\": \"253\",\n",
      "    \"254\": \"254\",\n",
      "    \"255\": \"255\",\n",
      "    \"256\": \"256\",\n",
      "    \"257\": \"257\",\n",
      "    \"258\": \"258\",\n",
      "    \"259\": \"259\",\n",
      "    \"260\": \"260\",\n",
      "    \"261\": \"261\",\n",
      "    \"262\": \"262\",\n",
      "    \"263\": \"263\",\n",
      "    \"264\": \"264\",\n",
      "    \"265\": \"265\",\n",
      "    \"266\": \"266\",\n",
      "    \"267\": \"267\",\n",
      "    \"268\": \"268\",\n",
      "    \"269\": \"269\",\n",
      "    \"270\": \"270\",\n",
      "    \"271\": \"271\",\n",
      "    \"272\": \"272\",\n",
      "    \"273\": \"273\",\n",
      "    \"274\": \"274\",\n",
      "    \"275\": \"275\",\n",
      "    \"276\": \"276\",\n",
      "    \"277\": \"277\",\n",
      "    \"278\": \"278\",\n",
      "    \"279\": \"279\",\n",
      "    \"280\": \"280\",\n",
      "    \"281\": \"281\",\n",
      "    \"282\": \"282\",\n",
      "    \"283\": \"283\",\n",
      "    \"284\": \"284\",\n",
      "    \"285\": \"285\",\n",
      "    \"286\": \"286\",\n",
      "    \"287\": \"287\",\n",
      "    \"288\": \"288\",\n",
      "    \"289\": \"289\",\n",
      "    \"290\": \"290\",\n",
      "    \"291\": \"291\",\n",
      "    \"292\": \"292\",\n",
      "    \"293\": \"293\",\n",
      "    \"294\": \"294\",\n",
      "    \"295\": \"295\",\n",
      "    \"296\": \"296\",\n",
      "    \"297\": \"297\",\n",
      "    \"298\": \"298\",\n",
      "    \"299\": \"299\",\n",
      "    \"300\": \"300\",\n",
      "    \"301\": \"301\",\n",
      "    \"302\": \"302\",\n",
      "    \"303\": \"303\",\n",
      "    \"304\": \"304\",\n",
      "    \"305\": \"305\",\n",
      "    \"306\": \"306\",\n",
      "    \"307\": \"307\",\n",
      "    \"308\": \"308\",\n",
      "    \"309\": \"309\",\n",
      "    \"310\": \"310\",\n",
      "    \"311\": \"311\",\n",
      "    \"312\": \"312\",\n",
      "    \"313\": \"313\",\n",
      "    \"314\": \"314\",\n",
      "    \"315\": \"315\",\n",
      "    \"316\": \"316\",\n",
      "    \"317\": \"317\",\n",
      "    \"318\": \"318\",\n",
      "    \"319\": \"319\",\n",
      "    \"320\": \"320\",\n",
      "    \"321\": \"321\",\n",
      "    \"322\": \"322\",\n",
      "    \"323\": \"323\",\n",
      "    \"324\": \"324\",\n",
      "    \"325\": \"325\",\n",
      "    \"326\": \"326\",\n",
      "    \"327\": \"327\",\n",
      "    \"328\": \"328\",\n",
      "    \"329\": \"329\",\n",
      "    \"330\": \"330\",\n",
      "    \"331\": \"331\",\n",
      "    \"332\": \"332\",\n",
      "    \"333\": \"333\",\n",
      "    \"334\": \"334\",\n",
      "    \"335\": \"335\",\n",
      "    \"336\": \"336\",\n",
      "    \"337\": \"337\",\n",
      "    \"338\": \"338\",\n",
      "    \"339\": \"339\",\n",
      "    \"340\": \"340\",\n",
      "    \"341\": \"341\",\n",
      "    \"342\": \"342\",\n",
      "    \"343\": \"343\",\n",
      "    \"344\": \"344\",\n",
      "    \"345\": \"345\",\n",
      "    \"346\": \"346\",\n",
      "    \"347\": \"347\",\n",
      "    \"348\": \"348\",\n",
      "    \"349\": \"349\",\n",
      "    \"350\": \"350\",\n",
      "    \"351\": \"351\",\n",
      "    \"352\": \"352\",\n",
      "    \"353\": \"353\",\n",
      "    \"354\": \"354\",\n",
      "    \"355\": \"355\",\n",
      "    \"356\": \"356\",\n",
      "    \"357\": \"357\",\n",
      "    \"358\": \"358\",\n",
      "    \"359\": \"359\",\n",
      "    \"360\": \"360\",\n",
      "    \"361\": \"361\",\n",
      "    \"362\": \"362\",\n",
      "    \"363\": \"363\",\n",
      "    \"364\": \"364\",\n",
      "    \"365\": \"365\",\n",
      "    \"366\": \"366\",\n",
      "    \"367\": \"367\",\n",
      "    \"368\": \"368\",\n",
      "    \"369\": \"369\",\n",
      "    \"370\": \"370\",\n",
      "    \"371\": \"371\",\n",
      "    \"372\": \"372\",\n",
      "    \"373\": \"373\",\n",
      "    \"374\": \"374\",\n",
      "    \"375\": \"375\",\n",
      "    \"376\": \"376\",\n",
      "    \"377\": \"377\",\n",
      "    \"378\": \"378\",\n",
      "    \"379\": \"379\",\n",
      "    \"380\": \"380\",\n",
      "    \"381\": \"381\",\n",
      "    \"382\": \"382\",\n",
      "    \"383\": \"383\",\n",
      "    \"384\": \"384\",\n",
      "    \"385\": \"385\",\n",
      "    \"386\": \"386\",\n",
      "    \"387\": \"387\",\n",
      "    \"388\": \"388\",\n",
      "    \"389\": \"389\",\n",
      "    \"390\": \"390\",\n",
      "    \"391\": \"391\",\n",
      "    \"392\": \"392\",\n",
      "    \"393\": \"393\",\n",
      "    \"394\": \"394\",\n",
      "    \"395\": \"395\",\n",
      "    \"396\": \"396\",\n",
      "    \"397\": \"397\",\n",
      "    \"398\": \"398\",\n",
      "    \"399\": \"399\",\n",
      "    \"400\": \"400\",\n",
      "    \"401\": \"401\",\n",
      "    \"402\": \"402\",\n",
      "    \"403\": \"403\",\n",
      "    \"404\": \"404\",\n",
      "    \"405\": \"405\",\n",
      "    \"406\": \"406\",\n",
      "    \"407\": \"407\",\n",
      "    \"408\": \"408\",\n",
      "    \"409\": \"409\",\n",
      "    \"410\": \"410\",\n",
      "    \"411\": \"411\",\n",
      "    \"412\": \"412\",\n",
      "    \"413\": \"413\",\n",
      "    \"414\": \"414\",\n",
      "    \"415\": \"415\",\n",
      "    \"416\": \"416\",\n",
      "    \"417\": \"417\",\n",
      "    \"418\": \"418\",\n",
      "    \"419\": \"419\",\n",
      "    \"420\": \"420\",\n",
      "    \"421\": \"421\",\n",
      "    \"422\": \"422\",\n",
      "    \"423\": \"423\",\n",
      "    \"424\": \"424\",\n",
      "    \"425\": \"425\",\n",
      "    \"426\": \"426\",\n",
      "    \"427\": \"427\",\n",
      "    \"428\": \"428\",\n",
      "    \"429\": \"429\",\n",
      "    \"430\": \"430\",\n",
      "    \"431\": \"431\",\n",
      "    \"432\": \"432\",\n",
      "    \"433\": \"433\",\n",
      "    \"434\": \"434\",\n",
      "    \"435\": \"435\",\n",
      "    \"436\": \"436\",\n",
      "    \"437\": \"437\",\n",
      "    \"438\": \"438\",\n",
      "    \"439\": \"439\",\n",
      "    \"440\": \"440\",\n",
      "    \"441\": \"441\",\n",
      "    \"442\": \"442\",\n",
      "    \"443\": \"443\",\n",
      "    \"444\": \"444\",\n",
      "    \"445\": \"445\",\n",
      "    \"446\": \"446\",\n",
      "    \"447\": \"447\",\n",
      "    \"448\": \"448\",\n",
      "    \"449\": \"449\",\n",
      "    \"450\": \"450\",\n",
      "    \"451\": \"451\",\n",
      "    \"452\": \"452\",\n",
      "    \"453\": \"453\",\n",
      "    \"454\": \"454\",\n",
      "    \"455\": \"455\",\n",
      "    \"456\": \"456\",\n",
      "    \"457\": \"457\",\n",
      "    \"458\": \"458\",\n",
      "    \"459\": \"459\",\n",
      "    \"460\": \"460\",\n",
      "    \"461\": \"461\",\n",
      "    \"462\": \"462\",\n",
      "    \"463\": \"463\",\n",
      "    \"464\": \"464\",\n",
      "    \"465\": \"465\",\n",
      "    \"466\": \"466\",\n",
      "    \"467\": \"467\",\n",
      "    \"468\": \"468\",\n",
      "    \"469\": \"469\",\n",
      "    \"470\": \"470\",\n",
      "    \"471\": \"471\",\n",
      "    \"472\": \"472\",\n",
      "    \"473\": \"473\",\n",
      "    \"474\": \"474\",\n",
      "    \"475\": \"475\",\n",
      "    \"476\": \"476\",\n",
      "    \"477\": \"477\",\n",
      "    \"478\": \"478\",\n",
      "    \"479\": \"479\",\n",
      "    \"480\": \"480\",\n",
      "    \"481\": \"481\",\n",
      "    \"482\": \"482\",\n",
      "    \"483\": \"483\",\n",
      "    \"484\": \"484\",\n",
      "    \"485\": \"485\",\n",
      "    \"486\": \"486\",\n",
      "    \"487\": \"487\",\n",
      "    \"488\": \"488\",\n",
      "    \"489\": \"489\",\n",
      "    \"490\": \"490\",\n",
      "    \"491\": \"491\",\n",
      "    \"492\": \"492\",\n",
      "    \"493\": \"493\",\n",
      "    \"494\": \"494\",\n",
      "    \"495\": \"495\",\n",
      "    \"496\": \"496\",\n",
      "    \"497\": \"497\",\n",
      "    \"498\": \"498\",\n",
      "    \"499\": \"499\",\n",
      "    \"500\": \"500\",\n",
      "    \"501\": \"501\",\n",
      "    \"502\": \"502\",\n",
      "    \"503\": \"503\",\n",
      "    \"504\": \"504\",\n",
      "    \"505\": \"505\",\n",
      "    \"506\": \"506\",\n",
      "    \"507\": \"507\",\n",
      "    \"508\": \"508\",\n",
      "    \"509\": \"509\",\n",
      "    \"510\": \"510\",\n",
      "    \"511\": \"511\",\n",
      "    \"512\": \"512\",\n",
      "    \"513\": \"513\",\n",
      "    \"514\": \"514\",\n",
      "    \"515\": \"515\",\n",
      "    \"516\": \"516\",\n",
      "    \"517\": \"517\",\n",
      "    \"518\": \"518\",\n",
      "    \"519\": \"519\",\n",
      "    \"520\": \"520\",\n",
      "    \"521\": \"521\",\n",
      "    \"522\": \"522\",\n",
      "    \"523\": \"523\",\n",
      "    \"524\": \"524\",\n",
      "    \"525\": \"525\",\n",
      "    \"526\": \"526\",\n",
      "    \"527\": \"527\",\n",
      "    \"528\": \"528\",\n",
      "    \"529\": \"529\",\n",
      "    \"530\": \"530\",\n",
      "    \"531\": \"531\",\n",
      "    \"532\": \"532\",\n",
      "    \"533\": \"533\",\n",
      "    \"534\": \"534\",\n",
      "    \"535\": \"535\",\n",
      "    \"536\": \"536\",\n",
      "    \"537\": \"537\",\n",
      "    \"538\": \"538\",\n",
      "    \"539\": \"539\",\n",
      "    \"540\": \"540\",\n",
      "    \"541\": \"541\",\n",
      "    \"542\": \"542\",\n",
      "    \"543\": \"543\",\n",
      "    \"544\": \"544\",\n",
      "    \"545\": \"545\",\n",
      "    \"546\": \"546\",\n",
      "    \"547\": \"547\",\n",
      "    \"548\": \"548\",\n",
      "    \"549\": \"549\",\n",
      "    \"550\": \"550\",\n",
      "    \"551\": \"551\",\n",
      "    \"552\": \"552\",\n",
      "    \"553\": \"553\",\n",
      "    \"554\": \"554\",\n",
      "    \"555\": \"555\",\n",
      "    \"556\": \"556\",\n",
      "    \"557\": \"557\",\n",
      "    \"558\": \"558\",\n",
      "    \"559\": \"559\",\n",
      "    \"560\": \"560\",\n",
      "    \"561\": \"561\",\n",
      "    \"562\": \"562\",\n",
      "    \"563\": \"563\",\n",
      "    \"564\": \"564\",\n",
      "    \"565\": \"565\",\n",
      "    \"566\": \"566\",\n",
      "    \"567\": \"567\",\n",
      "    \"568\": \"568\",\n",
      "    \"569\": \"569\",\n",
      "    \"570\": \"570\",\n",
      "    \"571\": \"571\",\n",
      "    \"572\": \"572\",\n",
      "    \"573\": \"573\",\n",
      "    \"574\": \"574\",\n",
      "    \"575\": \"575\",\n",
      "    \"576\": \"576\",\n",
      "    \"577\": \"577\",\n",
      "    \"578\": \"578\",\n",
      "    \"579\": \"579\",\n",
      "    \"580\": \"580\",\n",
      "    \"581\": \"581\",\n",
      "    \"582\": \"582\",\n",
      "    \"583\": \"583\",\n",
      "    \"584\": \"584\",\n",
      "    \"585\": \"585\",\n",
      "    \"586\": \"586\",\n",
      "    \"587\": \"587\",\n",
      "    \"588\": \"588\",\n",
      "    \"589\": \"589\",\n",
      "    \"590\": \"590\",\n",
      "    \"591\": \"591\",\n",
      "    \"592\": \"592\",\n",
      "    \"593\": \"593\",\n",
      "    \"594\": \"594\",\n",
      "    \"595\": \"595\",\n",
      "    \"596\": \"596\",\n",
      "    \"597\": \"597\",\n",
      "    \"598\": \"598\",\n",
      "    \"599\": \"599\",\n",
      "    \"600\": \"600\",\n",
      "    \"601\": \"601\",\n",
      "    \"602\": \"602\",\n",
      "    \"603\": \"603\",\n",
      "    \"604\": \"604\",\n",
      "    \"605\": \"605\",\n",
      "    \"606\": \"606\",\n",
      "    \"607\": \"607\",\n",
      "    \"608\": \"608\",\n",
      "    \"609\": \"609\",\n",
      "    \"610\": \"610\",\n",
      "    \"611\": \"611\",\n",
      "    \"612\": \"612\",\n",
      "    \"613\": \"613\",\n",
      "    \"614\": \"614\",\n",
      "    \"615\": \"615\",\n",
      "    \"616\": \"616\",\n",
      "    \"617\": \"617\",\n",
      "    \"618\": \"618\",\n",
      "    \"619\": \"619\",\n",
      "    \"620\": \"620\",\n",
      "    \"621\": \"621\",\n",
      "    \"622\": \"622\",\n",
      "    \"623\": \"623\",\n",
      "    \"624\": \"624\",\n",
      "    \"625\": \"625\",\n",
      "    \"626\": \"626\",\n",
      "    \"627\": \"627\",\n",
      "    \"628\": \"628\",\n",
      "    \"629\": \"629\",\n",
      "    \"630\": \"630\",\n",
      "    \"631\": \"631\",\n",
      "    \"632\": \"632\",\n",
      "    \"633\": \"633\",\n",
      "    \"634\": \"634\",\n",
      "    \"635\": \"635\",\n",
      "    \"636\": \"636\",\n",
      "    \"637\": \"637\",\n",
      "    \"638\": \"638\",\n",
      "    \"639\": \"639\",\n",
      "    \"640\": \"640\",\n",
      "    \"641\": \"641\",\n",
      "    \"642\": \"642\",\n",
      "    \"643\": \"643\",\n",
      "    \"644\": \"644\",\n",
      "    \"645\": \"645\",\n",
      "    \"646\": \"646\",\n",
      "    \"647\": \"647\",\n",
      "    \"648\": \"648\",\n",
      "    \"649\": \"649\",\n",
      "    \"650\": \"650\",\n",
      "    \"651\": \"651\",\n",
      "    \"652\": \"652\",\n",
      "    \"653\": \"653\",\n",
      "    \"654\": \"654\",\n",
      "    \"655\": \"655\",\n",
      "    \"656\": \"656\",\n",
      "    \"657\": \"657\",\n",
      "    \"658\": \"658\",\n",
      "    \"659\": \"659\",\n",
      "    \"660\": \"660\",\n",
      "    \"661\": \"661\",\n",
      "    \"662\": \"662\",\n",
      "    \"663\": \"663\",\n",
      "    \"664\": \"664\",\n",
      "    \"665\": \"665\",\n",
      "    \"666\": \"666\",\n",
      "    \"667\": \"667\",\n",
      "    \"668\": \"668\",\n",
      "    \"669\": \"669\",\n",
      "    \"670\": \"670\",\n",
      "    \"671\": \"671\",\n",
      "    \"672\": \"672\",\n",
      "    \"673\": \"673\",\n",
      "    \"674\": \"674\",\n",
      "    \"675\": \"675\",\n",
      "    \"676\": \"676\",\n",
      "    \"677\": \"677\",\n",
      "    \"678\": \"678\",\n",
      "    \"679\": \"679\",\n",
      "    \"680\": \"680\",\n",
      "    \"681\": \"681\",\n",
      "    \"682\": \"682\",\n",
      "    \"683\": \"683\",\n",
      "    \"684\": \"684\",\n",
      "    \"685\": \"685\",\n",
      "    \"686\": \"686\",\n",
      "    \"687\": \"687\",\n",
      "    \"688\": \"688\",\n",
      "    \"689\": \"689\",\n",
      "    \"690\": \"690\",\n",
      "    \"691\": \"691\",\n",
      "    \"692\": \"692\",\n",
      "    \"693\": \"693\",\n",
      "    \"694\": \"694\",\n",
      "    \"695\": \"695\",\n",
      "    \"696\": \"696\",\n",
      "    \"697\": \"697\",\n",
      "    \"698\": \"698\",\n",
      "    \"699\": \"699\",\n",
      "    \"700\": \"700\",\n",
      "    \"701\": \"701\",\n",
      "    \"702\": \"702\",\n",
      "    \"703\": \"703\",\n",
      "    \"704\": \"704\",\n",
      "    \"705\": \"705\",\n",
      "    \"706\": \"706\",\n",
      "    \"707\": \"707\",\n",
      "    \"708\": \"708\",\n",
      "    \"709\": \"709\",\n",
      "    \"710\": \"710\",\n",
      "    \"711\": \"711\",\n",
      "    \"712\": \"712\",\n",
      "    \"713\": \"713\",\n",
      "    \"714\": \"714\",\n",
      "    \"715\": \"715\",\n",
      "    \"716\": \"716\",\n",
      "    \"717\": \"717\",\n",
      "    \"718\": \"718\",\n",
      "    \"719\": \"719\",\n",
      "    \"720\": \"720\",\n",
      "    \"721\": \"721\",\n",
      "    \"722\": \"722\",\n",
      "    \"723\": \"723\",\n",
      "    \"724\": \"724\",\n",
      "    \"725\": \"725\",\n",
      "    \"726\": \"726\",\n",
      "    \"727\": \"727\",\n",
      "    \"728\": \"728\",\n",
      "    \"729\": \"729\",\n",
      "    \"730\": \"730\",\n",
      "    \"731\": \"731\",\n",
      "    \"732\": \"732\",\n",
      "    \"733\": \"733\",\n",
      "    \"734\": \"734\",\n",
      "    \"735\": \"735\",\n",
      "    \"736\": \"736\",\n",
      "    \"737\": \"737\",\n",
      "    \"738\": \"738\",\n",
      "    \"739\": \"739\",\n",
      "    \"740\": \"740\",\n",
      "    \"741\": \"741\",\n",
      "    \"742\": \"742\",\n",
      "    \"743\": \"743\",\n",
      "    \"744\": \"744\",\n",
      "    \"745\": \"745\",\n",
      "    \"746\": \"746\",\n",
      "    \"747\": \"747\",\n",
      "    \"748\": \"748\",\n",
      "    \"749\": \"749\",\n",
      "    \"750\": \"750\",\n",
      "    \"751\": \"751\",\n",
      "    \"752\": \"752\",\n",
      "    \"753\": \"753\",\n",
      "    \"754\": \"754\",\n",
      "    \"755\": \"755\",\n",
      "    \"756\": \"756\",\n",
      "    \"757\": \"757\",\n",
      "    \"758\": \"758\",\n",
      "    \"759\": \"759\",\n",
      "    \"760\": \"760\",\n",
      "    \"761\": \"761\",\n",
      "    \"762\": \"762\",\n",
      "    \"763\": \"763\",\n",
      "    \"764\": \"764\",\n",
      "    \"765\": \"765\",\n",
      "    \"766\": \"766\",\n",
      "    \"767\": \"767\",\n",
      "    \"768\": \"768\",\n",
      "    \"769\": \"769\",\n",
      "    \"770\": \"770\",\n",
      "    \"771\": \"771\",\n",
      "    \"772\": \"772\",\n",
      "    \"773\": \"773\",\n",
      "    \"774\": \"774\",\n",
      "    \"775\": \"775\",\n",
      "    \"776\": \"776\",\n",
      "    \"777\": \"777\",\n",
      "    \"778\": \"778\",\n",
      "    \"779\": \"779\",\n",
      "    \"780\": \"780\",\n",
      "    \"781\": \"781\",\n",
      "    \"782\": \"782\",\n",
      "    \"783\": \"783\",\n",
      "    \"784\": \"784\",\n",
      "    \"785\": \"785\",\n",
      "    \"786\": \"786\",\n",
      "    \"787\": \"787\",\n",
      "    \"788\": \"788\",\n",
      "    \"789\": \"789\",\n",
      "    \"790\": \"790\",\n",
      "    \"791\": \"791\",\n",
      "    \"792\": \"792\",\n",
      "    \"793\": \"793\",\n",
      "    \"794\": \"794\",\n",
      "    \"795\": \"795\",\n",
      "    \"796\": \"796\",\n",
      "    \"797\": \"797\",\n",
      "    \"798\": \"798\",\n",
      "    \"799\": \"799\",\n",
      "    \"800\": \"800\",\n",
      "    \"801\": \"801\",\n",
      "    \"802\": \"802\",\n",
      "    \"803\": \"803\",\n",
      "    \"804\": \"804\",\n",
      "    \"805\": \"805\",\n",
      "    \"806\": \"806\",\n",
      "    \"807\": \"807\",\n",
      "    \"808\": \"808\",\n",
      "    \"809\": \"809\",\n",
      "    \"810\": \"810\",\n",
      "    \"811\": \"811\",\n",
      "    \"812\": \"812\",\n",
      "    \"813\": \"813\",\n",
      "    \"814\": \"814\",\n",
      "    \"815\": \"815\",\n",
      "    \"816\": \"816\",\n",
      "    \"817\": \"817\",\n",
      "    \"818\": \"818\",\n",
      "    \"819\": \"819\",\n",
      "    \"820\": \"820\",\n",
      "    \"821\": \"821\",\n",
      "    \"822\": \"822\",\n",
      "    \"823\": \"823\",\n",
      "    \"824\": \"824\",\n",
      "    \"825\": \"825\",\n",
      "    \"826\": \"826\",\n",
      "    \"827\": \"827\",\n",
      "    \"828\": \"828\",\n",
      "    \"829\": \"829\",\n",
      "    \"830\": \"830\",\n",
      "    \"831\": \"831\",\n",
      "    \"832\": \"832\",\n",
      "    \"833\": \"833\",\n",
      "    \"834\": \"834\",\n",
      "    \"835\": \"835\",\n",
      "    \"836\": \"836\",\n",
      "    \"837\": \"837\",\n",
      "    \"838\": \"838\",\n",
      "    \"839\": \"839\",\n",
      "    \"840\": \"840\",\n",
      "    \"841\": \"841\",\n",
      "    \"842\": \"842\",\n",
      "    \"843\": \"843\",\n",
      "    \"844\": \"844\",\n",
      "    \"845\": \"845\",\n",
      "    \"846\": \"846\",\n",
      "    \"847\": \"847\",\n",
      "    \"848\": \"848\",\n",
      "    \"849\": \"849\",\n",
      "    \"850\": \"850\",\n",
      "    \"851\": \"851\",\n",
      "    \"852\": \"852\",\n",
      "    \"853\": \"853\",\n",
      "    \"854\": \"854\",\n",
      "    \"855\": \"855\",\n",
      "    \"856\": \"856\",\n",
      "    \"857\": \"857\",\n",
      "    \"858\": \"858\",\n",
      "    \"859\": \"859\",\n",
      "    \"860\": \"860\",\n",
      "    \"861\": \"861\",\n",
      "    \"862\": \"862\",\n",
      "    \"863\": \"863\",\n",
      "    \"864\": \"864\",\n",
      "    \"865\": \"865\",\n",
      "    \"866\": \"866\",\n",
      "    \"867\": \"867\",\n",
      "    \"868\": \"868\",\n",
      "    \"869\": \"869\",\n",
      "    \"870\": \"870\",\n",
      "    \"871\": \"871\",\n",
      "    \"872\": \"872\",\n",
      "    \"873\": \"873\",\n",
      "    \"874\": \"874\",\n",
      "    \"875\": \"875\",\n",
      "    \"876\": \"876\",\n",
      "    \"877\": \"877\",\n",
      "    \"878\": \"878\",\n",
      "    \"879\": \"879\",\n",
      "    \"880\": \"880\",\n",
      "    \"881\": \"881\",\n",
      "    \"882\": \"882\",\n",
      "    \"883\": \"883\",\n",
      "    \"884\": \"884\",\n",
      "    \"885\": \"885\",\n",
      "    \"886\": \"886\",\n",
      "    \"887\": \"887\",\n",
      "    \"888\": \"888\",\n",
      "    \"889\": \"889\",\n",
      "    \"890\": \"890\",\n",
      "    \"891\": \"891\",\n",
      "    \"892\": \"892\",\n",
      "    \"893\": \"893\",\n",
      "    \"894\": \"894\",\n",
      "    \"895\": \"895\",\n",
      "    \"896\": \"896\",\n",
      "    \"897\": \"897\",\n",
      "    \"898\": \"898\",\n",
      "    \"899\": \"899\",\n",
      "    \"900\": \"900\",\n",
      "    \"901\": \"901\",\n",
      "    \"902\": \"902\",\n",
      "    \"903\": \"903\",\n",
      "    \"904\": \"904\",\n",
      "    \"905\": \"905\",\n",
      "    \"906\": \"906\",\n",
      "    \"907\": \"907\",\n",
      "    \"908\": \"908\",\n",
      "    \"909\": \"909\",\n",
      "    \"910\": \"910\",\n",
      "    \"911\": \"911\",\n",
      "    \"912\": \"912\",\n",
      "    \"913\": \"913\",\n",
      "    \"914\": \"914\",\n",
      "    \"915\": \"915\",\n",
      "    \"916\": \"916\",\n",
      "    \"917\": \"917\",\n",
      "    \"918\": \"918\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2400,\n",
      "  \"is_folding_model\": false,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"100\": 100,\n",
      "    \"101\": 101,\n",
      "    \"102\": 102,\n",
      "    \"103\": 103,\n",
      "    \"104\": 104,\n",
      "    \"105\": 105,\n",
      "    \"106\": 106,\n",
      "    \"107\": 107,\n",
      "    \"108\": 108,\n",
      "    \"109\": 109,\n",
      "    \"11\": 11,\n",
      "    \"110\": 110,\n",
      "    \"111\": 111,\n",
      "    \"112\": 112,\n",
      "    \"113\": 113,\n",
      "    \"114\": 114,\n",
      "    \"115\": 115,\n",
      "    \"116\": 116,\n",
      "    \"117\": 117,\n",
      "    \"118\": 118,\n",
      "    \"119\": 119,\n",
      "    \"12\": 12,\n",
      "    \"120\": 120,\n",
      "    \"121\": 121,\n",
      "    \"122\": 122,\n",
      "    \"123\": 123,\n",
      "    \"124\": 124,\n",
      "    \"125\": 125,\n",
      "    \"126\": 126,\n",
      "    \"127\": 127,\n",
      "    \"128\": 128,\n",
      "    \"129\": 129,\n",
      "    \"13\": 13,\n",
      "    \"130\": 130,\n",
      "    \"131\": 131,\n",
      "    \"132\": 132,\n",
      "    \"133\": 133,\n",
      "    \"134\": 134,\n",
      "    \"135\": 135,\n",
      "    \"136\": 136,\n",
      "    \"137\": 137,\n",
      "    \"138\": 138,\n",
      "    \"139\": 139,\n",
      "    \"14\": 14,\n",
      "    \"140\": 140,\n",
      "    \"141\": 141,\n",
      "    \"142\": 142,\n",
      "    \"143\": 143,\n",
      "    \"144\": 144,\n",
      "    \"145\": 145,\n",
      "    \"146\": 146,\n",
      "    \"147\": 147,\n",
      "    \"148\": 148,\n",
      "    \"149\": 149,\n",
      "    \"15\": 15,\n",
      "    \"150\": 150,\n",
      "    \"151\": 151,\n",
      "    \"152\": 152,\n",
      "    \"153\": 153,\n",
      "    \"154\": 154,\n",
      "    \"155\": 155,\n",
      "    \"156\": 156,\n",
      "    \"157\": 157,\n",
      "    \"158\": 158,\n",
      "    \"159\": 159,\n",
      "    \"16\": 16,\n",
      "    \"160\": 160,\n",
      "    \"161\": 161,\n",
      "    \"162\": 162,\n",
      "    \"163\": 163,\n",
      "    \"164\": 164,\n",
      "    \"165\": 165,\n",
      "    \"166\": 166,\n",
      "    \"167\": 167,\n",
      "    \"168\": 168,\n",
      "    \"169\": 169,\n",
      "    \"17\": 17,\n",
      "    \"170\": 170,\n",
      "    \"171\": 171,\n",
      "    \"172\": 172,\n",
      "    \"173\": 173,\n",
      "    \"174\": 174,\n",
      "    \"175\": 175,\n",
      "    \"176\": 176,\n",
      "    \"177\": 177,\n",
      "    \"178\": 178,\n",
      "    \"179\": 179,\n",
      "    \"18\": 18,\n",
      "    \"180\": 180,\n",
      "    \"181\": 181,\n",
      "    \"182\": 182,\n",
      "    \"183\": 183,\n",
      "    \"184\": 184,\n",
      "    \"185\": 185,\n",
      "    \"186\": 186,\n",
      "    \"187\": 187,\n",
      "    \"188\": 188,\n",
      "    \"189\": 189,\n",
      "    \"19\": 19,\n",
      "    \"190\": 190,\n",
      "    \"191\": 191,\n",
      "    \"192\": 192,\n",
      "    \"193\": 193,\n",
      "    \"194\": 194,\n",
      "    \"195\": 195,\n",
      "    \"196\": 196,\n",
      "    \"197\": 197,\n",
      "    \"198\": 198,\n",
      "    \"199\": 199,\n",
      "    \"2\": 2,\n",
      "    \"20\": 20,\n",
      "    \"200\": 200,\n",
      "    \"201\": 201,\n",
      "    \"202\": 202,\n",
      "    \"203\": 203,\n",
      "    \"204\": 204,\n",
      "    \"205\": 205,\n",
      "    \"206\": 206,\n",
      "    \"207\": 207,\n",
      "    \"208\": 208,\n",
      "    \"209\": 209,\n",
      "    \"21\": 21,\n",
      "    \"210\": 210,\n",
      "    \"211\": 211,\n",
      "    \"212\": 212,\n",
      "    \"213\": 213,\n",
      "    \"214\": 214,\n",
      "    \"215\": 215,\n",
      "    \"216\": 216,\n",
      "    \"217\": 217,\n",
      "    \"218\": 218,\n",
      "    \"219\": 219,\n",
      "    \"22\": 22,\n",
      "    \"220\": 220,\n",
      "    \"221\": 221,\n",
      "    \"222\": 222,\n",
      "    \"223\": 223,\n",
      "    \"224\": 224,\n",
      "    \"225\": 225,\n",
      "    \"226\": 226,\n",
      "    \"227\": 227,\n",
      "    \"228\": 228,\n",
      "    \"229\": 229,\n",
      "    \"23\": 23,\n",
      "    \"230\": 230,\n",
      "    \"231\": 231,\n",
      "    \"232\": 232,\n",
      "    \"233\": 233,\n",
      "    \"234\": 234,\n",
      "    \"235\": 235,\n",
      "    \"236\": 236,\n",
      "    \"237\": 237,\n",
      "    \"238\": 238,\n",
      "    \"239\": 239,\n",
      "    \"24\": 24,\n",
      "    \"240\": 240,\n",
      "    \"241\": 241,\n",
      "    \"242\": 242,\n",
      "    \"243\": 243,\n",
      "    \"244\": 244,\n",
      "    \"245\": 245,\n",
      "    \"246\": 246,\n",
      "    \"247\": 247,\n",
      "    \"248\": 248,\n",
      "    \"249\": 249,\n",
      "    \"25\": 25,\n",
      "    \"250\": 250,\n",
      "    \"251\": 251,\n",
      "    \"252\": 252,\n",
      "    \"253\": 253,\n",
      "    \"254\": 254,\n",
      "    \"255\": 255,\n",
      "    \"256\": 256,\n",
      "    \"257\": 257,\n",
      "    \"258\": 258,\n",
      "    \"259\": 259,\n",
      "    \"26\": 26,\n",
      "    \"260\": 260,\n",
      "    \"261\": 261,\n",
      "    \"262\": 262,\n",
      "    \"263\": 263,\n",
      "    \"264\": 264,\n",
      "    \"265\": 265,\n",
      "    \"266\": 266,\n",
      "    \"267\": 267,\n",
      "    \"268\": 268,\n",
      "    \"269\": 269,\n",
      "    \"27\": 27,\n",
      "    \"270\": 270,\n",
      "    \"271\": 271,\n",
      "    \"272\": 272,\n",
      "    \"273\": 273,\n",
      "    \"274\": 274,\n",
      "    \"275\": 275,\n",
      "    \"276\": 276,\n",
      "    \"277\": 277,\n",
      "    \"278\": 278,\n",
      "    \"279\": 279,\n",
      "    \"28\": 28,\n",
      "    \"280\": 280,\n",
      "    \"281\": 281,\n",
      "    \"282\": 282,\n",
      "    \"283\": 283,\n",
      "    \"284\": 284,\n",
      "    \"285\": 285,\n",
      "    \"286\": 286,\n",
      "    \"287\": 287,\n",
      "    \"288\": 288,\n",
      "    \"289\": 289,\n",
      "    \"29\": 29,\n",
      "    \"290\": 290,\n",
      "    \"291\": 291,\n",
      "    \"292\": 292,\n",
      "    \"293\": 293,\n",
      "    \"294\": 294,\n",
      "    \"295\": 295,\n",
      "    \"296\": 296,\n",
      "    \"297\": 297,\n",
      "    \"298\": 298,\n",
      "    \"299\": 299,\n",
      "    \"3\": 3,\n",
      "    \"30\": 30,\n",
      "    \"300\": 300,\n",
      "    \"301\": 301,\n",
      "    \"302\": 302,\n",
      "    \"303\": 303,\n",
      "    \"304\": 304,\n",
      "    \"305\": 305,\n",
      "    \"306\": 306,\n",
      "    \"307\": 307,\n",
      "    \"308\": 308,\n",
      "    \"309\": 309,\n",
      "    \"31\": 31,\n",
      "    \"310\": 310,\n",
      "    \"311\": 311,\n",
      "    \"312\": 312,\n",
      "    \"313\": 313,\n",
      "    \"314\": 314,\n",
      "    \"315\": 315,\n",
      "    \"316\": 316,\n",
      "    \"317\": 317,\n",
      "    \"318\": 318,\n",
      "    \"319\": 319,\n",
      "    \"32\": 32,\n",
      "    \"320\": 320,\n",
      "    \"321\": 321,\n",
      "    \"322\": 322,\n",
      "    \"323\": 323,\n",
      "    \"324\": 324,\n",
      "    \"325\": 325,\n",
      "    \"326\": 326,\n",
      "    \"327\": 327,\n",
      "    \"328\": 328,\n",
      "    \"329\": 329,\n",
      "    \"33\": 33,\n",
      "    \"330\": 330,\n",
      "    \"331\": 331,\n",
      "    \"332\": 332,\n",
      "    \"333\": 333,\n",
      "    \"334\": 334,\n",
      "    \"335\": 335,\n",
      "    \"336\": 336,\n",
      "    \"337\": 337,\n",
      "    \"338\": 338,\n",
      "    \"339\": 339,\n",
      "    \"34\": 34,\n",
      "    \"340\": 340,\n",
      "    \"341\": 341,\n",
      "    \"342\": 342,\n",
      "    \"343\": 343,\n",
      "    \"344\": 344,\n",
      "    \"345\": 345,\n",
      "    \"346\": 346,\n",
      "    \"347\": 347,\n",
      "    \"348\": 348,\n",
      "    \"349\": 349,\n",
      "    \"35\": 35,\n",
      "    \"350\": 350,\n",
      "    \"351\": 351,\n",
      "    \"352\": 352,\n",
      "    \"353\": 353,\n",
      "    \"354\": 354,\n",
      "    \"355\": 355,\n",
      "    \"356\": 356,\n",
      "    \"357\": 357,\n",
      "    \"358\": 358,\n",
      "    \"359\": 359,\n",
      "    \"36\": 36,\n",
      "    \"360\": 360,\n",
      "    \"361\": 361,\n",
      "    \"362\": 362,\n",
      "    \"363\": 363,\n",
      "    \"364\": 364,\n",
      "    \"365\": 365,\n",
      "    \"366\": 366,\n",
      "    \"367\": 367,\n",
      "    \"368\": 368,\n",
      "    \"369\": 369,\n",
      "    \"37\": 37,\n",
      "    \"370\": 370,\n",
      "    \"371\": 371,\n",
      "    \"372\": 372,\n",
      "    \"373\": 373,\n",
      "    \"374\": 374,\n",
      "    \"375\": 375,\n",
      "    \"376\": 376,\n",
      "    \"377\": 377,\n",
      "    \"378\": 378,\n",
      "    \"379\": 379,\n",
      "    \"38\": 38,\n",
      "    \"380\": 380,\n",
      "    \"381\": 381,\n",
      "    \"382\": 382,\n",
      "    \"383\": 383,\n",
      "    \"384\": 384,\n",
      "    \"385\": 385,\n",
      "    \"386\": 386,\n",
      "    \"387\": 387,\n",
      "    \"388\": 388,\n",
      "    \"389\": 389,\n",
      "    \"39\": 39,\n",
      "    \"390\": 390,\n",
      "    \"391\": 391,\n",
      "    \"392\": 392,\n",
      "    \"393\": 393,\n",
      "    \"394\": 394,\n",
      "    \"395\": 395,\n",
      "    \"396\": 396,\n",
      "    \"397\": 397,\n",
      "    \"398\": 398,\n",
      "    \"399\": 399,\n",
      "    \"4\": 4,\n",
      "    \"40\": 40,\n",
      "    \"400\": 400,\n",
      "    \"401\": 401,\n",
      "    \"402\": 402,\n",
      "    \"403\": 403,\n",
      "    \"404\": 404,\n",
      "    \"405\": 405,\n",
      "    \"406\": 406,\n",
      "    \"407\": 407,\n",
      "    \"408\": 408,\n",
      "    \"409\": 409,\n",
      "    \"41\": 41,\n",
      "    \"410\": 410,\n",
      "    \"411\": 411,\n",
      "    \"412\": 412,\n",
      "    \"413\": 413,\n",
      "    \"414\": 414,\n",
      "    \"415\": 415,\n",
      "    \"416\": 416,\n",
      "    \"417\": 417,\n",
      "    \"418\": 418,\n",
      "    \"419\": 419,\n",
      "    \"42\": 42,\n",
      "    \"420\": 420,\n",
      "    \"421\": 421,\n",
      "    \"422\": 422,\n",
      "    \"423\": 423,\n",
      "    \"424\": 424,\n",
      "    \"425\": 425,\n",
      "    \"426\": 426,\n",
      "    \"427\": 427,\n",
      "    \"428\": 428,\n",
      "    \"429\": 429,\n",
      "    \"43\": 43,\n",
      "    \"430\": 430,\n",
      "    \"431\": 431,\n",
      "    \"432\": 432,\n",
      "    \"433\": 433,\n",
      "    \"434\": 434,\n",
      "    \"435\": 435,\n",
      "    \"436\": 436,\n",
      "    \"437\": 437,\n",
      "    \"438\": 438,\n",
      "    \"439\": 439,\n",
      "    \"44\": 44,\n",
      "    \"440\": 440,\n",
      "    \"441\": 441,\n",
      "    \"442\": 442,\n",
      "    \"443\": 443,\n",
      "    \"444\": 444,\n",
      "    \"445\": 445,\n",
      "    \"446\": 446,\n",
      "    \"447\": 447,\n",
      "    \"448\": 448,\n",
      "    \"449\": 449,\n",
      "    \"45\": 45,\n",
      "    \"450\": 450,\n",
      "    \"451\": 451,\n",
      "    \"452\": 452,\n",
      "    \"453\": 453,\n",
      "    \"454\": 454,\n",
      "    \"455\": 455,\n",
      "    \"456\": 456,\n",
      "    \"457\": 457,\n",
      "    \"458\": 458,\n",
      "    \"459\": 459,\n",
      "    \"46\": 46,\n",
      "    \"460\": 460,\n",
      "    \"461\": 461,\n",
      "    \"462\": 462,\n",
      "    \"463\": 463,\n",
      "    \"464\": 464,\n",
      "    \"465\": 465,\n",
      "    \"466\": 466,\n",
      "    \"467\": 467,\n",
      "    \"468\": 468,\n",
      "    \"469\": 469,\n",
      "    \"47\": 47,\n",
      "    \"470\": 470,\n",
      "    \"471\": 471,\n",
      "    \"472\": 472,\n",
      "    \"473\": 473,\n",
      "    \"474\": 474,\n",
      "    \"475\": 475,\n",
      "    \"476\": 476,\n",
      "    \"477\": 477,\n",
      "    \"478\": 478,\n",
      "    \"479\": 479,\n",
      "    \"48\": 48,\n",
      "    \"480\": 480,\n",
      "    \"481\": 481,\n",
      "    \"482\": 482,\n",
      "    \"483\": 483,\n",
      "    \"484\": 484,\n",
      "    \"485\": 485,\n",
      "    \"486\": 486,\n",
      "    \"487\": 487,\n",
      "    \"488\": 488,\n",
      "    \"489\": 489,\n",
      "    \"49\": 49,\n",
      "    \"490\": 490,\n",
      "    \"491\": 491,\n",
      "    \"492\": 492,\n",
      "    \"493\": 493,\n",
      "    \"494\": 494,\n",
      "    \"495\": 495,\n",
      "    \"496\": 496,\n",
      "    \"497\": 497,\n",
      "    \"498\": 498,\n",
      "    \"499\": 499,\n",
      "    \"5\": 5,\n",
      "    \"50\": 50,\n",
      "    \"500\": 500,\n",
      "    \"501\": 501,\n",
      "    \"502\": 502,\n",
      "    \"503\": 503,\n",
      "    \"504\": 504,\n",
      "    \"505\": 505,\n",
      "    \"506\": 506,\n",
      "    \"507\": 507,\n",
      "    \"508\": 508,\n",
      "    \"509\": 509,\n",
      "    \"51\": 51,\n",
      "    \"510\": 510,\n",
      "    \"511\": 511,\n",
      "    \"512\": 512,\n",
      "    \"513\": 513,\n",
      "    \"514\": 514,\n",
      "    \"515\": 515,\n",
      "    \"516\": 516,\n",
      "    \"517\": 517,\n",
      "    \"518\": 518,\n",
      "    \"519\": 519,\n",
      "    \"52\": 52,\n",
      "    \"520\": 520,\n",
      "    \"521\": 521,\n",
      "    \"522\": 522,\n",
      "    \"523\": 523,\n",
      "    \"524\": 524,\n",
      "    \"525\": 525,\n",
      "    \"526\": 526,\n",
      "    \"527\": 527,\n",
      "    \"528\": 528,\n",
      "    \"529\": 529,\n",
      "    \"53\": 53,\n",
      "    \"530\": 530,\n",
      "    \"531\": 531,\n",
      "    \"532\": 532,\n",
      "    \"533\": 533,\n",
      "    \"534\": 534,\n",
      "    \"535\": 535,\n",
      "    \"536\": 536,\n",
      "    \"537\": 537,\n",
      "    \"538\": 538,\n",
      "    \"539\": 539,\n",
      "    \"54\": 54,\n",
      "    \"540\": 540,\n",
      "    \"541\": 541,\n",
      "    \"542\": 542,\n",
      "    \"543\": 543,\n",
      "    \"544\": 544,\n",
      "    \"545\": 545,\n",
      "    \"546\": 546,\n",
      "    \"547\": 547,\n",
      "    \"548\": 548,\n",
      "    \"549\": 549,\n",
      "    \"55\": 55,\n",
      "    \"550\": 550,\n",
      "    \"551\": 551,\n",
      "    \"552\": 552,\n",
      "    \"553\": 553,\n",
      "    \"554\": 554,\n",
      "    \"555\": 555,\n",
      "    \"556\": 556,\n",
      "    \"557\": 557,\n",
      "    \"558\": 558,\n",
      "    \"559\": 559,\n",
      "    \"56\": 56,\n",
      "    \"560\": 560,\n",
      "    \"561\": 561,\n",
      "    \"562\": 562,\n",
      "    \"563\": 563,\n",
      "    \"564\": 564,\n",
      "    \"565\": 565,\n",
      "    \"566\": 566,\n",
      "    \"567\": 567,\n",
      "    \"568\": 568,\n",
      "    \"569\": 569,\n",
      "    \"57\": 57,\n",
      "    \"570\": 570,\n",
      "    \"571\": 571,\n",
      "    \"572\": 572,\n",
      "    \"573\": 573,\n",
      "    \"574\": 574,\n",
      "    \"575\": 575,\n",
      "    \"576\": 576,\n",
      "    \"577\": 577,\n",
      "    \"578\": 578,\n",
      "    \"579\": 579,\n",
      "    \"58\": 58,\n",
      "    \"580\": 580,\n",
      "    \"581\": 581,\n",
      "    \"582\": 582,\n",
      "    \"583\": 583,\n",
      "    \"584\": 584,\n",
      "    \"585\": 585,\n",
      "    \"586\": 586,\n",
      "    \"587\": 587,\n",
      "    \"588\": 588,\n",
      "    \"589\": 589,\n",
      "    \"59\": 59,\n",
      "    \"590\": 590,\n",
      "    \"591\": 591,\n",
      "    \"592\": 592,\n",
      "    \"593\": 593,\n",
      "    \"594\": 594,\n",
      "    \"595\": 595,\n",
      "    \"596\": 596,\n",
      "    \"597\": 597,\n",
      "    \"598\": 598,\n",
      "    \"599\": 599,\n",
      "    \"6\": 6,\n",
      "    \"60\": 60,\n",
      "    \"600\": 600,\n",
      "    \"601\": 601,\n",
      "    \"602\": 602,\n",
      "    \"603\": 603,\n",
      "    \"604\": 604,\n",
      "    \"605\": 605,\n",
      "    \"606\": 606,\n",
      "    \"607\": 607,\n",
      "    \"608\": 608,\n",
      "    \"609\": 609,\n",
      "    \"61\": 61,\n",
      "    \"610\": 610,\n",
      "    \"611\": 611,\n",
      "    \"612\": 612,\n",
      "    \"613\": 613,\n",
      "    \"614\": 614,\n",
      "    \"615\": 615,\n",
      "    \"616\": 616,\n",
      "    \"617\": 617,\n",
      "    \"618\": 618,\n",
      "    \"619\": 619,\n",
      "    \"62\": 62,\n",
      "    \"620\": 620,\n",
      "    \"621\": 621,\n",
      "    \"622\": 622,\n",
      "    \"623\": 623,\n",
      "    \"624\": 624,\n",
      "    \"625\": 625,\n",
      "    \"626\": 626,\n",
      "    \"627\": 627,\n",
      "    \"628\": 628,\n",
      "    \"629\": 629,\n",
      "    \"63\": 63,\n",
      "    \"630\": 630,\n",
      "    \"631\": 631,\n",
      "    \"632\": 632,\n",
      "    \"633\": 633,\n",
      "    \"634\": 634,\n",
      "    \"635\": 635,\n",
      "    \"636\": 636,\n",
      "    \"637\": 637,\n",
      "    \"638\": 638,\n",
      "    \"639\": 639,\n",
      "    \"64\": 64,\n",
      "    \"640\": 640,\n",
      "    \"641\": 641,\n",
      "    \"642\": 642,\n",
      "    \"643\": 643,\n",
      "    \"644\": 644,\n",
      "    \"645\": 645,\n",
      "    \"646\": 646,\n",
      "    \"647\": 647,\n",
      "    \"648\": 648,\n",
      "    \"649\": 649,\n",
      "    \"65\": 65,\n",
      "    \"650\": 650,\n",
      "    \"651\": 651,\n",
      "    \"652\": 652,\n",
      "    \"653\": 653,\n",
      "    \"654\": 654,\n",
      "    \"655\": 655,\n",
      "    \"656\": 656,\n",
      "    \"657\": 657,\n",
      "    \"658\": 658,\n",
      "    \"659\": 659,\n",
      "    \"66\": 66,\n",
      "    \"660\": 660,\n",
      "    \"661\": 661,\n",
      "    \"662\": 662,\n",
      "    \"663\": 663,\n",
      "    \"664\": 664,\n",
      "    \"665\": 665,\n",
      "    \"666\": 666,\n",
      "    \"667\": 667,\n",
      "    \"668\": 668,\n",
      "    \"669\": 669,\n",
      "    \"67\": 67,\n",
      "    \"670\": 670,\n",
      "    \"671\": 671,\n",
      "    \"672\": 672,\n",
      "    \"673\": 673,\n",
      "    \"674\": 674,\n",
      "    \"675\": 675,\n",
      "    \"676\": 676,\n",
      "    \"677\": 677,\n",
      "    \"678\": 678,\n",
      "    \"679\": 679,\n",
      "    \"68\": 68,\n",
      "    \"680\": 680,\n",
      "    \"681\": 681,\n",
      "    \"682\": 682,\n",
      "    \"683\": 683,\n",
      "    \"684\": 684,\n",
      "    \"685\": 685,\n",
      "    \"686\": 686,\n",
      "    \"687\": 687,\n",
      "    \"688\": 688,\n",
      "    \"689\": 689,\n",
      "    \"69\": 69,\n",
      "    \"690\": 690,\n",
      "    \"691\": 691,\n",
      "    \"692\": 692,\n",
      "    \"693\": 693,\n",
      "    \"694\": 694,\n",
      "    \"695\": 695,\n",
      "    \"696\": 696,\n",
      "    \"697\": 697,\n",
      "    \"698\": 698,\n",
      "    \"699\": 699,\n",
      "    \"7\": 7,\n",
      "    \"70\": 70,\n",
      "    \"700\": 700,\n",
      "    \"701\": 701,\n",
      "    \"702\": 702,\n",
      "    \"703\": 703,\n",
      "    \"704\": 704,\n",
      "    \"705\": 705,\n",
      "    \"706\": 706,\n",
      "    \"707\": 707,\n",
      "    \"708\": 708,\n",
      "    \"709\": 709,\n",
      "    \"71\": 71,\n",
      "    \"710\": 710,\n",
      "    \"711\": 711,\n",
      "    \"712\": 712,\n",
      "    \"713\": 713,\n",
      "    \"714\": 714,\n",
      "    \"715\": 715,\n",
      "    \"716\": 716,\n",
      "    \"717\": 717,\n",
      "    \"718\": 718,\n",
      "    \"719\": 719,\n",
      "    \"72\": 72,\n",
      "    \"720\": 720,\n",
      "    \"721\": 721,\n",
      "    \"722\": 722,\n",
      "    \"723\": 723,\n",
      "    \"724\": 724,\n",
      "    \"725\": 725,\n",
      "    \"726\": 726,\n",
      "    \"727\": 727,\n",
      "    \"728\": 728,\n",
      "    \"729\": 729,\n",
      "    \"73\": 73,\n",
      "    \"730\": 730,\n",
      "    \"731\": 731,\n",
      "    \"732\": 732,\n",
      "    \"733\": 733,\n",
      "    \"734\": 734,\n",
      "    \"735\": 735,\n",
      "    \"736\": 736,\n",
      "    \"737\": 737,\n",
      "    \"738\": 738,\n",
      "    \"739\": 739,\n",
      "    \"74\": 74,\n",
      "    \"740\": 740,\n",
      "    \"741\": 741,\n",
      "    \"742\": 742,\n",
      "    \"743\": 743,\n",
      "    \"744\": 744,\n",
      "    \"745\": 745,\n",
      "    \"746\": 746,\n",
      "    \"747\": 747,\n",
      "    \"748\": 748,\n",
      "    \"749\": 749,\n",
      "    \"75\": 75,\n",
      "    \"750\": 750,\n",
      "    \"751\": 751,\n",
      "    \"752\": 752,\n",
      "    \"753\": 753,\n",
      "    \"754\": 754,\n",
      "    \"755\": 755,\n",
      "    \"756\": 756,\n",
      "    \"757\": 757,\n",
      "    \"758\": 758,\n",
      "    \"759\": 759,\n",
      "    \"76\": 76,\n",
      "    \"760\": 760,\n",
      "    \"761\": 761,\n",
      "    \"762\": 762,\n",
      "    \"763\": 763,\n",
      "    \"764\": 764,\n",
      "    \"765\": 765,\n",
      "    \"766\": 766,\n",
      "    \"767\": 767,\n",
      "    \"768\": 768,\n",
      "    \"769\": 769,\n",
      "    \"77\": 77,\n",
      "    \"770\": 770,\n",
      "    \"771\": 771,\n",
      "    \"772\": 772,\n",
      "    \"773\": 773,\n",
      "    \"774\": 774,\n",
      "    \"775\": 775,\n",
      "    \"776\": 776,\n",
      "    \"777\": 777,\n",
      "    \"778\": 778,\n",
      "    \"779\": 779,\n",
      "    \"78\": 78,\n",
      "    \"780\": 780,\n",
      "    \"781\": 781,\n",
      "    \"782\": 782,\n",
      "    \"783\": 783,\n",
      "    \"784\": 784,\n",
      "    \"785\": 785,\n",
      "    \"786\": 786,\n",
      "    \"787\": 787,\n",
      "    \"788\": 788,\n",
      "    \"789\": 789,\n",
      "    \"79\": 79,\n",
      "    \"790\": 790,\n",
      "    \"791\": 791,\n",
      "    \"792\": 792,\n",
      "    \"793\": 793,\n",
      "    \"794\": 794,\n",
      "    \"795\": 795,\n",
      "    \"796\": 796,\n",
      "    \"797\": 797,\n",
      "    \"798\": 798,\n",
      "    \"799\": 799,\n",
      "    \"8\": 8,\n",
      "    \"80\": 80,\n",
      "    \"800\": 800,\n",
      "    \"801\": 801,\n",
      "    \"802\": 802,\n",
      "    \"803\": 803,\n",
      "    \"804\": 804,\n",
      "    \"805\": 805,\n",
      "    \"806\": 806,\n",
      "    \"807\": 807,\n",
      "    \"808\": 808,\n",
      "    \"809\": 809,\n",
      "    \"81\": 81,\n",
      "    \"810\": 810,\n",
      "    \"811\": 811,\n",
      "    \"812\": 812,\n",
      "    \"813\": 813,\n",
      "    \"814\": 814,\n",
      "    \"815\": 815,\n",
      "    \"816\": 816,\n",
      "    \"817\": 817,\n",
      "    \"818\": 818,\n",
      "    \"819\": 819,\n",
      "    \"82\": 82,\n",
      "    \"820\": 820,\n",
      "    \"821\": 821,\n",
      "    \"822\": 822,\n",
      "    \"823\": 823,\n",
      "    \"824\": 824,\n",
      "    \"825\": 825,\n",
      "    \"826\": 826,\n",
      "    \"827\": 827,\n",
      "    \"828\": 828,\n",
      "    \"829\": 829,\n",
      "    \"83\": 83,\n",
      "    \"830\": 830,\n",
      "    \"831\": 831,\n",
      "    \"832\": 832,\n",
      "    \"833\": 833,\n",
      "    \"834\": 834,\n",
      "    \"835\": 835,\n",
      "    \"836\": 836,\n",
      "    \"837\": 837,\n",
      "    \"838\": 838,\n",
      "    \"839\": 839,\n",
      "    \"84\": 84,\n",
      "    \"840\": 840,\n",
      "    \"841\": 841,\n",
      "    \"842\": 842,\n",
      "    \"843\": 843,\n",
      "    \"844\": 844,\n",
      "    \"845\": 845,\n",
      "    \"846\": 846,\n",
      "    \"847\": 847,\n",
      "    \"848\": 848,\n",
      "    \"849\": 849,\n",
      "    \"85\": 85,\n",
      "    \"850\": 850,\n",
      "    \"851\": 851,\n",
      "    \"852\": 852,\n",
      "    \"853\": 853,\n",
      "    \"854\": 854,\n",
      "    \"855\": 855,\n",
      "    \"856\": 856,\n",
      "    \"857\": 857,\n",
      "    \"858\": 858,\n",
      "    \"859\": 859,\n",
      "    \"86\": 86,\n",
      "    \"860\": 860,\n",
      "    \"861\": 861,\n",
      "    \"862\": 862,\n",
      "    \"863\": 863,\n",
      "    \"864\": 864,\n",
      "    \"865\": 865,\n",
      "    \"866\": 866,\n",
      "    \"867\": 867,\n",
      "    \"868\": 868,\n",
      "    \"869\": 869,\n",
      "    \"87\": 87,\n",
      "    \"870\": 870,\n",
      "    \"871\": 871,\n",
      "    \"872\": 872,\n",
      "    \"873\": 873,\n",
      "    \"874\": 874,\n",
      "    \"875\": 875,\n",
      "    \"876\": 876,\n",
      "    \"877\": 877,\n",
      "    \"878\": 878,\n",
      "    \"879\": 879,\n",
      "    \"88\": 88,\n",
      "    \"880\": 880,\n",
      "    \"881\": 881,\n",
      "    \"882\": 882,\n",
      "    \"883\": 883,\n",
      "    \"884\": 884,\n",
      "    \"885\": 885,\n",
      "    \"886\": 886,\n",
      "    \"887\": 887,\n",
      "    \"888\": 888,\n",
      "    \"889\": 889,\n",
      "    \"89\": 89,\n",
      "    \"890\": 890,\n",
      "    \"891\": 891,\n",
      "    \"892\": 892,\n",
      "    \"893\": 893,\n",
      "    \"894\": 894,\n",
      "    \"895\": 895,\n",
      "    \"896\": 896,\n",
      "    \"897\": 897,\n",
      "    \"898\": 898,\n",
      "    \"899\": 899,\n",
      "    \"9\": 9,\n",
      "    \"90\": 90,\n",
      "    \"900\": 900,\n",
      "    \"901\": 901,\n",
      "    \"902\": 902,\n",
      "    \"903\": 903,\n",
      "    \"904\": 904,\n",
      "    \"905\": 905,\n",
      "    \"906\": 906,\n",
      "    \"907\": 907,\n",
      "    \"908\": 908,\n",
      "    \"909\": 909,\n",
      "    \"91\": 91,\n",
      "    \"910\": 910,\n",
      "    \"911\": 911,\n",
      "    \"912\": 912,\n",
      "    \"913\": 913,\n",
      "    \"914\": 914,\n",
      "    \"915\": 915,\n",
      "    \"916\": 916,\n",
      "    \"917\": 917,\n",
      "    \"918\": 918,\n",
      "    \"92\": 92,\n",
      "    \"93\": 93,\n",
      "    \"94\": 94,\n",
      "    \"95\": 95,\n",
      "    \"96\": 96,\n",
      "    \"97\": 97,\n",
      "    \"98\": 98,\n",
      "    \"99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 23,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"omnigenome\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_generation\": 50,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_population\": 100,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"verify_ss\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 24\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "--- Creating Datasets ---\n",
      "Detected max_length=200 in the dataset, using it as the max_length.\n",
      "Loading data from tfb_prediction_dataset/train_tfb.npy...\n",
      "Loaded 4400000 examples from tfb_prediction_dataset/train_tfb.npy\n",
      "Detected shuffle=True, shuffling the examples...\n",
      "Detected max_examples=100000, truncating the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100000/100000 [00:57<00:00, 1727.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys have consistent sequence lengths, skipping padding and truncation.\n",
      "Detected max_length=200 in the dataset, using it as the max_length.\n",
      "Loading data from tfb_prediction_dataset/test_tfb.npy...\n",
      "Loaded 455024 examples from tfb_prediction_dataset/test_tfb.npy\n",
      "Detected shuffle=True, shuffling the examples...\n",
      "Detected max_examples=100000, truncating the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100000/100000 [00:55<00:00, 1793.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys have consistent sequence lengths, skipping padding and truncation.\n",
      "Detected max_length=200 in the dataset, using it as the max_length.\n",
      "Loading data from tfb_prediction_dataset/valid_tfb.npy...\n",
      "Loaded 8000 examples from tfb_prediction_dataset/valid_tfb.npy\n",
      "Detected shuffle=True, shuffling the examples...\n",
      "Detected max_examples=100000, truncating the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8000/8000 [00:04<00:00, 1779.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys have consistent sequence lengths, skipping padding and truncation.\n",
      "\n",
      "--- Initialization Complete ---\n",
      "Training set size: 100000\n",
      "Test set size: 100000\n",
      "Validation set size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizer and Model\n",
    "print(\"--- Initializing Tokenizer and Model ---\")\n",
    "if \"multimolecule\" in MODEL_NAME_OR_PATH.lower():\n",
    "    from multimolecule import AutoModelForTokenPrediction, RnaTokenizer\n",
    "    base_model = AutoModelForTokenPrediction.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True).base_model\n",
    "    tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    base_model = AutoModel.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "\n",
    "model = OmniModelForMultiLabelClassification(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    num_labels=919,  # DeepSEA has 919 binary labels for different chromatin features\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "# If you want to use LoRA, uncomment the following lines\n",
    "# lora_config = {\n",
    "#     \"lora_r\": 8,  # Rank of the LoRA layers\n",
    "#     \"lora_alpha\": 16,  # Scaling factor for LoRA\n",
    "#     \"lora_dropout\": 0.1,  # Dropout rate for LoRA layers\n",
    "#     \"target_modules\": [\"deepsea_classifier\"],  # Target modules to apply LoRA\n",
    "# }\n",
    "# model = OmniLoraModel(model, lora_config=lora_config)\n",
    "\n",
    "model.to(DEVICE).to(torch.float32) # Move model to the selected device\n",
    "# 2. Create Datasets\n",
    "print(\"\\n--- Creating Datasets ---\")\n",
    "train_set = DeepSEADataset(\n",
    "    data_source=TRAIN_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "test_set = DeepSEADataset(\n",
    "    data_source=TEST_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "valid_set = DeepSEADataset(\n",
    "    data_source=VALID_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ") if os.path.exists(VALID_FILE) else None\n",
    "\n",
    "print(\"\\n--- Initialization Complete ---\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "if valid_set:\n",
    "    print(f\"Validation set size: {len(valid_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2. Inspect a Data Sample\n",
    "Let's examine a single sample from our `train_set` to understand its structure. Our `DeepSEADataset` class processes the raw data into tokenized tensors (`input_ids`) and a label tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "------------------------------\n",
      "Input IDs Tensor Shape: torch.Size([102])\n",
      "Decoded Sequence (first 60 chars): 'G T T C A A G A A T G C A T A A A T T G T A T C T T C A G A ...'\n",
      "------------------------------\n",
      "Labels Tensor Shape: torch.Size([919])\n",
      "Labels Tensor Dtype: torch.float32\n",
      "First 20 labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample from the training set\n",
    "sample = train_set[0]\n",
    "\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Inspect Input ---\n",
    "input_ids = sample['input_ids']\n",
    "print(f\"Input IDs Tensor Shape: {input_ids.shape}\")\n",
    "\n",
    "# Decode the input_ids back to a human-readable sequence\n",
    "decoded_sequence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(f\"Decoded Sequence (first 60 chars): '{decoded_sequence[:60]}...'\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Inspect Labels ---\n",
    "labels = sample['labels']\n",
    "print(f\"Labels Tensor Shape: {labels.shape}\")\n",
    "print(f\"Labels Tensor Dtype: {labels.dtype}\")\n",
    "\n",
    "# Show the first 20 labels for this sequence\n",
    "print(f\"First 20 labels: {labels[:20].int().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the Model\n",
    "We are now ready to train. We will use the `AccelerateTrainer` from `omnigenbench`, which simplifies the training and evaluation loop, handles device placement, and integrates with `torch.utils.data.DataLoader` and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 500/500 [00:08<00:00, 62.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.49679106328943734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 0.6642: 100%|| 6250/6250 [06:12<00:00, 16.76it/s]\n",
      "Evaluating: 100%|| 500/500 [00:07<00:00, 65.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6626522016257755}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Loss: 0.6421: 100%|| 6250/6250 [06:11<00:00, 16.82it/s]\n",
      "Evaluating: 100%|| 500/500 [00:08<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6548992354311663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Loss: 0.6299: 100%|| 6250/6250 [06:15<00:00, 16.66it/s]\n",
      "Evaluating: 100%|| 500/500 [00:08<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.655689085400226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Loss: 0.6026: 100%|| 6250/6250 [06:10<00:00, 16.89it/s]\n",
      "Evaluating: 100%|| 500/500 [00:08<00:00, 60.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6620117452714009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Loss: 0.5628: 100%|| 6250/6250 [06:13<00:00, 16.74it/s]\n",
      "Evaluating: 100%|| 500/500 [00:07<00:00, 62.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6503828399274646}\n",
      "Early stopping at epoch 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 6250/6250 [01:42<00:00, 61.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6840973996364564}\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility across all libraries\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE) if valid_set else None\n",
    "\n",
    "# Define the metric for evaluation. For DeepSEA, ROC AUC is a standard metric.\n",
    "metrics = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=valid_loader, # Use validation set for early stopping and checkpointing\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    compute_metrics=metrics,\n",
    "    patience=PATIENCE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"--- Starting Training ---\")\n",
    "metrics = trainer.train()\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluation\n",
    "After training, the `AccelerateTrainer` automatically loads the best model checkpoint (based on validation set performance). It then runs a final evaluation on the held-out test set to provide an unbiased measure of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating on Test Set ---\n",
      "All metrics: {'valid': [{'roc_auc_score': 0.49679106328943734}, {'roc_auc_score': 0.6626522016257755}, {'roc_auc_score': 0.6548992354311663}, {'roc_auc_score': 0.655689085400226}, {'roc_auc_score': 0.6620117452714009}, {'roc_auc_score': 0.6503828399274646}], 'best_valid': {'roc_auc_score': 0.6626522016257755}, 'test': [{'roc_auc_score': 0.6840973996364564}]}\n",
      "Test metric:  {'roc_auc_score': 0.6840973996364564}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating on Test Set ---\")\n",
    "# The evaluation is automated by the trainer\n",
    "print(f\"All metrics:\", metrics)\n",
    "for metric in metrics['test']:\n",
    "    print(f\"Test metric:  {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Inference Example\n",
    "Finally, let's see how to use our fine-tuned model on a real-world example. Instead of a synthetic sequence, we'll take the first sample from our held-out test set. This is a powerful way to gut-check the model's performance, as we can directly compare its predictions with the actual ground truth labels for that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G C C A T T G G C C G T C T G T G C C A C C T G C C C A C T G T G A A G G C A T G T G A C T T G G A T C C T G G T G A A G G A G G T G G C T G T G T G G C G G G G T G G G C A G G T A A A G A A G C A G\n",
      "Input Sequence (from test set): '{'input_ids': tensor([[0, 6, 5, 5, 4, 7, 7, 6, 6, 5, 5, 6, 7, 5, 7, 6, 7, 6, 5, 5, 4, 5, 5, 7,\n",
      "         6, 5, 5, 5, 4, 5, 7, 6, 7, 6, 4, 4, 6, 6, 5, 4, 7, 6, 7, 6, 4, 5, 7, 7,\n",
      "         6, 6, 4, 7, 5, 5, 7, 6, 6, 7, 6, 4, 4, 6, 6, 4, 6, 6, 7, 6, 6, 5, 7, 6,\n",
      "         7, 6, 7, 6, 6, 5, 6, 6, 6, 6, 7, 6, 6, 6, 5, 4, 6, 6, 7, 4, 4, 4, 6, 4,\n",
      "         4, 6, 5, 4, 6, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}...'\n",
      "-----------------------------------------------------------------\n",
      "--- Inference on a Test Sample (first 20 labels) ---\n",
      "Label #    | Prediction    | Ground Truth  | Probability\n",
      "-----------------------------------------------------------------\n",
      "Label 1       | Binds         | Does not bind | 0.5301  false\n",
      "Label 2       | Does not bind | Does not bind | 0.4706  correct\n",
      "Label 3       | Binds         | Does not bind | 0.5001  false\n",
      "Label 4       | Does not bind | Does not bind | 0.4076  correct\n",
      "Label 5       | Binds         | Does not bind | 0.6681  false\n",
      "Label 6       | Binds         | Does not bind | 0.5687  false\n",
      "Label 7       | Binds         | Does not bind | 0.5205  false\n",
      "Label 8       | Does not bind | Does not bind | 0.4923  correct\n",
      "Label 9       | Binds         | Does not bind | 0.5204  false\n",
      "Label 10      | Does not bind | Does not bind | 0.4523  correct\n",
      "Label 11      | Binds         | Does not bind | 0.5381  false\n",
      "Label 12      | Does not bind | Does not bind | 0.4835  correct\n",
      "Label 13      | Binds         | Does not bind | 0.5223  false\n",
      "Label 14      | Binds         | Does not bind | 0.5478  false\n",
      "Label 15      | Does not bind | Does not bind | 0.4196  correct\n",
      "Label 16      | Does not bind | Does not bind | 0.4903  correct\n",
      "Label 17      | Does not bind | Does not bind | 0.4962  correct\n",
      "Label 18      | Does not bind | Does not bind | 0.4642  correct\n",
      "Label 19      | Does not bind | Does not bind | 0.4898  correct\n",
      "Label 20      | Binds         | Does not bind | 0.5522  false\n",
      "-----------------------------------------------------------------\n",
      "Accuracy for this single sample: 85.96%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Select the first sample from the test set\n",
    "test_sequence = tokenizer.decode(test_set[0]['input_ids'], skip_special_tokens=True) # you can also use any DNA sequence you want\n",
    "print(test_sequence)\n",
    "true_labels = test_set[0]['labels'].int() # The ground truth\n",
    "\n",
    "\n",
    "\n",
    "# 2. Prepare the sequence for the model (add spaces between characters)\n",
    "spaced_sequence = ' '.join(list(test_sequence))\n",
    "inputs = tokenizer(spaced_sequence, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n",
    "print(f\"Input Sequence (from test set): '{inputs[:80]}...'\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 4. Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model.predict(inputs)\n",
    "\n",
    "# 5. Extract predictions and probabilities\n",
    "# We take the first element [0] as our batch size is 1.\n",
    "predictions = outputs['predictions'][0].cpu().numpy()\n",
    "probabilities = outputs['probabilities'][0].cpu().numpy()\n",
    "\n",
    "# 6. Compare predictions with true labels for the first 20 TFs\n",
    "print(\"--- Inference on a Test Sample (first 20 labels) ---\")\n",
    "print(f\"{'Label #':<10} | {'Prediction':<13} | {'Ground Truth':<13} | {'Probability'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(20):\n",
    "    pred_label = 'Binds' if predictions[i] == 1 else 'Does not bind'\n",
    "    true_label = 'Binds' if true_labels[i] == 1 else 'Does not bind'\n",
    "    prob = probabilities[i]\n",
    "    # Add a checkmark if the prediction is correct\n",
    "    correct = \"correct\" if pred_label == true_label else \"false\"\n",
    "    \n",
    "    print(f\"Label {i+1:<7} | {pred_label:<13} | {true_label:<13} | {prob:.4f}  {correct}\")\n",
    "\n",
    "# Optional: Calculate and print the accuracy just for this single sample\n",
    "accuracy = (predictions == true_labels.cpu().numpy()).mean()\n",
    "print(\"-\" * 65)\n",
    "print(f\"Accuracy for this single sample: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "Congratulations! You have successfully fine-tuned a genomic foundation model for transcription factor binding prediction.\n",
    "\n",
    "In this tutorial, you have learned how to:\n",
    "\n",
    "- Set up a project environment for genomic deep learning.\n",
    "\n",
    "- Load and preprocess the DeepSEA dataset using a custom `OmniDataset` class.\n",
    "\n",
    "- Define a multi-label classification model by adding a custom head to a pre-trained `OmniGenome` backbone.\n",
    "\n",
    "- Train and evaluate the model efficiently using the `AccelerateTrainer` from OmniGenBench.\n",
    "\n",
    "- Use the final model to make predictions on new DNA sequences.\n",
    "\n",
    "From here, you could explore:\n",
    "\n",
    "- Experimenting with other models: Try different backbones from the `AVAILABLE_MODELS` list (see section 3.2).\n",
    "\n",
    "- Hyperparameter tuning: Adjust `LEARNING_RATE`, `BATCH_SIZE`, or `WEIGHT_DECAY` to improve performance (see section 3.3).\n",
    "\n",
    "- Using LoRA: Uncomment the `OmniLoraModel` code in the Initialization section to try parameter-efficient fine-tuning (see section 6.1).\n",
    "\n",
    "- Applying to other tasks: Adapt the pipeline for other genomic classification tasks, such as predicting promoter regions or splice sites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
