{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4bbd8f",
   "metadata": {},
   "source": [
    "## \u001f TFB Prediction Tutorial 4/4: Model Inference - From Trained Model to Predictions\n",
    "\n",
    "Welcome to the final tutorial in our series on fine-tuning a Genomic Foundation Model. We have come a long way:\n",
    "\n",
    "1.  We prepared the DeepSEA dataset ([01_data_preparation.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/01_data_preparation.ipynb)).\n",
    "2.  We initialized a model architecture suitable for our task ([02_model_initialization.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/02_model_initialization.ipynb)).\n",
    "3.  We trained the model and saved the best-performing checkpoint ([03_model_training.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/tfb_prediction/03_model_training.ipynb)).\n",
    "\n",
    "Now, we have a powerful, fine-tuned model stored as `best_model.pth`. But a model is only useful if we can use it to make predictions on new, unseen data. This process is called **inference**.\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "1.  **The Inference Pipeline**: Understanding the essential steps for getting predictions.\n",
    "2.  **Inference with `ModelHub`**: The easy, one-line way to load and predict with `OmniGenBench`.\n",
    "3.  **Manual Inference**: A more hands-on approach for custom data processing workflows.\n",
    "4.  **Final Evaluation**: Assessing our model's performance on the held-out test set.\n",
    "5.  **Deployment Concepts**: A brief look at how to serve your model as an API.\n",
    "\n",
    "By the end, you will be able to use your trained model to predict transcription factor binding sites and understand how to integrate it into larger applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7ccda",
   "metadata": {},
   "source": [
    "### 1. The Inference Pipeline\n",
    "\n",
    "Making a prediction with a trained model involves a clear, logical sequence of steps. It's crucial that the data processing during inference **exactly matches** the processing used during training.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "1.  **Load Model and Tokenizer**: You must load the exact model checkpoint (`best_model.pth`) that you saved during training. You also need the same tokenizer that was used to prepare the training data.\n",
    "2.  **Prepare Input**: Take a new, raw DNA sequence.\n",
    "3.  **Tokenize**: Use the loaded tokenizer to convert the DNA sequence into `input_ids` and an `attention_mask`, just as we did for the training data. This includes applying the same `max_length`, padding, and truncation strategies.\n",
    "4.  **Predict**: Pass the tokenized input through the model to get the raw output scores (logits).\n",
    "5.  **Post-process**: Convert the logits into a more interpretable format, such as probabilities (by applying a Sigmoid function) or binary predictions (by applying a threshold like 0.5).\n",
    "\n",
    "**`OmniGenBench` simplify all the processes and provides a unified interface for inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d0891",
   "metadata": {},
   "source": [
    "### 2. Inference with `ModelHub`: The Easy Way\n",
    "\n",
    "For many standard use cases, `OmniGenBench` offers a high-level `ModelHub` API that encapsulates the entire inference pipeline. It allows you to load a fine-tuned model and get predictions with a single line of code.\n",
    "\n",
    "`ModelHub` automatically handles loading the correct model architecture, the tokenizer, and the saved weights from your checkpoint directory.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d07202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\onedrive - university of exeter\\aiprojects\\omnigenbench\\omnigenbench\\src\\misc\\utils.py:376: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  - \\*objects: Objects to print\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA GPU found in your device\n",
      "\n",
      "    **@@ #========= @@**            ___                     _\n",
      "      **@@ +----- @@**             / _ \\  _ __ ___   _ __  (_)\n",
      "        **@@ = @@**               | | | || '_ ` _ \\ | '_ \\ | |\n",
      "           **@@                   | |_| || | | | | || | | || |\n",
      "        @@** = **@@                \\___/ |_| |_| |_||_| |_||_|\n",
      "     @@** ------+ **@@\n",
      "   @@** =========# **@@            ____\n",
      "  @@ ---------------+ @@          / ___|  ___  _ __\n",
      " @@ ================== @@        | |  _  / _ \\| '_ \\\n",
      "  @@ +--------------- @@         | |_| ||  __/| | | |\n",
      "   @@** #========= **@@           \\____| \\___||_| |_|\n",
      "    @@** +------ **@@\n",
      "       @@** = **@@\n",
      "          @@**                    ____                      _\n",
      "       **@@ = @@**               | __ )   ___  _ __    ___ | |__\n",
      "    **@@ -----+  @@**            |  _ \\  / _ \\| '_ \\  / __|| '_ \\\n",
      "  **@@ ==========# @@**          | |_) ||  __/| | | || (__ | | | |\n",
      "  @@ --------------+ @@**        |____/  \\___||_| |_| \\___||_| |_|\n",
      "\n",
      "[2025-10-31 23:31:26.200] [omnigenbench 0.3.25alpha]  [INFO] Using HuggingFace Hub API to download yangheng/ogb_tfb_finetuned\n",
      "[2025-10-31 23:31:26.211] [omnigenbench 0.3.25alpha]  [INFO] Downloading model 'yangheng/ogb_tfb_finetuned' from HuggingFace Hub...\n",
      "[2025-10-31 23:31:26.220] [omnigenbench 0.3.25alpha]  [INFO] This may take a while for large models (no git-lfs required)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 14 files:   7%|▋         | 1/14 [00:03<00:40,  3.10s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 14 files: 100%|██████████| 14/14 [00:39<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-31 23:32:07.912] [omnigenbench 0.3.25alpha]  [SUCCESS] Successfully downloaded yangheng/ogb_tfb_finetuned to D:\\OneDrive - University of Exeter\\AIProjects\\OmniGenBench\\examples\\tfb_prediction\\__OMNIGENOME_DATA__\\models\\yangheng--ogb_tfb_finetuned\n",
      "[2025-10-31 23:32:07.912] [omnigenbench 0.3.25alpha]  [SUCCESS] All required files present in D:\\OneDrive - University of Exeter\\AIProjects\\OmniGenBench\\examples\\tfb_prediction\\__OMNIGENOME_DATA__\\models\\yangheng--ogb_tfb_finetuned\n",
      "[2025-10-31 23:32:07.917] [omnigenbench 0.3.25alpha]  Downloaded model from Hugging Face Hub to: D:\\OneDrive - University of Exeter\\AIProjects\\OmniGenBench\\examples\\tfb_prediction\\__OMNIGENOME_DATA__\\models\\yangheng--ogb_tfb_finetuned\n",
      "[2025-10-31 23:32:07.973] [omnigenbench 0.3.25alpha]  Loaded metadata from: D:\\OneDrive - University of Exeter\\AIProjects\\OmniGenBench\\examples\\tfb_prediction\\__OMNIGENOME_DATA__\\models\\yangheng--ogb_tfb_finetuned\\metadata.json\n",
      "[2025-10-31 23:32:08.065] [omnigenbench 0.3.25alpha]  Loaded model class OmniModelForMultiLabelSequenceClassification from omnigenbench\n",
      "[2025-10-31 23:32:09.753] [omnigenbench 0.3.25alpha]  Warning: No dataset_class is provided for the model, please set 'dataset_class=...' when initializing the model if you want to use the dataset's prepare_input method during inference.\n",
      "[2025-10-31 23:32:09.770] [omnigenbench 0.3.25alpha]  Loading state dict from: D:\\OneDrive - University of Exeter\\AIProjects\\OmniGenBench\\examples\\tfb_prediction\\__OMNIGENOME_DATA__\\models\\yangheng--ogb_tfb_finetuned\\pytorch_model.bin\n",
      "[2025-10-31 23:32:09.895] [omnigenbench 0.3.25alpha]  No device is specified, the model will be loaded to the default device: cpu\n",
      "🧬 Testing model on sample DNA sequences:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\onedrive - university of exeter\\aiprojects\\omnigenbench\\omnigenbench\\src\\abc\\abstract_model.py:1450: UserWarning: Failed to use dataset.prepare_input: 'NoneType' object is not callable. Falling back to tokenizer.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), 'logits': tensor([7.2266e-02, 2.2797e-02, 4.9591e-02, 4.0771e-02, 6.8054e-02, 4.0375e-02,\n",
      "        2.9251e-02, 7.2083e-02, 7.5562e-02, 4.2480e-02, 6.3110e-02, 5.2826e-02,\n",
      "        6.0852e-02, 8.5876e-02, 2.5269e-02, 5.6122e-02, 2.2766e-02, 2.7435e-02,\n",
      "        2.5421e-02, 7.4890e-02, 7.0068e-02, 1.1633e-01, 4.4189e-02, 4.8218e-02,\n",
      "        5.8136e-02, 4.0100e-02, 8.0505e-02, 4.4434e-02, 2.5558e-02, 7.0068e-02,\n",
      "        3.2043e-02, 3.6438e-02, 4.5685e-02, 4.1779e-02, 2.0844e-02, 3.3783e-02,\n",
      "        5.4993e-02, 6.4209e-02, 3.1677e-02, 3.5736e-02, 1.4786e-02, 1.6098e-02,\n",
      "        1.8097e-02, 1.7776e-02, 1.8509e-02, 2.0294e-02, 4.9500e-02, 1.3535e-02,\n",
      "        8.5999e-02, 5.6641e-02, 5.5511e-02, 5.0598e-02, 4.2725e-02, 6.7200e-02,\n",
      "        1.2347e-01, 5.1453e-02, 9.1553e-02, 7.6416e-02, 5.4291e-02, 4.4434e-02,\n",
      "        4.7699e-02, 6.3110e-02, 9.1370e-02, 5.1666e-02, 3.8452e-02, 9.2224e-02,\n",
      "        3.4943e-02, 3.7964e-02, 3.6896e-02, 6.1310e-02, 1.8982e-02, 1.9165e-02,\n",
      "        1.9455e-02, 1.5251e-02, 1.5488e-02, 1.1246e-02, 2.2797e-02, 2.5955e-02,\n",
      "        2.3727e-02, 2.3651e-02, 2.6199e-02, 1.8616e-02, 2.4002e-02, 2.0523e-02,\n",
      "        1.7242e-02, 1.6342e-02, 3.6896e-02, 1.6098e-02, 2.2934e-02, 2.4567e-02,\n",
      "        2.6154e-02, 2.4277e-02, 2.5558e-02, 2.8702e-02, 1.9089e-02, 3.0853e-02,\n",
      "        1.5602e-02, 1.9424e-02, 2.4612e-02, 1.8127e-02, 2.0172e-02, 1.5839e-02,\n",
      "        1.5305e-02, 1.8051e-02, 2.3956e-02, 1.8906e-02, 4.3121e-02, 2.9541e-02,\n",
      "        3.7109e-02, 1.2970e-02, 1.7242e-02, 2.1820e-02, 1.3023e-02, 5.9326e-02,\n",
      "        1.8875e-02, 1.2192e-02, 1.2527e-02, 1.5366e-02, 1.6922e-02, 1.9562e-02,\n",
      "        4.9133e-02, 2.9709e-02, 4.6906e-02, 1.6159e-02, 1.7105e-02, 4.6478e-02,\n",
      "        1.6289e-03, 4.8950e-02, 1.2398e-03, 7.4883e-03, 5.5420e-02, 1.3023e-02,\n",
      "        3.3927e-04, 3.0502e-02, 5.7922e-02, 1.2789e-03, 2.5120e-03, 4.3274e-02,\n",
      "        9.5978e-03, 2.5665e-02, 6.0272e-03, 3.4241e-02, 1.5125e-03, 4.3701e-02,\n",
      "        3.0746e-03, 3.9276e-02, 1.5305e-02, 4.1008e-03, 1.3535e-02, 7.9041e-02,\n",
      "        1.2255e-03, 1.6464e-02, 6.0959e-03, 7.1812e-04, 1.2016e-03, 2.4994e-02,\n",
      "        1.7639e-02, 1.2146e-02, 1.9562e-02, 9.0866e-03, 2.7954e-02, 1.2527e-02,\n",
      "        3.2654e-02, 1.2482e-02, 4.0466e-02, 1.4618e-02, 4.6463e-03, 3.9795e-02,\n",
      "        1.4282e-02, 6.5369e-02, 3.0270e-03, 4.6806e-03, 6.0501e-03, 1.4229e-02,\n",
      "        1.7776e-02, 4.2801e-03, 2.9011e-03, 3.1967e-03, 5.6791e-04, 6.5384e-03,\n",
      "        2.0337e-04, 3.7804e-03, 9.5463e-04, 9.4833e-03, 8.3466e-03, 6.2408e-03,\n",
      "        3.4363e-02, 3.4882e-02, 5.7068e-03, 2.2526e-03, 7.0648e-03, 1.0132e-02,\n",
      "        3.0632e-03, 3.5648e-03, 3.5248e-03, 1.0094e-02, 3.0518e-03, 1.0170e-02,\n",
      "        1.4954e-03, 6.8512e-03, 4.4174e-03, 9.5844e-04, 2.4433e-03, 1.8372e-02,\n",
      "        9.9716e-03, 2.5511e-04, 1.7004e-03, 2.7046e-03, 6.2637e-03, 4.3297e-03,\n",
      "        2.6627e-03, 1.9791e-02, 9.1248e-03, 2.0172e-02, 6.5660e-04, 1.7441e-02,\n",
      "        1.5545e-03, 5.3635e-03, 4.3640e-03, 2.3136e-03, 8.7814e-03, 6.7711e-03,\n",
      "        1.2054e-02, 5.7755e-03, 1.3885e-03, 1.4389e-02, 1.0094e-02, 2.4624e-03,\n",
      "        1.5190e-02, 3.4760e-02, 7.0923e-02, 1.1070e-02, 1.0452e-02, 1.1467e-02,\n",
      "        2.8168e-02, 2.8682e-04, 7.7915e-04, 8.5144e-03, 2.3422e-03, 6.0501e-03,\n",
      "        7.0114e-03, 1.0902e-02, 1.0490e-02, 5.1003e-03, 2.5620e-02, 5.7030e-04,\n",
      "        5.0621e-03, 8.9693e-04, 1.4450e-02, 1.3954e-02, 8.0643e-03, 1.0818e-02,\n",
      "        5.6000e-03, 8.4763e-03, 6.0043e-03, 1.9272e-02, 2.0142e-02, 4.0855e-03,\n",
      "        9.7427e-03, 7.4043e-03, 1.6613e-03, 4.6921e-04, 3.3478e-02, 6.8779e-03,\n",
      "        4.0936e-04, 4.7569e-03, 3.1242e-03, 3.0518e-03, 1.8463e-03, 8.0032e-03,\n",
      "        6.4125e-03, 2.0523e-02, 2.6062e-02, 7.4959e-04, 5.2917e-02, 5.7936e-04,\n",
      "        6.1455e-03, 8.7976e-04, 9.5215e-03, 9.7752e-04, 7.4883e-03, 1.5125e-03,\n",
      "        1.6922e-02, 7.3166e-03, 5.0392e-03, 1.0368e-02, 1.0612e-02, 1.8021e-02,\n",
      "        2.4002e-02, 4.8294e-03, 8.1348e-04, 1.2941e-03, 3.9444e-03, 2.0523e-02,\n",
      "        5.8899e-03, 8.8310e-04, 8.2636e-04, 3.1357e-03, 6.6910e-03, 3.1204e-02,\n",
      "        8.3466e-03, 2.9812e-03, 9.9335e-03, 9.2316e-03, 8.0948e-03, 4.6654e-03,\n",
      "        1.3374e-02, 6.5117e-03, 9.7427e-03, 4.5547e-03, 3.8242e-03, 1.6220e-02,\n",
      "        1.0651e-02, 5.3635e-03, 7.4310e-03, 8.1558e-03, 1.5778e-02, 3.0853e-02,\n",
      "        3.0563e-02, 9.3384e-03, 1.4175e-02, 1.2970e-02, 9.4748e-04, 1.2016e-03,\n",
      "        1.3481e-02, 5.4216e-04, 5.3024e-03, 7.3166e-03, 2.0523e-02, 6.6137e-04,\n",
      "        1.0246e-02, 2.2629e-02, 2.0889e-02, 2.6531e-03, 3.9363e-04, 2.2793e-03,\n",
      "        1.7517e-02, 6.6910e-03, 3.4424e-02, 4.5929e-03, 1.9165e-02, 2.6611e-02,\n",
      "        1.4282e-02, 7.8125e-03, 6.2656e-04, 6.8512e-03, 3.8700e-03, 2.0580e-03,\n",
      "        3.7537e-02, 1.0204e-03, 1.0056e-02, 7.4615e-03, 1.3123e-02, 1.3023e-02,\n",
      "        4.2084e-02, 7.2594e-03, 1.4732e-02, 7.1487e-03, 7.1239e-04, 2.9240e-03,\n",
      "        7.1239e-04, 9.4748e-04, 3.0403e-03, 7.5493e-03, 1.6489e-03, 1.3168e-02,\n",
      "        1.0729e-03, 7.0381e-03, 6.8512e-03, 7.5493e-03, 1.5717e-02, 1.3456e-03,\n",
      "        3.5339e-02, 6.9580e-03, 8.9188e-03, 8.0948e-03, 1.1375e-02, 6.8779e-03,\n",
      "        2.0275e-03, 2.5501e-03, 7.6370e-03, 1.7578e-02, 1.3168e-02, 3.6793e-03,\n",
      "        9.7427e-03, 1.3069e-02, 7.4043e-03, 2.5467e-02, 5.7297e-03, 9.0866e-03,\n",
      "        5.8899e-03, 4.0855e-03, 1.4389e-02, 6.6662e-04, 2.0676e-03, 1.8387e-03,\n",
      "        8.1558e-03, 9.5215e-03, 4.5395e-03, 9.1934e-03, 1.1871e-02, 9.6226e-04,\n",
      "        4.6806e-03, 5.6000e-03, 3.2730e-03, 1.1642e-02, 1.7130e-04, 5.4283e-03,\n",
      "        4.2152e-03, 7.6370e-03, 7.4267e-05, 4.3559e-04, 5.2185e-03, 1.1246e-02,\n",
      "        8.3303e-04, 3.2043e-02, 1.1158e-03, 1.6983e-02, 2.4929e-03, 3.4828e-03,\n",
      "        1.6737e-04, 1.2527e-02, 2.6657e-02, 1.6663e-02, 2.1994e-04, 7.2899e-03,\n",
      "        4.1723e-04, 7.4883e-03, 1.0366e-03, 1.1654e-03, 6.3629e-03, 2.0599e-02,\n",
      "        1.0818e-02, 6.9046e-03, 6.6578e-05, 1.7517e-02, 1.6937e-03, 1.1116e-02,\n",
      "        2.0569e-02, 1.3514e-03, 2.3880e-03, 9.5596e-03, 7.5493e-03, 1.1787e-03,\n",
      "        4.2801e-03, 1.0452e-02, 5.9748e-04, 1.5190e-02, 9.1732e-05, 2.8920e-04,\n",
      "        1.8021e-02, 1.9913e-02, 5.4703e-03, 2.0523e-02, 1.0246e-02, 1.1688e-02,\n",
      "        4.4861e-03, 2.2888e-02, 1.4553e-03, 1.1917e-02, 8.8806e-03, 1.9119e-02,\n",
      "        4.0855e-03, 1.8509e-02, 2.4433e-03, 2.2369e-02, 4.6272e-03, 6.3372e-04,\n",
      "        4.8103e-03, 1.4782e-03, 3.6507e-03, 4.1413e-04, 2.8667e-03, 1.7853e-02,\n",
      "        1.5488e-03, 3.2234e-03, 1.6680e-03, 7.8430e-03, 4.8485e-03, 8.0109e-04,\n",
      "        3.7323e-02, 9.6989e-04, 3.4424e-02, 5.3444e-03, 1.3123e-02, 4.6463e-03,\n",
      "        3.1555e-02, 1.2428e-02, 1.0406e-02, 7.7915e-04, 1.5015e-02, 4.5395e-03,\n",
      "        4.1504e-03, 6.0211e-02, 2.0844e-02, 3.1204e-02, 8.8120e-03, 1.8826e-03,\n",
      "        9.8407e-05, 2.1152e-03, 4.9114e-05, 5.7399e-05, 1.2550e-03, 9.1629e-03,\n",
      "        3.8290e-04, 9.0179e-03, 1.2350e-03, 5.4054e-03, 7.9346e-03, 2.1152e-03,\n",
      "        1.1339e-03, 3.9291e-03, 1.3990e-03, 3.1967e-03, 6.8512e-03, 4.0855e-03,\n",
      "        1.4839e-02, 3.1495e-04, 2.0428e-03, 2.3060e-03, 1.9012e-02, 1.6159e-02,\n",
      "        7.0953e-03, 2.8667e-03, 3.0041e-03, 4.2224e-04, 1.4105e-03, 1.2428e-02,\n",
      "        2.9373e-02, 4.8041e-04, 9.4528e-03, 6.8512e-03, 1.0729e-03, 1.8509e-02,\n",
      "        3.7861e-04, 1.0330e-02, 1.0118e-03, 8.2855e-03, 1.2238e-02, 1.5249e-03,\n",
      "        9.8515e-04, 7.1812e-04, 2.9354e-03, 2.5406e-03, 4.6997e-03, 5.9187e-05,\n",
      "        8.9586e-05, 6.4125e-03, 9.1248e-03, 3.7551e-04, 5.5771e-03, 7.0114e-03,\n",
      "        2.2259e-03, 9.5844e-04, 4.7188e-03, 8.3303e-04, 1.2147e-04, 6.9046e-03,\n",
      "        5.5075e-04, 1.3137e-04, 2.1992e-03, 2.3422e-03, 2.8114e-03, 2.3499e-03,\n",
      "        8.5754e-03, 1.5251e-02, 1.7776e-02, 3.0065e-04, 4.4174e-03, 8.1062e-04,\n",
      "        2.1576e-02, 1.8875e-02, 3.2482e-03, 2.4139e-02, 2.9583e-03, 1.6281e-02,\n",
      "        1.6232e-03, 7.6942e-03, 1.0948e-03, 1.2207e-03, 2.7466e-03, 1.0723e-04,\n",
      "        5.6877e-03, 1.4496e-03, 4.5738e-03, 3.2349e-03, 6.3629e-03, 5.5351e-03,\n",
      "        2.6001e-02, 2.2087e-03, 1.7242e-02, 1.2924e-02, 2.8896e-03, 2.5311e-03,\n",
      "        3.7861e-04, 5.7697e-04, 1.3137e-04, 1.5671e-02, 5.9187e-05, 2.4533e-04,\n",
      "        1.8530e-03, 2.3148e-02, 9.3384e-03, 9.4748e-04, 4.4327e-03, 8.7261e-04,\n",
      "        5.8365e-04, 9.1839e-04, 7.0143e-04, 5.8842e-04, 5.7755e-03, 8.6136e-03,\n",
      "        3.0670e-02, 1.7044e-02, 2.7847e-02, 2.5215e-03, 3.6068e-03, 2.0370e-02,\n",
      "        3.7048e-02, 2.6627e-03, 1.3847e-02, 1.2846e-03, 1.8101e-03, 3.4428e-03,\n",
      "        1.0405e-03, 1.6422e-03, 1.9943e-02, 1.1244e-03, 3.4332e-04, 3.0880e-03,\n",
      "        4.2496e-03, 8.0032e-03, 8.8501e-03, 1.9493e-03, 2.0428e-03, 1.9045e-03,\n",
      "        2.5375e-02, 4.0771e-02, 3.2978e-03, 2.4915e-04, 1.7405e-04, 1.8463e-03,\n",
      "        5.0621e-03, 6.9618e-04, 8.5144e-03, 9.5978e-03, 8.8806e-03, 9.2697e-03,\n",
      "        1.0330e-02, 1.6464e-02, 7.9651e-03, 1.9121e-03, 1.2772e-02, 1.9264e-04,\n",
      "        5.9586e-03, 9.9659e-04, 6.6137e-04, 7.2384e-04, 4.4518e-03, 5.8603e-04,\n",
      "        1.6098e-02, 5.5933e-04, 4.6921e-04, 1.0080e-03, 1.1606e-03, 1.3094e-03,\n",
      "        5.6601e-04, 6.8779e-03, 4.1656e-03, 3.6926e-03, 1.1116e-02, 9.2173e-04,\n",
      "        2.3592e-04, 5.9814e-03, 1.6464e-02, 1.1606e-03, 2.3594e-03, 2.6505e-02,\n",
      "        5.7755e-03, 1.2350e-03, 9.9957e-05, 1.0777e-03, 1.4610e-03, 1.4381e-03,\n",
      "        1.1921e-03, 6.6681e-03, 3.7498e-03, 4.0855e-03, 1.8433e-02, 1.4503e-02,\n",
      "        2.7370e-03, 2.6016e-03, 1.2493e-03, 2.1400e-03, 3.8395e-03, 4.8676e-03,\n",
      "        7.1526e-04, 2.4704e-02, 4.2152e-03, 3.7956e-03, 1.8478e-02, 2.0447e-02,\n",
      "        1.1375e-02, 5.5542e-03, 2.5215e-03, 1.6606e-04, 1.1597e-02, 1.1377e-03,\n",
      "        1.7181e-02, 1.8433e-02, 1.8829e-02, 1.4786e-02, 2.1572e-03, 8.5449e-03,\n",
      "        8.3466e-03, 6.1417e-04, 4.7188e-03, 2.2087e-03, 2.3327e-03, 9.8190e-03,\n",
      "        4.4586e-02, 8.6746e-03, 2.5421e-02, 4.1534e-02, 6.5384e-03, 4.2000e-03,\n",
      "        2.0447e-02, 1.4175e-02, 3.2349e-02, 3.6163e-02, 3.8391e-02, 2.3422e-02,\n",
      "        2.8381e-02, 9.0027e-04, 2.8107e-02, 1.4061e-02, 1.2493e-03, 1.8692e-02,\n",
      "        1.4282e-02, 2.1400e-03, 1.1162e-02, 2.5467e-02, 1.0319e-03, 1.4954e-02,\n",
      "        4.4861e-03, 3.1357e-03, 2.3560e-02, 1.7639e-02, 5.8899e-03, 1.9272e-02,\n",
      "        5.0201e-03, 2.3422e-02, 3.3966e-02, 5.6122e-02, 4.4586e-02, 4.6997e-02,\n",
      "        6.0303e-02, 1.6785e-02, 1.3954e-02, 1.1688e-02, 2.8214e-02, 2.2415e-02,\n",
      "        1.0780e-02, 4.3274e-02, 2.1484e-02, 3.7476e-02, 3.4180e-02, 3.6835e-02,\n",
      "        2.7481e-02, 2.5620e-02, 5.0323e-02, 4.3213e-02, 2.8107e-02, 3.9062e-02,\n",
      "        9.0408e-04, 4.1534e-02, 3.1677e-02, 4.3213e-02, 4.4861e-02, 2.7527e-02,\n",
      "        2.6611e-02, 2.7176e-02, 4.1321e-02, 3.3081e-02, 5.5817e-02, 2.5558e-02,\n",
      "        6.0852e-02, 4.6814e-02, 4.0619e-02, 3.4302e-02, 3.2410e-02, 2.1942e-02,\n",
      "        4.0314e-02, 2.4994e-02, 5.0323e-02, 8.0948e-03, 3.9185e-02, 3.1555e-02,\n",
      "        4.2572e-02, 3.0212e-02, 3.3966e-02, 3.9185e-02, 2.7008e-02, 4.2023e-02,\n",
      "        4.0314e-02, 4.1382e-02, 2.5177e-02, 4.2816e-02, 3.6621e-02, 2.9251e-02,\n",
      "        5.3589e-02, 3.2349e-02, 2.6810e-02, 3.3722e-02, 2.1912e-01, 6.5125e-02,\n",
      "        8.1863e-03, 5.5771e-03, 1.8539e-02, 5.1384e-03, 3.7956e-03, 3.3142e-02,\n",
      "        5.9814e-03, 7.1350e-02, 2.7370e-03, 3.8391e-02, 9.2529e-02, 1.0632e-01,\n",
      "        4.1687e-02, 5.7068e-03, 1.0339e-01, 2.6636e-01, 1.8872e-01, 1.8341e-02,\n",
      "        1.4114e-02, 1.3588e-02, 8.2397e-02, 1.6922e-02, 2.0340e-02, 5.2338e-02,\n",
      "        1.8567e-01, 1.4099e-01, 1.1841e-01, 1.1377e-01, 1.7303e-02, 1.7990e-02,\n",
      "        8.6914e-02, 1.8140e-01, 1.3904e-01, 9.6558e-02, 1.2146e-01, 8.3160e-03,\n",
      "        1.2445e-03, 2.8214e-02, 1.8506e-01, 1.7908e-01, 1.7761e-01, 7.8613e-02,\n",
      "        3.1152e-01, 2.9175e-01, 2.3218e-01, 1.2189e-01, 1.8335e-01, 5.8350e-02,\n",
      "        1.3135e-01, 1.1499e-01, 1.2366e-01, 6.8787e-02, 1.7639e-02, 1.6455e-01,\n",
      "        2.7222e-01, 1.6638e-01, 1.0876e-01, 1.8311e-01, 1.2054e-02, 7.1716e-02,\n",
      "        7.8918e-02, 7.8247e-02, 2.3285e-02, 5.4474e-03, 4.8309e-02, 1.5552e-01,\n",
      "        8.3740e-02, 6.5613e-02, 8.8684e-02, 7.1526e-04, 5.9753e-02, 9.0759e-02,\n",
      "        1.3806e-01, 1.5857e-01, 5.6763e-02, 1.4282e-01, 2.3584e-01, 2.0764e-01,\n",
      "        1.0321e-01, 1.6882e-01, 6.8245e-03, 1.0529e-02, 1.8585e-02, 6.0303e-02,\n",
      "        1.4099e-01, 7.0923e-02, 3.6499e-02, 5.8441e-02, 1.4880e-01, 1.1633e-01,\n",
      "        8.8684e-02, 1.2140e-01, 1.1473e-03, 1.9798e-03, 1.1047e-01, 1.6406e-01,\n",
      "        1.1145e-01, 5.1178e-02, 1.3135e-01, 2.4304e-01, 1.9397e-01, 1.1877e-01,\n",
      "        3.8242e-03], dtype=torch.float16), 'confidence': tensor(0.3115, dtype=torch.float16), 'last_hidden_state': tensor([ 0.6465, -0.8911, -0.0890,  0.9634,  0.7300,  0.9126, -0.9609,  0.0763,\n",
      "        -0.9526,  0.3328, -0.7197, -0.6230, -0.7612,  0.9419, -0.0682, -0.9497,\n",
      "         0.8784, -0.7612,  0.9644, -0.9810, -0.5996,  0.9106,  0.8916, -0.9453,\n",
      "         0.3682, -0.9541,  0.6855, -0.3606, -0.2166,  0.9634, -0.9541, -0.8101,\n",
      "         0.7026,  0.1018,  0.8716, -0.5762,  0.9121, -0.8350, -0.9141,  0.8267,\n",
      "         0.9424,  0.9482,  0.8623, -0.9004, -0.9409,  0.8228, -0.6431, -0.8525,\n",
      "         0.9624, -0.8228, -0.8433, -0.9053, -0.8965, -0.4272, -0.2651,  0.9053,\n",
      "         0.9438,  0.7441, -0.7700, -0.7388, -0.0397, -0.2236,  0.8813,  0.9692,\n",
      "        -0.0762,  0.8311,  0.7388,  0.8521,  0.4011, -0.8916,  0.3540, -0.8223,\n",
      "         0.5527, -0.9160, -0.6807,  0.1570,  0.2451,  0.3538,  0.4436,  0.8931,\n",
      "        -0.9297,  0.9600, -0.8525, -0.9326,  0.9453, -0.9639, -0.8813,  0.4492,\n",
      "        -0.7832, -0.0695, -0.4878, -0.0240,  0.9697, -0.8848, -0.5093, -0.5996,\n",
      "        -0.8521,  0.2000, -0.7783,  0.8354, -0.9014, -0.8594,  0.8745, -0.8545,\n",
      "        -0.2615,  0.6504, -0.7295,  0.9751,  0.7520,  0.6816,  0.2671,  0.8667,\n",
      "        -0.9478,  0.6626,  0.4238,  0.8901,  0.4976, -0.9370, -0.9531, -0.5601,\n",
      "        -0.4907, -0.6465, -0.9502,  0.9575,  0.9316,  0.7588, -0.8423,  0.9624,\n",
      "         0.9331, -0.7959, -0.9009, -0.3267, -0.7886, -0.7632,  0.6919, -0.8247,\n",
      "         0.6426,  0.7290,  0.9814, -0.9692, -0.9346,  0.9600, -0.8682, -0.6704,\n",
      "        -0.9727, -0.9429,  0.8750, -0.8540, -0.9155,  0.6460,  0.7056,  0.0521,\n",
      "        -0.9126, -0.7529, -0.1532,  0.6172, -0.8813,  0.9028,  0.9609, -0.6597,\n",
      "         0.8296,  0.9180,  0.7944,  0.8501, -0.1647, -0.9004,  0.6377, -0.2235,\n",
      "         0.8921,  0.3403, -0.0181,  0.1317, -0.7642,  0.7275, -0.9473, -0.9048,\n",
      "         0.8208,  0.9429, -0.8887,  0.0909,  0.9053,  0.9751, -0.6641,  0.8350,\n",
      "        -0.6831, -0.9067,  0.8921,  0.9092, -0.8696, -0.7598,  0.6992,  0.9106,\n",
      "         0.9116, -0.2512,  0.9600, -0.9360, -0.9146,  0.9307, -0.8364,  0.8462,\n",
      "         0.8408, -0.9253,  0.4553, -0.8770,  0.4453,  0.7012,  0.7441,  0.5557,\n",
      "        -0.3342,  0.6792,  0.8755, -0.5176,  0.9629, -0.9531,  0.3311,  0.9258,\n",
      "         0.8013, -0.5781, -0.9111,  0.8901, -0.6372,  0.4370, -0.8579,  0.4729,\n",
      "         0.5493, -0.9238,  0.8384,  0.4253,  0.4785, -0.8774, -0.9414, -0.8975,\n",
      "        -0.9058, -0.8433, -0.4426,  0.6030, -0.9092,  0.6899,  0.4016, -0.6987,\n",
      "         0.9673,  0.8013, -0.9297,  0.6318,  0.7207, -0.8774, -0.8213,  0.5737,\n",
      "        -0.8696,  0.3386, -0.8823,  0.9258,  0.2532, -0.4180,  0.9263, -0.0126,\n",
      "        -0.9849,  0.8394,  0.5601, -0.9360,  0.8145, -0.9429,  0.9482,  0.7920,\n",
      "        -0.9702, -0.9629, -0.9600, -0.8350,  0.7153, -0.9409, -0.3684, -0.9761,\n",
      "         0.9585, -0.9263, -0.6675, -0.9409, -0.9448,  0.8770, -0.9663,  0.9502,\n",
      "         0.0967, -0.9536,  0.2400, -0.2047,  0.1547,  0.7642, -0.9702,  0.7788,\n",
      "        -0.7393,  0.8999, -0.2947,  0.8994,  0.0115, -0.7729,  0.7051,  0.9644,\n",
      "         0.8901, -0.3740,  0.0893, -0.6519,  0.0101,  0.9126, -0.5127, -0.7363,\n",
      "         0.9150,  0.9336, -0.9487, -0.9580, -0.6772, -0.7568, -0.9424, -0.8965,\n",
      "         0.9365,  0.9556,  0.3958, -0.6089,  0.8149,  0.9062,  0.7915,  0.3994,\n",
      "         0.8833,  0.6167, -0.2820, -0.0383, -0.7993,  0.9199, -0.1089,  0.8560,\n",
      "         0.5449,  0.8301, -0.3369, -0.1541, -0.7876,  0.9116, -0.4001, -0.2218,\n",
      "         0.9116, -0.6719,  0.4045, -0.9707,  0.2312, -0.9609, -0.9458,  0.8896,\n",
      "        -0.0637,  0.1752,  0.1353,  0.9395, -0.9600,  0.6558, -0.3701, -0.9272,\n",
      "         0.9194,  0.3049,  0.6138,  0.5508,  0.8218,  0.9653, -0.5479,  0.8091,\n",
      "         0.8555, -0.9819,  0.5903,  0.8081, -0.7227,  0.7993, -0.9292, -0.7939,\n",
      "        -0.7871, -0.2834,  0.2043,  0.9756, -0.9614,  0.8643, -0.7783,  0.9048,\n",
      "        -0.9668, -0.9609, -0.5332, -0.8628,  0.7983,  0.7183, -0.8975,  0.8882,\n",
      "        -0.7808, -0.7393, -0.6582,  0.7720,  0.9531, -0.9697,  0.9062, -0.8813,\n",
      "         0.6880,  0.8809,  0.9185, -0.0630, -0.8638,  0.7793,  0.9517,  0.8906,\n",
      "         0.2556,  0.8965,  0.8247,  0.6240,  0.8613,  0.4336,  0.8789, -0.9043,\n",
      "         0.8696,  0.8579,  0.5615,  0.9512,  0.8892, -0.7900,  0.9658, -0.9243,\n",
      "        -0.8833, -0.2786,  0.1273,  0.9443, -0.9053,  0.9492, -0.9741,  0.9380,\n",
      "         0.9536, -0.8296,  0.9253, -0.9443,  0.9380, -0.7798,  0.9111, -0.3037,\n",
      "         0.4485,  0.6821, -0.8867,  0.5176,  0.8770,  0.7427, -0.9497,  0.8735,\n",
      "        -0.2146,  0.9619,  0.9390,  0.8623,  0.5952,  0.9580,  0.7607,  0.8770,\n",
      "        -0.8706,  0.8657, -0.5068,  0.7280, -0.9136,  0.7695, -0.9463, -0.1776,\n",
      "         0.9536,  0.9243,  0.1779, -0.7959, -0.9233, -0.8584, -0.9482, -0.7974,\n",
      "         0.8794,  0.9448, -0.9243,  0.9492, -0.9385, -0.7378, -0.9092, -0.2583,\n",
      "        -0.8794, -0.9409, -0.9160,  0.8008, -0.6890,  0.4312, -0.8501,  0.7905,\n",
      "         0.8071, -0.4863,  0.9204, -0.7305,  0.8740,  0.8506, -0.8110, -0.9229,\n",
      "        -0.7632,  0.6685, -0.8379, -0.9028,  0.6611,  0.9023,  0.6621,  0.1281,\n",
      "         0.1802,  0.9263, -0.8872, -0.4167,  0.9551, -0.9775, -0.3716,  0.0525,\n",
      "         0.8867, -0.5146,  0.5210, -0.4375,  0.9277,  0.8003, -0.7769, -0.4971],\n",
      "       dtype=torch.float16)}\n",
      "==================================================\n",
      "{'predictions': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), 'logits': tensor([1.8097e-02, 6.3629e-03, 1.1200e-02, 8.8120e-03, 2.4277e-02, 1.2291e-02,\n",
      "        7.5760e-03, 1.5961e-02, 1.8509e-02, 1.0529e-02, 1.5717e-02, 1.3222e-02,\n",
      "        1.8906e-02, 1.4282e-02, 6.4888e-03, 1.8875e-02, 5.0812e-03, 6.1684e-03,\n",
      "        6.7444e-03, 1.9638e-02, 1.5717e-02, 2.0340e-02, 1.2527e-02, 1.1826e-02,\n",
      "        1.3687e-02, 1.1642e-02, 2.1118e-02, 1.4450e-02, 7.5760e-03, 2.5665e-02,\n",
      "        8.7433e-03, 9.4528e-03, 1.5602e-02, 9.3765e-03, 5.2605e-03, 9.5215e-03,\n",
      "        1.2192e-02, 1.6586e-02, 6.7177e-03, 9.5215e-03, 6.3133e-03, 5.3635e-03,\n",
      "        6.8779e-03, 6.0043e-03, 7.2327e-03, 7.8125e-03, 1.5541e-02, 5.8670e-03,\n",
      "        1.5129e-02, 1.3847e-02, 1.4229e-02, 1.6586e-02, 1.2146e-02, 2.2629e-02,\n",
      "        2.4185e-02, 1.8646e-02, 2.5620e-02, 2.7328e-02, 1.6846e-02, 1.5671e-02,\n",
      "        1.2337e-02, 2.1118e-02, 3.5278e-02, 1.8539e-02, 1.2527e-02, 2.8809e-02,\n",
      "        1.3634e-02, 1.4175e-02, 1.3329e-02, 1.7517e-02, 7.1487e-03, 7.9346e-03,\n",
      "        7.7858e-03, 9.3765e-03, 6.8245e-03, 5.8212e-03, 8.2550e-03, 8.7814e-03,\n",
      "        8.3771e-03, 8.2550e-03, 7.6942e-03, 6.8779e-03, 8.5144e-03, 7.4043e-03,\n",
      "        7.0114e-03, 7.3471e-03, 1.4732e-02, 6.3400e-03, 5.7297e-03, 7.4883e-03,\n",
      "        8.5449e-03, 7.4615e-03, 7.5493e-03, 7.9651e-03, 7.8735e-03, 8.6136e-03,\n",
      "        6.8512e-03, 5.9357e-03, 9.1629e-03, 6.0730e-03, 6.9847e-03, 6.9313e-03,\n",
      "        6.1455e-03, 6.8779e-03, 8.3771e-03, 8.5449e-03, 1.4229e-02, 9.8190e-03,\n",
      "        1.1467e-02, 5.4893e-03, 7.9651e-03, 8.6746e-03, 5.7983e-03, 1.3794e-02,\n",
      "        7.3166e-03, 5.4893e-03, 5.2414e-03, 6.2637e-03, 7.0953e-03, 9.1934e-03,\n",
      "        1.3634e-02, 9.4147e-03, 1.6342e-02, 6.6414e-03, 6.0501e-03, 1.6586e-02,\n",
      "        4.4775e-04, 2.0798e-02, 7.1526e-04, 3.3760e-03, 2.4521e-02, 2.4529e-03,\n",
      "        6.7115e-05, 4.6806e-03, 3.0045e-02, 1.8387e-03, 1.1606e-03, 2.0096e-02,\n",
      "        3.1242e-03, 1.1162e-02, 1.9045e-03, 1.2924e-02, 5.5504e-04, 1.7639e-02,\n",
      "        5.2977e-04, 1.2482e-02, 2.5806e-03, 1.5249e-03, 4.3488e-03, 3.1433e-02,\n",
      "        8.6260e-04, 3.1853e-03, 2.1400e-03, 2.8467e-04, 1.2350e-03, 3.4428e-03,\n",
      "        3.5934e-03, 4.3144e-03, 3.8090e-03, 1.5430e-03, 1.2573e-02, 1.9121e-03,\n",
      "        1.2871e-02, 1.9646e-03, 1.6846e-02, 2.9125e-03, 2.9240e-03, 1.5717e-02,\n",
      "        3.7365e-03, 3.0960e-02, 1.9121e-03, 3.3627e-03, 1.0605e-03, 4.5395e-03,\n",
      "        5.7755e-03, 1.3046e-03, 9.0742e-04, 3.4428e-03, 1.4439e-03, 2.5215e-03,\n",
      "        1.5962e-04, 4.7188e-03, 1.4381e-03, 4.8866e-03, 4.6082e-03, 5.5122e-03,\n",
      "        6.2408e-03, 4.8485e-03, 2.4242e-03, 8.8310e-04, 2.0199e-03, 6.5117e-03,\n",
      "        1.5192e-03, 1.4896e-03, 1.8463e-03, 2.6722e-03, 1.5974e-03, 3.3894e-03,\n",
      "        1.5850e-03, 7.2327e-03, 4.3144e-03, 1.1120e-03, 2.3689e-03, 3.1967e-03,\n",
      "        8.5449e-03, 6.9201e-05, 2.5311e-03, 3.3112e-03, 6.1226e-03, 1.6870e-03,\n",
      "        2.9812e-03, 1.1780e-02, 1.8101e-03, 3.8395e-03, 1.7953e-04, 1.0857e-02,\n",
      "        2.4915e-04, 3.9139e-03, 3.8853e-03, 1.9569e-03, 7.3471e-03, 7.4883e-03,\n",
      "        1.2192e-02, 2.5616e-03, 1.0204e-03, 6.0501e-03, 5.6190e-03, 1.3304e-03,\n",
      "        6.9580e-03, 1.2924e-02, 1.6159e-02, 4.5204e-03, 1.1551e-02, 4.0703e-03,\n",
      "        1.7441e-02, 8.8871e-05, 1.8668e-04, 2.9926e-03, 8.3923e-04, 5.4474e-03,\n",
      "        1.7271e-03, 5.1193e-03, 4.5929e-03, 1.7681e-03, 7.0114e-03, 1.7810e-04,\n",
      "        1.3723e-03, 4.7278e-04, 6.3133e-03, 5.7297e-03, 3.4962e-03, 1.1070e-02,\n",
      "        1.3943e-03, 2.0275e-03, 1.3456e-03, 8.3466e-03, 6.7711e-03, 7.5817e-04,\n",
      "        2.6321e-03, 3.2482e-03, 4.8399e-04, 5.1928e-04, 9.2697e-03, 1.4210e-03,\n",
      "        1.3030e-04, 8.2016e-04, 1.2646e-03, 8.4591e-04, 1.6489e-03, 5.2414e-03,\n",
      "        2.5024e-03, 6.1913e-03, 6.0959e-03, 9.8896e-04, 1.8051e-02, 2.8467e-04,\n",
      "        1.3514e-03, 3.3927e-04, 3.6793e-03, 2.4343e-04, 1.4381e-03, 7.7915e-04,\n",
      "        2.3422e-03, 1.3943e-03, 2.5616e-03, 5.9357e-03, 6.7444e-03, 5.1613e-03,\n",
      "        6.5880e-03, 1.0319e-03, 2.5511e-04, 3.1996e-04, 3.5248e-03, 5.6000e-03,\n",
      "        1.4553e-03, 3.0780e-04, 4.9353e-04, 4.1008e-03, 2.9354e-03, 8.3771e-03,\n",
      "        2.8000e-03, 1.8969e-03, 1.0612e-02, 1.2337e-02, 1.1330e-02, 6.7472e-04,\n",
      "        7.5493e-03, 5.9814e-03, 8.5144e-03, 1.9951e-03, 3.6507e-03, 1.0529e-02,\n",
      "        9.2316e-03, 2.1992e-03, 2.6112e-03, 8.9493e-03, 5.1804e-03, 8.3160e-03,\n",
      "        1.0902e-02, 6.8512e-03, 4.2305e-03, 8.6746e-03, 4.6730e-04, 7.0667e-04,\n",
      "        1.7204e-03, 5.5933e-04, 5.9357e-03, 4.2000e-03, 4.0855e-03, 3.0065e-04,\n",
      "        2.4338e-03, 9.8953e-03, 6.0959e-03, 1.0004e-03, 2.8253e-04, 8.7976e-04,\n",
      "        1.0567e-02, 6.7444e-03, 9.1629e-03, 8.7595e-04, 3.5934e-03, 8.1863e-03,\n",
      "        3.9005e-03, 1.5669e-03, 3.9506e-04, 1.8826e-03, 4.6082e-03, 1.2159e-03,\n",
      "        1.0490e-02, 9.2554e-04, 7.8125e-03, 4.2648e-03, 5.7983e-03, 5.9357e-03,\n",
      "        1.0696e-02, 5.5542e-03, 3.7212e-03, 1.3351e-03, 1.8525e-04, 9.2173e-04,\n",
      "        1.4770e-04, 3.5429e-04, 3.1605e-03, 1.6356e-03, 6.1417e-04, 9.4833e-03,\n",
      "        1.9717e-04, 5.2185e-03, 3.9005e-03, 1.2894e-03, 4.7569e-03, 4.5133e-04,\n",
      "        6.0959e-03, 2.9469e-03, 3.4962e-03, 1.2941e-03, 2.3327e-03, 4.9629e-03,\n",
      "        9.2888e-04, 5.9748e-04, 2.7790e-03, 6.7177e-03, 4.0245e-03, 1.8673e-03,\n",
      "        5.0392e-03, 3.8395e-03, 8.9188e-03, 9.7809e-03, 3.1242e-03, 2.8114e-03,\n",
      "        1.8311e-03, 8.7261e-04, 4.7379e-03, 1.2941e-03, 4.0245e-03, 3.6068e-03,\n",
      "        7.0648e-03, 9.4528e-03, 4.1809e-03, 2.1992e-03, 5.0392e-03, 3.3665e-04,\n",
      "        5.9586e-03, 1.3351e-03, 3.6354e-03, 3.8242e-03, 2.6727e-04, 3.2978e-03,\n",
      "        5.5075e-04, 3.7079e-03, 1.7226e-05, 1.2529e-04, 2.7256e-03, 4.1656e-03,\n",
      "        4.6015e-04, 1.0208e-02, 2.2519e-04, 1.6464e-02, 6.2132e-04, 5.7526e-03,\n",
      "        3.5143e-04, 2.8782e-03, 6.1913e-03, 5.3024e-03, 8.4162e-05, 6.5880e-03,\n",
      "        1.3447e-04, 4.6082e-03, 1.2636e-04, 1.9493e-03, 6.7711e-03, 6.1455e-03,\n",
      "        2.8458e-03, 1.3561e-03, 5.1439e-05, 7.4615e-03, 6.0463e-04, 2.7676e-03,\n",
      "        7.8125e-03, 7.1526e-04, 2.4815e-03, 5.8899e-03, 3.3894e-03, 7.2384e-04,\n",
      "        2.0199e-03, 6.4125e-03, 6.0463e-04, 3.9139e-03, 1.2147e-04, 2.8682e-04,\n",
      "        1.1330e-02, 6.9046e-03, 3.1242e-03, 5.3024e-03, 3.6068e-03, 1.8826e-03,\n",
      "        3.7498e-03, 6.1226e-03, 1.1206e-03, 1.6870e-03, 4.3297e-03, 3.8242e-03,\n",
      "        3.0155e-03, 4.5929e-03, 1.9417e-03, 5.1804e-03, 1.6546e-03, 1.3876e-04,\n",
      "        5.1003e-03, 6.5660e-04, 7.4959e-04, 2.5129e-04, 6.2656e-04, 4.3297e-03,\n",
      "        2.8682e-04, 1.5917e-03, 2.2163e-03, 2.9125e-03, 1.0405e-03, 1.1325e-04,\n",
      "        1.3741e-02, 6.9046e-04, 6.4392e-03, 2.8553e-03, 2.2335e-03, 1.8826e-03,\n",
      "        9.7809e-03, 2.0580e-03, 4.0703e-03, 3.1018e-04, 1.5129e-02, 4.9162e-04,\n",
      "        4.4861e-03, 3.1738e-02, 1.2482e-02, 1.4061e-02, 5.4703e-03, 9.9277e-04,\n",
      "        7.4267e-05, 8.0729e-04, 3.8505e-05, 7.3135e-05, 8.1348e-04, 8.3771e-03,\n",
      "        4.0770e-04, 3.0632e-03, 1.9045e-03, 2.1000e-03, 4.1351e-03, 2.5511e-04,\n",
      "        2.2340e-04, 8.6260e-04, 2.8467e-04, 8.3590e-04, 2.8000e-03, 3.2759e-04,\n",
      "        3.6221e-03, 9.9957e-05, 2.6932e-03, 2.7905e-03, 6.2408e-03, 3.0746e-03,\n",
      "        1.9417e-03, 7.8535e-04, 1.4162e-03, 7.2002e-05, 3.4695e-03, 3.7498e-03,\n",
      "        6.0043e-03, 6.6948e-04, 4.8676e-03, 3.3627e-03, 1.5850e-03, 6.1684e-03,\n",
      "        1.0815e-03, 5.9586e-03, 1.5850e-03, 3.0270e-03, 7.6675e-03, 1.4105e-03,\n",
      "        1.4267e-03, 3.2496e-04, 1.9493e-03, 1.1158e-03, 1.0443e-03, 4.6134e-05,\n",
      "        9.0301e-05, 5.6419e-03, 3.0880e-03, 7.6592e-05, 4.3144e-03, 1.1734e-02,\n",
      "        4.4084e-04, 1.7548e-03, 1.6813e-03, 5.1117e-04, 2.3723e-05, 5.4474e-03,\n",
      "        2.5320e-04, 5.1022e-05, 3.5515e-03, 6.1684e-03, 7.2899e-03, 5.7983e-03,\n",
      "        1.8101e-03, 3.1605e-03, 4.5547e-03, 5.7399e-05, 2.9812e-03, 4.6372e-04,\n",
      "        5.2605e-03, 4.8866e-03, 2.1744e-03, 9.0179e-03, 9.8515e-04, 7.9346e-03,\n",
      "        4.6730e-04, 2.1152e-03, 2.1248e-03, 3.1018e-04, 1.6108e-03, 1.5473e-04,\n",
      "        6.9847e-03, 2.0351e-03, 8.1558e-03, 9.1124e-04, 1.8826e-03, 8.9874e-03,\n",
      "        7.0953e-03, 5.0011e-03, 5.3215e-03, 4.1008e-03, 2.5311e-03, 1.4553e-03,\n",
      "        1.1683e-04, 6.4850e-04, 9.3877e-05, 4.8866e-03, 3.9160e-05, 3.7122e-04,\n",
      "        2.1324e-03, 5.7983e-03, 1.2100e-02, 4.7660e-04, 1.4381e-03, 5.3787e-04,\n",
      "        6.0940e-04, 9.3269e-04, 8.7261e-04, 3.9673e-04, 1.9569e-03, 2.9469e-03,\n",
      "        9.9716e-03, 5.5351e-03, 7.7248e-03, 5.8603e-04, 2.4719e-03, 1.1200e-02,\n",
      "        1.2772e-02, 3.2496e-04, 2.7466e-03, 2.8253e-04, 1.7548e-03, 4.1161e-03,\n",
      "        3.0780e-04, 4.2391e-04, 3.3379e-03, 1.2941e-03, 3.3545e-04, 1.6937e-03,\n",
      "        2.0580e-03, 1.7271e-03, 4.6806e-03, 7.2060e-03, 4.2000e-03, 3.0270e-03,\n",
      "        8.2169e-03, 9.9716e-03, 1.0815e-03, 8.3506e-05, 1.3876e-04, 7.0953e-04,\n",
      "        2.6836e-03, 1.1683e-04, 6.1913e-03, 2.1648e-03, 2.2087e-03, 2.2793e-03,\n",
      "        2.3136e-03, 8.5144e-03, 4.2801e-03, 2.2526e-03, 2.5616e-03, 1.8382e-04,\n",
      "        2.5024e-03, 5.5265e-04, 6.5899e-04, 1.6813e-03, 2.0752e-03, 6.4373e-04,\n",
      "        5.0621e-03, 3.9983e-04, 3.8290e-04, 7.8249e-04, 5.5933e-04, 7.4959e-04,\n",
      "        4.9543e-04, 8.5449e-03, 2.0199e-03, 2.1744e-03, 3.2597e-03, 7.2384e-04,\n",
      "        9.9957e-05, 1.0605e-03, 3.5248e-03, 6.4373e-04, 5.7268e-04, 9.3384e-03,\n",
      "        2.6016e-03, 1.9121e-03, 1.2052e-04, 1.6747e-03, 2.5024e-03, 2.5311e-03,\n",
      "        1.9188e-03, 3.9749e-03, 3.0041e-03, 1.1787e-03, 5.4893e-03, 3.9291e-03,\n",
      "        3.1967e-03, 3.3379e-03, 1.1740e-03, 2.8896e-03, 4.0855e-03, 8.3466e-03,\n",
      "        2.7142e-03, 6.0959e-03, 5.2414e-03, 8.2169e-03, 6.0501e-03, 7.4883e-03,\n",
      "        2.0504e-03, 1.3247e-03, 6.9857e-04, 3.0780e-04, 6.4392e-03, 1.4048e-03,\n",
      "        1.5602e-02, 5.4474e-03, 4.3297e-03, 1.8646e-02, 6.7978e-03, 8.4457e-03,\n",
      "        2.1164e-02, 7.1812e-04, 7.7858e-03, 1.3046e-03, 2.1076e-03, 7.4883e-03,\n",
      "        1.0986e-02, 1.0529e-03, 4.9629e-03, 1.1597e-02, 8.5592e-04, 7.3195e-04,\n",
      "        6.9847e-03, 3.5515e-03, 7.3738e-03, 7.4883e-03, 8.8120e-03, 6.0272e-03,\n",
      "        5.7297e-03, 8.0729e-04, 6.5880e-03, 2.8229e-03, 5.1737e-04, 7.7553e-03,\n",
      "        2.9697e-03, 5.5933e-04, 2.8782e-03, 9.3765e-03, 3.0065e-04, 5.2414e-03,\n",
      "        6.5899e-04, 7.0953e-04, 8.3771e-03, 3.9749e-03, 2.2793e-03, 3.4027e-03,\n",
      "        2.1324e-03, 5.4054e-03, 1.4839e-02, 1.1200e-02, 1.1330e-02, 1.0246e-02,\n",
      "        2.2720e-02, 2.9240e-03, 2.5902e-03, 2.4815e-03, 6.7711e-03, 5.2185e-03,\n",
      "        1.8606e-03, 1.7380e-02, 4.8103e-03, 1.3023e-02, 1.2726e-02, 1.1246e-02,\n",
      "        9.5978e-03, 7.4883e-03, 1.9089e-02, 1.2672e-02, 9.8953e-03, 1.2772e-02,\n",
      "        4.5300e-04, 1.4229e-02, 1.0132e-02, 1.4954e-02, 1.3741e-02, 8.6441e-03,\n",
      "        8.3466e-03, 8.0948e-03, 1.5427e-02, 1.0010e-02, 1.6281e-02, 8.9493e-03,\n",
      "        2.0172e-02, 1.7242e-02, 1.8234e-02, 1.2871e-02, 1.1871e-02, 8.3160e-03,\n",
      "        1.2383e-02, 9.4147e-03, 2.0767e-02, 2.7676e-03, 1.1871e-02, 1.3329e-02,\n",
      "        1.7990e-02, 1.0452e-02, 1.0651e-02, 1.2573e-02, 6.3400e-03, 1.4282e-02,\n",
      "        1.2146e-02, 1.8433e-02, 8.9188e-03, 1.4008e-02, 1.3275e-02, 9.9335e-03,\n",
      "        1.9913e-02, 8.6746e-03, 6.3629e-03, 1.0612e-02, 3.3386e-02, 2.2324e-02,\n",
      "        1.3481e-02, 7.9041e-03, 2.0065e-02, 7.5188e-03, 7.5493e-03, 2.2202e-02,\n",
      "        4.0703e-03, 3.3386e-02, 2.7256e-03, 1.4839e-02, 4.0100e-02, 3.7262e-02,\n",
      "        6.9275e-02, 4.8676e-03, 6.7200e-02, 6.2317e-02, 2.8809e-02, 9.0179e-03,\n",
      "        1.7044e-02, 1.7990e-02, 1.9089e-02, 3.3661e-02, 3.4607e-02, 2.2629e-02,\n",
      "        6.7444e-02, 5.9753e-02, 4.5105e-02, 5.1544e-02, 1.4557e-02, 3.8177e-02,\n",
      "        1.1029e-01, 6.3721e-02, 4.9316e-02, 6.5125e-02, 5.0140e-02, 9.8190e-03,\n",
      "        2.8229e-03, 3.5553e-02, 5.0995e-02, 1.1597e-01, 7.3181e-02, 1.4160e-01,\n",
      "        1.7383e-01, 9.0393e-02, 8.1543e-02, 1.0596e-01, 5.9326e-02, 5.6854e-02,\n",
      "        1.4819e-01, 2.2156e-02, 6.6467e-02, 3.9429e-02, 3.7811e-02, 1.1499e-01,\n",
      "        8.9478e-02, 3.4180e-02, 8.7585e-02, 6.1768e-02, 2.0645e-02, 8.6914e-02,\n",
      "        1.9501e-02, 3.9642e-02, 1.4786e-02, 1.3428e-02, 8.0200e-02, 5.0415e-02,\n",
      "        1.9791e-02, 6.9519e-02, 2.6611e-02, 1.5125e-03, 5.8014e-02, 3.0792e-02,\n",
      "        6.7932e-02, 7.6843e-02, 8.3435e-02, 1.2988e-01, 8.5083e-02, 5.2704e-02,\n",
      "        9.0759e-02, 6.8054e-02, 1.3794e-02, 2.1866e-02, 2.9587e-02, 1.6342e-02,\n",
      "        6.3721e-02, 3.7109e-02, 4.1626e-02, 6.2927e-02, 5.2246e-02, 2.4750e-02,\n",
      "        8.0811e-02, 2.3865e-02, 2.2087e-03, 4.0245e-03, 3.6713e-02, 9.1248e-02,\n",
      "        6.5857e-02, 9.2224e-02, 1.2524e-01, 7.8247e-02, 4.6112e-02, 1.0614e-01,\n",
      "        1.1032e-02], dtype=torch.float16), 'confidence': tensor(0.1738, dtype=torch.float16), 'last_hidden_state': tensor([ 0.8540, -0.8613,  0.6768,  0.9399,  0.9077,  0.8320, -0.9473,  0.8091,\n",
      "        -0.9287,  0.6396, -0.9077, -0.8765, -0.8599,  0.9028, -0.7363, -0.9502,\n",
      "         0.8970, -0.8994,  0.9194, -0.9448, -0.4731,  0.8428,  0.8638, -0.9243,\n",
      "         0.7119, -0.9385,  0.7754,  0.0895, -0.5718,  0.9062, -0.9600, -0.7856,\n",
      "         0.7427,  0.8706,  0.7583, -0.8535,  0.9316, -0.8799, -0.8838,  0.7031,\n",
      "         0.9023,  0.5796,  0.8687, -0.8984, -0.9395,  0.8491, -0.9214, -0.7422,\n",
      "         0.9268, -0.6411, -0.7671, -0.8740, -0.8125, -0.9111, -0.6948,  0.9209,\n",
      "         0.9390,  0.8350, -0.8726, -0.8784, -0.2130,  0.5366,  0.8213,  0.9341,\n",
      "         0.4441,  0.8906,  0.9043,  0.9404,  0.8887, -0.8535,  0.8379, -0.8457,\n",
      "         0.9434, -0.9170, -0.7632,  0.6328, -0.6201, -0.2947,  0.6641,  0.7935,\n",
      "        -0.9385,  0.8955, -0.7026, -0.9414,  0.9263, -0.8799, -0.7998,  0.8721,\n",
      "        -0.4299,  0.8574, -0.4619,  0.7573,  0.9336, -0.9199, -0.9268, -0.6592,\n",
      "        -0.8965, -0.6270, -0.7017,  0.8994, -0.8379, -0.8813,  0.8594, -0.8379,\n",
      "         0.5571,  0.6475, -0.7788,  0.9409,  0.9155,  0.9224,  0.8237,  0.9087,\n",
      "        -0.9409,  0.9355,  0.8442,  0.9468,  0.7632, -0.8315, -0.8916,  0.4138,\n",
      "        -0.5889, -0.8687, -0.8745,  0.9272,  0.9131,  0.7476, -0.8867,  0.8506,\n",
      "         0.9624, -0.9482, -0.7812,  0.3025, -0.7690, -0.8799,  0.9336, -0.7783,\n",
      "         0.4563,  0.8784,  0.9580, -0.8892, -0.8892,  0.9014, -0.7974, -0.8706,\n",
      "        -0.9590, -0.7505,  0.9204, -0.9077, -0.9263,  0.8672,  0.4788,  0.8770,\n",
      "        -0.8682, -0.7412,  0.3552,  0.0376, -0.8721,  0.9473,  0.9360, -0.9336,\n",
      "         0.8438,  0.9014,  0.8872,  0.8867, -0.5635, -0.9033,  0.7515,  0.6777,\n",
      "         0.9326,  0.3152,  0.6816,  0.1400, -0.8257,  0.9668, -0.9282, -0.8818,\n",
      "         0.9062,  0.8691, -0.7686, -0.8506,  0.7192,  0.9233, -0.8154,  0.8945,\n",
      "        -0.3059, -0.8892,  0.8677,  0.8882, -0.9561, -0.2220,  0.8320,  0.9458,\n",
      "         0.9248, -0.7026,  0.8906, -0.8784, -0.7622,  0.9331, -0.9155,  0.9141,\n",
      "         0.6494, -0.7881, -0.5508, -0.8740,  0.6958,  0.8447,  0.9292,  0.8916,\n",
      "        -0.5918,  0.6895,  0.6597,  0.3149,  0.9536, -0.9268,  0.2893,  0.9102,\n",
      "         0.6201, -0.8823, -0.8691,  0.9292, -0.8179, -0.5474, -0.8706,  0.8955,\n",
      "         0.7759, -0.8467,  0.9214,  0.8218,  0.0947, -0.7666, -0.8701, -0.8936,\n",
      "        -0.9160, -0.8125,  0.0409,  0.8130, -0.9268,  0.3782,  0.5464, -0.8657,\n",
      "         0.9160,  0.7241, -0.9419,  0.9243,  0.9194, -0.9146, -0.5742,  0.8560,\n",
      "        -0.7788,  0.7007, -0.7212,  0.8716,  0.8438, -0.9346,  0.8853, -0.4648,\n",
      "        -0.8906,  0.3010,  0.6753, -0.8657,  0.5649, -0.8818,  0.8618,  0.8408,\n",
      "        -0.8999, -0.9243, -0.9624, -0.3611,  0.9033, -0.8809, -0.7632, -0.8926,\n",
      "         0.9233, -0.9067, -0.8066, -0.9272, -0.9399,  0.7856, -0.9336,  0.8735,\n",
      "        -0.7598, -0.9175,  0.8687,  0.6304,  0.5884,  0.8906, -0.9478,  0.4395,\n",
      "        -0.6104,  0.7832,  0.4395,  0.7217, -0.7974, -0.9375,  0.9268,  0.9272,\n",
      "         0.8652, -0.7285, -0.4578, -0.6465,  0.7769,  0.9351, -0.8013, -0.8257,\n",
      "         0.8613,  0.9219, -0.8633, -0.9092, -0.8623, -0.8398, -0.9077, -0.5342,\n",
      "         0.8774,  0.9038,  0.7905, -0.8477,  0.7817,  0.8120,  0.8247, -0.1003,\n",
      "         0.9399,  0.7334, -0.6714,  0.6670, -0.8369,  0.8965, -0.7612,  0.7754,\n",
      "         0.7798,  0.8701, -0.7676, -0.7344, -0.8428,  0.8843,  0.2747, -0.4353,\n",
      "         0.8281, -0.8774,  0.7485, -0.9336,  0.2340, -0.9507, -0.9009,  0.9053,\n",
      "         0.7534, -0.2433, -0.1542,  0.9116, -0.9355,  0.4290, -0.8403, -0.3716,\n",
      "         0.9644, -0.1918,  0.8516,  0.9282,  0.8022,  0.9556, -0.6978,  0.8613,\n",
      "         0.8022, -0.8906,  0.2428,  0.8828, -0.9067,  0.9277, -0.9580, -0.8911,\n",
      "        -0.4648,  0.3113, -0.4541,  0.9355, -0.9238,  0.8999, -0.8389,  0.9102,\n",
      "        -0.8584, -0.9453, -0.4915, -0.5928,  0.8457,  0.9263, -0.9199,  0.7256,\n",
      "        -0.8979, -0.8022, -0.9326,  0.9106,  0.8945, -0.9106,  0.8921, -0.9414,\n",
      "         0.8833,  0.9438,  0.8682, -0.3999, -0.8164,  0.7573,  0.8315,  0.8428,\n",
      "         0.9048,  0.9395,  0.8706,  0.8662,  0.6450,  0.8672,  0.9512, -0.7988,\n",
      "         0.8984,  0.7334,  0.7905,  0.9355,  0.7964, -0.8169,  0.9326, -0.9346,\n",
      "        -0.6641,  0.3777,  0.4856,  0.9082, -0.8657,  0.8833, -0.9619,  0.9360,\n",
      "         0.9316, -0.8989,  0.8735, -0.9277,  0.8906, -0.5635,  0.8945, -0.5601,\n",
      "         0.5391,  0.8320, -0.9404,  0.8633,  0.9321,  0.8887, -0.9453,  0.9307,\n",
      "        -0.7324,  0.9072,  0.9531,  0.9331,  0.5215,  0.8896,  0.8721,  0.5391,\n",
      "        -0.9653,  0.8887, -0.2502,  0.8589, -0.9033,  0.8984, -0.8359, -0.9058,\n",
      "         0.9287,  0.8491, -0.7080, -0.6836, -0.9272, -0.8882, -0.8179, -0.8403,\n",
      "         0.9233,  0.9473, -0.7988,  0.8877, -0.8965, -0.9521, -0.8174,  0.7207,\n",
      "        -0.9146, -0.9131, -0.8540,  0.9194,  0.4844,  0.8862, -0.8726,  0.9067,\n",
      "         0.9102, -0.8433,  0.8755, -0.8965,  0.6929,  0.8916, -0.9492, -0.9204,\n",
      "        -0.7686,  0.8945, -0.9370, -0.8896,  0.3916,  0.8193,  0.9048,  0.0468,\n",
      "         0.7646,  0.9307, -0.8188, -0.7441,  0.9355, -0.9434, -0.3218, -0.3596,\n",
      "         0.8389,  0.3752,  0.9219, -0.7749,  0.8774,  0.8462, -0.9482, -0.8740],\n",
      "       dtype=torch.float16)}\n",
      "==================================================\n",
      "{'predictions': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), 'logits': tensor([4.9629e-03, 4.9820e-03, 3.9902e-03, 3.2482e-03, 1.1597e-02, 7.5760e-03,\n",
      "        3.2597e-03, 5.1613e-03, 5.0621e-03, 4.7722e-03, 5.8441e-03, 4.6997e-03,\n",
      "        7.4883e-03, 4.1656e-03, 3.1853e-03, 7.4043e-03, 2.7790e-03, 2.5311e-03,\n",
      "        3.4161e-03, 7.2899e-03, 6.0959e-03, 3.5381e-03, 4.9629e-03, 4.4670e-03,\n",
      "        5.4283e-03, 4.5052e-03, 8.5754e-03, 8.4763e-03, 4.0703e-03, 1.1963e-02,\n",
      "        4.7188e-03, 4.1161e-03, 6.8512e-03, 4.3297e-03, 3.0518e-03, 4.6082e-03,\n",
      "        5.5771e-03, 6.1913e-03, 2.6722e-03, 3.9005e-03, 5.5351e-03, 4.4518e-03,\n",
      "        6.4888e-03, 4.9438e-03, 5.6419e-03, 6.4621e-03, 7.2060e-03, 5.8899e-03,\n",
      "        3.2349e-03, 6.6910e-03, 7.7858e-03, 7.5188e-03, 7.2060e-03, 1.1826e-02,\n",
      "        6.6414e-03, 1.1375e-02, 9.4833e-03, 1.4732e-02, 8.0948e-03, 1.0696e-02,\n",
      "        8.4457e-03, 1.0490e-02, 1.8829e-02, 1.0567e-02, 6.8779e-03, 1.1467e-02,\n",
      "        8.8120e-03, 9.3002e-03, 9.1248e-03, 8.3466e-03, 5.8212e-03, 6.6910e-03,\n",
      "        5.8899e-03, 1.1826e-02, 5.9128e-03, 6.2904e-03, 6.4392e-03, 6.8779e-03,\n",
      "        5.6190e-03, 7.1487e-03, 4.3488e-03, 5.7526e-03, 6.4621e-03, 5.6000e-03,\n",
      "        5.4893e-03, 7.5760e-03, 8.7433e-03, 5.0201e-03, 3.6507e-03, 5.5771e-03,\n",
      "        5.9586e-03, 5.1994e-03, 5.1003e-03, 4.8866e-03, 5.9814e-03, 5.5122e-03,\n",
      "        6.0959e-03, 3.9444e-03, 6.9580e-03, 5.6000e-03, 5.4283e-03, 5.5771e-03,\n",
      "        4.9438e-03, 6.1684e-03, 6.5384e-03, 7.5493e-03, 7.8125e-03, 5.9128e-03,\n",
      "        6.4888e-03, 4.7722e-03, 6.0501e-03, 7.2594e-03, 5.0621e-03, 6.2637e-03,\n",
      "        5.3864e-03, 4.0703e-03, 4.5204e-03, 5.1994e-03, 5.7526e-03, 9.1934e-03,\n",
      "        5.6190e-03, 5.4893e-03, 8.1558e-03, 5.3864e-03, 5.3024e-03, 8.5754e-03,\n",
      "        1.3447e-04, 1.2337e-02, 4.3058e-04, 1.9951e-03, 1.1070e-02, 6.7997e-04,\n",
      "        2.1756e-05, 1.3409e-03, 1.7105e-02, 1.2493e-03, 5.4646e-04, 1.0208e-02,\n",
      "        1.0986e-03, 6.3400e-03, 6.1178e-04, 6.2904e-03, 1.9407e-04, 1.0170e-02,\n",
      "        1.3030e-04, 7.0648e-03, 6.4850e-04, 7.6437e-04, 1.6356e-03, 1.6464e-02,\n",
      "        3.9506e-04, 9.3985e-04, 6.4373e-04, 9.6858e-05, 8.4925e-04, 8.6260e-04,\n",
      "        1.2493e-03, 1.9121e-03, 1.3723e-03, 4.1580e-04, 7.3471e-03, 3.7861e-04,\n",
      "        6.9580e-03, 4.5300e-04, 9.4833e-03, 7.4673e-04, 1.4839e-03, 8.4457e-03,\n",
      "        8.9693e-04, 1.9348e-02, 1.3828e-03, 2.4433e-03, 3.2496e-04, 1.8177e-03,\n",
      "        2.7466e-03, 7.1526e-04, 5.0545e-04, 3.8700e-03, 1.9264e-03, 1.7271e-03,\n",
      "        1.0312e-04, 3.1967e-03, 1.2398e-03, 2.1648e-03, 3.0155e-03, 5.3864e-03,\n",
      "        1.3723e-03, 1.0529e-03, 1.4725e-03, 4.6921e-04, 8.5592e-04, 5.7983e-03,\n",
      "        1.0004e-03, 8.0395e-04, 9.9277e-04, 1.1473e-03, 1.1740e-03, 1.3943e-03,\n",
      "        7.4959e-04, 4.2000e-03, 2.4147e-03, 8.2302e-04, 1.3351e-03, 9.0027e-04,\n",
      "        8.7128e-03, 2.4319e-05, 2.9926e-03, 3.0403e-03, 3.7956e-03, 7.9155e-04,\n",
      "        2.4719e-03, 4.9820e-03, 5.8365e-04, 1.1120e-03, 6.6042e-05, 8.7128e-03,\n",
      "        7.9036e-05, 3.0518e-03, 4.0855e-03, 1.8311e-03, 5.7526e-03, 5.7297e-03,\n",
      "        1.3023e-02, 1.1425e-03, 5.8603e-04, 3.2730e-03, 3.1471e-03, 6.6423e-04,\n",
      "        4.6654e-03, 6.2408e-03, 3.6926e-03, 2.9011e-03, 9.6741e-03, 1.8101e-03,\n",
      "        1.4282e-02, 3.6776e-05, 6.9737e-05, 1.5192e-03, 4.2391e-04, 3.8548e-03,\n",
      "        6.9332e-04, 2.2335e-03, 1.8101e-03, 6.0701e-04, 3.5248e-03, 7.2539e-05,\n",
      "        3.5977e-04, 2.3234e-04, 2.8114e-03, 2.6627e-03, 1.8387e-03, 9.3384e-03,\n",
      "        5.0926e-04, 7.6437e-04, 4.7469e-04, 3.3760e-03, 2.6722e-03, 2.3973e-04,\n",
      "        1.2894e-03, 1.7204e-03, 1.7953e-04, 3.3545e-04, 2.9125e-03, 3.9673e-04,\n",
      "        4.2975e-05, 1.7810e-04, 5.2118e-04, 3.3021e-04, 1.0777e-03, 2.6112e-03,\n",
      "        1.0481e-03, 2.5024e-03, 2.6932e-03, 7.4100e-04, 6.1913e-03, 1.1063e-04,\n",
      "        4.6372e-04, 1.6475e-04, 1.8606e-03, 8.0287e-05, 4.4775e-04, 4.9925e-04,\n",
      "        7.3528e-04, 4.5133e-04, 9.3985e-04, 2.9469e-03, 3.5934e-03, 1.9722e-03,\n",
      "        3.0994e-03, 4.5657e-04, 1.1325e-04, 1.2529e-04, 2.1572e-03, 2.6417e-03,\n",
      "        6.3610e-04, 1.2732e-04, 2.5725e-04, 3.2864e-03, 1.3351e-03, 2.6207e-03,\n",
      "        1.2493e-03, 1.2550e-03, 9.5978e-03, 1.2291e-02, 1.1032e-02, 1.6344e-04,\n",
      "        4.5052e-03, 3.9902e-03, 5.1613e-03, 1.2550e-03, 2.8782e-03, 7.6370e-03,\n",
      "        7.2899e-03, 6.7711e-04, 1.3247e-03, 7.1754e-03, 2.3422e-03, 2.1572e-03,\n",
      "        4.0703e-03, 4.0703e-03, 1.5736e-03, 5.9357e-03, 2.1482e-04, 4.3058e-04,\n",
      "        3.2496e-04, 3.1495e-04, 5.6648e-03, 1.9188e-03, 1.3618e-03, 1.5354e-04,\n",
      "        5.9319e-04, 6.4621e-03, 3.2482e-03, 5.6791e-04, 1.4889e-04, 4.8971e-04,\n",
      "        7.4043e-03, 6.1684e-03, 2.9697e-03, 1.9264e-04, 9.1839e-04, 3.2101e-03,\n",
      "        1.4553e-03, 4.7112e-04, 2.9826e-04, 7.4673e-04, 3.7365e-03, 5.4836e-04,\n",
      "        3.9749e-03, 6.6423e-04, 4.8485e-03, 1.9722e-03, 3.8242e-03, 3.2101e-03,\n",
      "        3.9005e-03, 3.4161e-03, 1.0166e-03, 3.9053e-04, 6.8128e-05, 3.8004e-04,\n",
      "        4.3988e-05, 1.8966e-04, 2.5902e-03, 5.7936e-04, 2.7156e-04, 7.4043e-03,\n",
      "        5.6505e-05, 4.0550e-03, 2.0351e-03, 4.0293e-04, 2.1915e-03, 1.7536e-04,\n",
      "        1.3828e-03, 9.8896e-04, 1.7681e-03, 3.6120e-04, 6.7997e-04, 3.0632e-03,\n",
      "        6.6423e-04, 3.2258e-04, 1.7071e-03, 5.1613e-03, 1.2302e-03, 9.3985e-04,\n",
      "        3.9005e-03, 1.4553e-03, 9.9335e-03, 4.8866e-03, 1.7958e-03, 1.5249e-03,\n",
      "        9.7370e-04, 3.7694e-04, 1.8606e-03, 1.0529e-03, 3.5801e-03, 3.4161e-03,\n",
      "        6.9580e-03, 1.0612e-02, 3.3760e-03, 6.4373e-04, 3.2597e-03, 1.4198e-04,\n",
      "        8.3160e-03, 5.0735e-04, 1.9341e-03, 1.6870e-03, 2.4724e-04, 1.9264e-03,\n",
      "        1.4770e-04, 2.0027e-03, 6.5565e-06, 4.6492e-05, 1.6165e-03, 2.0103e-03,\n",
      "        2.5129e-04, 4.8103e-03, 8.4817e-05, 1.0567e-02, 2.4343e-04, 5.5122e-03,\n",
      "        4.5824e-04, 1.1654e-03, 2.3880e-03, 2.8114e-03, 3.8803e-05, 5.0621e-03,\n",
      "        5.3883e-05, 2.8782e-03, 3.5644e-05, 1.8826e-03, 6.1455e-03, 2.9240e-03,\n",
      "        1.2789e-03, 4.5466e-04, 2.6464e-05, 4.5547e-03, 3.1495e-04, 1.1473e-03,\n",
      "        4.2992e-03, 3.1257e-04, 2.1744e-03, 3.8853e-03, 2.4529e-03, 2.6536e-04,\n",
      "        9.2173e-04, 3.6354e-03, 4.7660e-04, 1.7138e-03, 9.9182e-05, 1.6999e-04,\n",
      "        7.3166e-03, 3.6640e-03, 1.8387e-03, 2.0275e-03, 1.8826e-03, 5.3978e-04,\n",
      "        2.4719e-03, 2.5024e-03, 6.2895e-04, 5.0735e-04, 2.3232e-03, 1.2064e-03,\n",
      "        1.6937e-03, 1.6937e-03, 1.1034e-03, 1.8606e-03, 8.5592e-04, 5.2691e-05,\n",
      "        4.4518e-03, 3.6979e-04, 2.5725e-04, 1.3554e-04, 2.0337e-04, 1.2255e-03,\n",
      "        9.3162e-05, 9.1457e-04, 2.4719e-03, 1.2789e-03, 3.4070e-04, 3.1710e-05,\n",
      "        5.9357e-03, 3.2759e-04, 2.1744e-03, 9.2888e-04, 7.2956e-04, 7.1526e-04,\n",
      "        4.1809e-03, 7.3195e-04, 2.5902e-03, 1.6868e-04, 1.3535e-02, 1.3030e-04,\n",
      "        4.9248e-03, 1.3741e-02, 6.0730e-03, 6.7711e-03, 2.9926e-03, 5.0354e-04,\n",
      "        4.9889e-05, 5.2977e-04, 2.1100e-05, 5.5194e-05, 4.3559e-04, 1.0010e-02,\n",
      "        3.8600e-04, 1.5736e-03, 2.6016e-03, 9.9659e-04, 2.5616e-03, 7.2002e-05,\n",
      "        8.0287e-05, 2.5129e-04, 1.0389e-04, 4.1080e-04, 1.6489e-03, 6.7115e-05,\n",
      "        1.0729e-03, 4.4703e-05, 3.4561e-03, 3.3112e-03, 3.3112e-03, 7.6151e-04,\n",
      "        9.4366e-04, 3.3283e-04, 8.1682e-04, 2.5272e-05, 4.9248e-03, 1.3456e-03,\n",
      "        2.1000e-03, 8.4591e-04, 2.9469e-03, 2.7466e-03, 1.5793e-03, 3.1719e-03,\n",
      "        1.5736e-03, 4.2801e-03, 2.1000e-03, 2.1152e-03, 6.2408e-03, 1.1883e-03,\n",
      "        1.3781e-03, 1.5354e-04, 1.1425e-03, 5.2118e-04, 3.4189e-04, 3.3975e-05,\n",
      "        5.7399e-05, 6.2637e-03, 1.3456e-03, 2.5868e-05, 2.8667e-03, 1.4618e-02,\n",
      "        1.5724e-04, 2.5711e-03, 6.5899e-04, 1.9264e-04, 7.3314e-06, 2.5616e-03,\n",
      "        1.2529e-04, 2.1935e-05, 4.5204e-03, 1.0330e-02, 1.4954e-02, 9.7427e-03,\n",
      "        5.7936e-04, 8.9693e-04, 1.6747e-03, 1.9670e-05, 1.6356e-03, 2.6941e-04,\n",
      "        1.3094e-03, 2.1152e-03, 1.2064e-03, 3.9291e-03, 5.0354e-04, 4.2000e-03,\n",
      "        1.6224e-04, 9.9277e-04, 2.2526e-03, 1.0639e-04, 7.6723e-04, 1.1504e-04,\n",
      "        9.8953e-03, 2.7790e-03, 9.2697e-03, 3.6979e-04, 8.9359e-04, 1.3222e-02,\n",
      "        3.3894e-03, 8.4763e-03, 2.6531e-03, 1.9722e-03, 2.5616e-03, 1.0691e-03,\n",
      "        4.1306e-05, 5.8365e-04, 5.8293e-05, 1.9875e-03, 2.2113e-05, 3.1996e-04,\n",
      "        1.6232e-03, 2.6207e-03, 1.3428e-02, 2.8920e-04, 5.4216e-04, 4.1580e-04,\n",
      "        5.1117e-04, 8.4257e-04, 9.1124e-04, 2.7370e-04, 8.2970e-04, 1.3094e-03,\n",
      "        4.8676e-03, 2.8000e-03, 3.6793e-03, 2.2173e-04, 1.3885e-03, 6.1455e-03,\n",
      "        5.5351e-03, 8.0943e-05, 7.8535e-04, 1.0806e-04, 8.3923e-04, 2.4147e-03,\n",
      "        1.3137e-04, 1.5724e-04, 1.0948e-03, 1.0080e-03, 2.5320e-04, 1.3885e-03,\n",
      "        1.5430e-03, 6.3133e-04, 3.2349e-03, 1.0094e-02, 6.3896e-03, 4.1008e-03,\n",
      "        3.5114e-03, 3.7079e-03, 4.5657e-04, 3.1471e-05, 1.2338e-04, 2.7800e-04,\n",
      "        1.2350e-03, 3.9756e-05, 4.8294e-03, 8.3923e-04, 9.1839e-04, 8.3303e-04,\n",
      "        8.1348e-04, 3.4828e-03, 2.2697e-03, 1.4725e-03, 8.2970e-04, 1.4198e-04,\n",
      "        1.0948e-03, 3.3927e-04, 4.4417e-04, 1.7614e-03, 9.1839e-04, 5.2118e-04,\n",
      "        2.2259e-03, 2.5129e-04, 2.4533e-04, 4.6563e-04, 2.6941e-04, 4.0770e-04,\n",
      "        3.3665e-04, 6.5117e-03, 8.8978e-04, 1.4668e-03, 1.5488e-03, 3.8290e-04,\n",
      "        4.1664e-05, 3.1257e-04, 1.1606e-03, 3.0780e-04, 1.9109e-04, 4.6463e-03,\n",
      "        6.4850e-04, 1.9798e-03, 1.0151e-04, 2.5711e-03, 3.9005e-03, 3.9444e-03,\n",
      "        3.2234e-03, 3.0632e-03, 2.4242e-03, 6.5136e-04, 3.4428e-03, 2.0504e-03,\n",
      "        4.0550e-03, 3.8395e-03, 1.1072e-03, 3.4294e-03, 5.1384e-03, 6.9313e-03,\n",
      "        4.3144e-03, 2.6932e-03, 5.4283e-03, 8.4457e-03, 2.6932e-03, 3.2730e-03,\n",
      "        7.4959e-04, 3.8910e-04, 2.6727e-04, 2.8253e-04, 5.1804e-03, 8.5258e-04,\n",
      "        1.4175e-02, 2.4147e-03, 1.5974e-03, 1.6281e-02, 1.0452e-02, 7.8735e-03,\n",
      "        3.1021e-02, 5.5504e-04, 7.0953e-03, 5.5504e-04, 1.8387e-03, 6.0501e-03,\n",
      "        2.8458e-03, 2.3234e-04, 1.1034e-03, 3.6354e-03, 2.0826e-04, 1.9717e-04,\n",
      "        2.6207e-03, 1.3561e-03, 1.4553e-03, 1.7071e-03, 2.0275e-03, 1.4725e-03,\n",
      "        1.3046e-03, 3.0303e-04, 1.0653e-03, 9.6989e-04, 2.4533e-04, 3.0880e-03,\n",
      "        1.1034e-03, 1.9264e-04, 9.5463e-04, 4.4518e-03, 1.1414e-04, 2.4052e-03,\n",
      "        2.0659e-04, 2.3413e-04, 2.6531e-03, 1.4668e-03, 7.6437e-04, 7.7629e-04,\n",
      "        6.0940e-04, 1.4324e-03, 5.4474e-03, 1.9951e-03, 2.4147e-03, 1.9798e-03,\n",
      "        8.5144e-03, 7.0667e-04, 6.4850e-04, 8.5258e-04, 1.6165e-03, 1.7138e-03,\n",
      "        5.9748e-04, 8.7128e-03, 1.7204e-03, 7.6065e-03, 6.8779e-03, 5.2414e-03,\n",
      "        4.6463e-03, 3.3760e-03, 1.0056e-02, 6.5384e-03, 3.8548e-03, 6.3896e-03,\n",
      "        1.5962e-04, 6.6681e-03, 4.5395e-03, 7.4310e-03, 6.2180e-03, 3.3245e-03,\n",
      "        3.5248e-03, 3.6221e-03, 8.8501e-03, 4.7569e-03, 6.8779e-03, 4.2992e-03,\n",
      "        1.0902e-02, 8.3160e-03, 9.4528e-03, 7.1220e-03, 5.0201e-03, 3.5248e-03,\n",
      "        4.2152e-03, 5.1994e-03, 1.2238e-02, 1.0204e-03, 6.1684e-03, 7.4615e-03,\n",
      "        1.0010e-02, 5.1384e-03, 5.0812e-03, 6.0272e-03, 2.5806e-03, 7.7248e-03,\n",
      "        5.1003e-03, 1.0056e-02, 4.0398e-03, 8.2855e-03, 7.2899e-03, 5.1003e-03,\n",
      "        1.1917e-02, 3.4828e-03, 2.2793e-03, 3.0994e-03, 1.0948e-02, 1.0696e-02,\n",
      "        5.9814e-03, 5.1384e-03, 7.6675e-03, 3.0518e-03, 4.0245e-03, 8.9188e-03,\n",
      "        1.5736e-03, 1.2573e-02, 1.4381e-03, 5.6190e-03, 2.1774e-02, 1.4839e-02,\n",
      "        4.1779e-02, 2.1324e-03, 2.4887e-02, 3.3539e-02, 1.3374e-02, 4.5738e-03,\n",
      "        6.6414e-03, 1.1551e-02, 8.5449e-03, 2.6306e-02, 1.5366e-02, 1.2054e-02,\n",
      "        1.2825e-02, 4.6539e-02, 3.0273e-02, 3.2654e-02, 5.7983e-03, 3.0670e-02,\n",
      "        7.4524e-02, 3.5950e-02, 3.3539e-02, 5.4199e-02, 3.5492e-02, 3.1471e-03,\n",
      "        2.4147e-03, 1.3847e-02, 2.9877e-02, 9.0576e-02, 4.0253e-02, 1.0968e-01,\n",
      "        1.3416e-01, 6.1768e-02, 5.7190e-02, 9.0576e-02, 3.1738e-02, 4.5685e-02,\n",
      "        1.1066e-01, 1.0735e-02, 5.4901e-02, 2.0523e-02, 3.5675e-02, 8.8013e-02,\n",
      "        6.3965e-02, 2.2247e-02, 7.5989e-02, 4.0680e-02, 1.2573e-02, 5.8777e-02,\n",
      "        1.1375e-02, 2.6459e-02, 7.2899e-03, 1.2192e-02, 6.2927e-02, 3.4363e-02,\n",
      "        1.2672e-02, 6.1310e-02, 1.5602e-02, 1.5736e-03, 2.9373e-02, 2.0340e-02,\n",
      "        5.8777e-02, 2.5177e-02, 5.0812e-02, 1.0931e-01, 6.7444e-02, 3.0273e-02,\n",
      "        7.5195e-02, 4.3854e-02, 7.0953e-03, 1.5427e-02, 1.7105e-02, 8.3771e-03,\n",
      "        4.2877e-02, 1.4839e-02, 1.3222e-02, 3.9795e-02, 3.7384e-02, 1.4008e-02,\n",
      "        6.0638e-02, 9.1934e-03, 1.4668e-03, 3.6640e-03, 2.2415e-02, 7.8064e-02,\n",
      "        3.6560e-02, 7.1350e-02, 1.1554e-01, 6.0211e-02, 2.8870e-02, 8.6487e-02,\n",
      "        1.0986e-02], dtype=torch.float16), 'confidence': tensor(0.1342, dtype=torch.float16), 'last_hidden_state': tensor([ 0.8896, -0.8564,  0.8477,  0.9517,  0.9292,  0.8628, -0.9575,  0.9194,\n",
      "        -0.9282,  0.8013, -0.9453, -0.9155, -0.8999,  0.9370, -0.8838, -0.9736,\n",
      "         0.9263, -0.9287,  0.9390, -0.9521, -0.0602,  0.9038,  0.8989, -0.9209,\n",
      "         0.8765, -0.9536,  0.8135, -0.2593, -0.8340,  0.8989, -0.9658, -0.7920,\n",
      "         0.8174,  0.9165,  0.7671, -0.9360,  0.9380, -0.9053, -0.9268,  0.8306,\n",
      "         0.9409,  0.0867,  0.8970, -0.8887, -0.9297,  0.8882, -0.9375, -0.7856,\n",
      "         0.9414, -0.7705, -0.8228, -0.9175, -0.8652, -0.9546, -0.7905,  0.9233,\n",
      "         0.9487,  0.8232, -0.9180, -0.9028, -0.6060,  0.8076,  0.8794,  0.9502,\n",
      "         0.7280,  0.9316,  0.9131,  0.9600,  0.9263, -0.8701,  0.9072, -0.8950,\n",
      "         0.9551, -0.9541, -0.8135,  0.7798, -0.8037, -0.6997,  0.7393,  0.8408,\n",
      "        -0.9395,  0.9111, -0.7676, -0.9517,  0.9409, -0.5625, -0.8188,  0.9116,\n",
      "         0.2180,  0.9180, -0.6621,  0.8901,  0.9414, -0.9390, -0.9346, -0.8525,\n",
      "        -0.9121, -0.7046, -0.7529,  0.9180, -0.8643, -0.9224,  0.9204, -0.8760,\n",
      "         0.7461,  0.7720, -0.8511,  0.9463,  0.9307,  0.9438,  0.9087,  0.9214,\n",
      "        -0.9561,  0.9565,  0.9243,  0.9497,  0.7563, -0.8911, -0.8989,  0.7178,\n",
      "        -0.7197, -0.9199, -0.9170,  0.9336,  0.9185,  0.8394, -0.8994,  0.8638,\n",
      "         0.9663, -0.9683, -0.4424,  0.6328, -0.7876, -0.9355,  0.9663, -0.7822,\n",
      "        -0.0369,  0.9277,  0.9731, -0.9160, -0.9009,  0.9072, -0.8301, -0.9282,\n",
      "        -0.9692, -0.8062,  0.9204, -0.9175, -0.9414,  0.8975,  0.6514,  0.8516,\n",
      "        -0.8989, -0.8604,  0.6089,  0.3228, -0.8936,  0.9458,  0.9556, -0.9604,\n",
      "         0.8867,  0.9194,  0.9131,  0.9268, -0.7842, -0.9194,  0.7427,  0.8081,\n",
      "         0.9473,  0.6094,  0.8662,  0.5073, -0.8247,  0.9731, -0.9429, -0.9351,\n",
      "         0.9507,  0.8755, -0.8677, -0.9082,  0.7954,  0.9224, -0.4390,  0.9321,\n",
      "        -0.5747, -0.8438,  0.9023,  0.9131, -0.9736,  0.4109,  0.9023,  0.9463,\n",
      "         0.9531, -0.7773,  0.9121, -0.8765, -0.2424,  0.9478, -0.9409,  0.9424,\n",
      "         0.7734, -0.5762, -0.7832, -0.8843,  0.8301,  0.8486,  0.9136,  0.8970,\n",
      "        -0.7783,  0.5630,  0.7202,  0.6914,  0.9575, -0.9297,  0.5640,  0.9404,\n",
      "         0.6665, -0.9302, -0.9189,  0.9395, -0.8882, -0.8467, -0.8672,  0.9268,\n",
      "         0.8228, -0.8555,  0.9404,  0.8662, -0.4753, -0.2642, -0.9004, -0.9219,\n",
      "        -0.9331, -0.8291,  0.4800,  0.8916, -0.9214, -0.2054,  0.6895, -0.8784,\n",
      "         0.9185,  0.7275, -0.9395,  0.9619,  0.9414, -0.9253, -0.7583,  0.9224,\n",
      "        -0.8301,  0.8760, -0.7769,  0.9253,  0.9189, -0.9580,  0.9062, -0.7178,\n",
      "        -0.8857,  0.4292,  0.8018, -0.9023, -0.1239, -0.9185,  0.9023,  0.8521,\n",
      "        -0.9199, -0.9351, -0.9536,  0.2876,  0.9644, -0.9136, -0.8633, -0.9072,\n",
      "         0.9272, -0.9463, -0.8735, -0.9512, -0.9482,  0.8291, -0.9458,  0.8882,\n",
      "        -0.9258, -0.9341,  0.9199,  0.7246,  0.2018,  0.9126, -0.9614, -0.1921,\n",
      "        -0.7334,  0.8467,  0.7017,  0.7803, -0.9131, -0.9585,  0.9448,  0.9487,\n",
      "         0.8789, -0.8643, -0.7456, -0.0967,  0.8867,  0.9209, -0.8701, -0.9224,\n",
      "         0.8911,  0.9258, -0.9224, -0.8818, -0.9287, -0.8125, -0.9326, -0.0361,\n",
      "         0.8882,  0.9341,  0.8389, -0.9067,  0.8579,  0.8257,  0.8936,  0.3433,\n",
      "         0.9531,  0.8271, -0.8438,  0.8325, -0.8833,  0.9126, -0.8721,  0.8779,\n",
      "         0.8848,  0.8525, -0.8779, -0.8394, -0.9019,  0.9248, -0.0732, -0.7734,\n",
      "         0.8540, -0.9351,  0.8115, -0.9478,  0.5962, -0.9590, -0.8950,  0.9209,\n",
      "         0.8667, -0.5190, -0.4788,  0.9321, -0.9385,  0.0076, -0.8643,  0.4062,\n",
      "         0.9736, -0.5781,  0.9180,  0.9512,  0.4871,  0.9751, -0.8550,  0.9028,\n",
      "         0.8564, -0.9341, -0.4099,  0.9111, -0.9209,  0.9521, -0.9526, -0.9287,\n",
      "         0.2542,  0.7266, -0.7061,  0.9429, -0.9526,  0.9390, -0.8560,  0.9272,\n",
      "        -0.8794, -0.9551, -0.6875, -0.7412,  0.8936,  0.9062, -0.9399,  0.7783,\n",
      "        -0.9312, -0.8555, -0.9658,  0.9272,  0.9023, -0.8940,  0.9009, -0.9424,\n",
      "         0.9263,  0.9624,  0.9097, -0.6875, -0.8486,  0.8413,  0.8794,  0.7852,\n",
      "         0.9648,  0.9492,  0.9146,  0.9141,  0.7222,  0.9048,  0.9644, -0.7808,\n",
      "         0.8979,  0.7891,  0.8804,  0.9453,  0.8623, -0.8877,  0.9507, -0.9336,\n",
      "        -0.7173,  0.7178,  0.7480,  0.7852, -0.9048,  0.9106, -0.9702,  0.9492,\n",
      "         0.9316, -0.9243,  0.8867, -0.9155,  0.9351, -0.7588,  0.9150, -0.7075,\n",
      "         0.7339,  0.8794, -0.9609,  0.9229,  0.9595,  0.9385, -0.9482,  0.9360,\n",
      "        -0.8955,  0.9385,  0.9673,  0.9307,  0.7314,  0.9141,  0.9297,  0.5161,\n",
      "        -0.9839,  0.9058,  0.2281,  0.8613, -0.9243,  0.9341, -0.8608, -0.9570,\n",
      "         0.9316,  0.9033, -0.8516, -0.7666, -0.9370, -0.8818, -0.9028, -0.8770,\n",
      "         0.9458,  0.9546, -0.8154,  0.9146, -0.9097, -0.9736, -0.8037,  0.8823,\n",
      "        -0.9204, -0.9375, -0.9092,  0.9263,  0.5635,  0.9106, -0.9287,  0.9268,\n",
      "         0.9307, -0.9189,  0.8877, -0.9243,  0.2095,  0.9023, -0.9683, -0.9131,\n",
      "        -0.8345,  0.9165, -0.9253, -0.9209, -0.2361,  0.8389,  0.9497, -0.4050,\n",
      "         0.8657,  0.9346, -0.8633, -0.8315,  0.9438, -0.9424, -0.7363, -0.7539,\n",
      "         0.8999,  0.7695,  0.9619, -0.8706,  0.9214,  0.8203, -0.9595, -0.9170],\n",
      "       dtype=torch.float16)}\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model using ModelHub - matches complete tutorial\n",
    "from omnigenbench import ModelHub\n",
    "\n",
    "# Load the trained model\n",
    "inference_model = ModelHub.load(\"yangheng/ogb_tfb_finetuned\")\n",
    "\n",
    "# Define sample sequences for testing - matches complete tutorial style\n",
    "sample_sequences = {\n",
    "    \"Random sequence\": \"AGCT\" * (128 // 4),\n",
    "    \"AT-rich sequence\": \"AATT\" * (128 // 4),\n",
    "    \"GC-rich sequence\": \"GCGC\" * (128 // 4),\n",
    "}\n",
    "\n",
    "print(\"🧬 Testing model on sample DNA sequences:\")\n",
    "print(\"=\" * 50)\n",
    "print(inference_model.inference(sample_sequences[\"Random sequence\"]))\n",
    "print(\"=\" * 50)\n",
    "print(inference_model.inference(sample_sequences[\"AT-rich sequence\"]))\n",
    "print(\"=\" * 50)\n",
    "print(inference_model.inference(sample_sequences[\"GC-rich sequence\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14233e",
   "metadata": {},
   "source": [
    "This approach is incredibly convenient for standard models and tasks. You simply point `ModelHub` to your checkpoint directory, and it takes care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d6678",
   "metadata": {},
   "source": [
    "### 3. Manual Inference: For Custom Workflows\n",
    "\n",
    "While `ModelHub` is convenient, sometimes you need more control. For example, you might have a custom data loading pipeline or want to perform inference on a large dataset more efficiently. In these cases, you can perform the inference steps manually.\n",
    "\n",
    "This involves loading the model and tokenizer yourself and then explicitly tokenizing the input before passing it to the model. This is essentially what `ModelHub` does under the hood.\n",
    "\n",
    "#### 3.1. Setup: Re-initializing Model, Tokenizer, and Data\n",
    "\n",
    "First, let's set up our environment again. We need to load the necessary libraries and define our configuration. We also need our test dataloader to evaluate the model on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for manual inference\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForMultiLabelSequenceClassification,\n",
    "    OmniDatasetForMultiLabelClassification\n",
    ")\n",
    "\n",
    "# Configuration for inference - matches complete tutorial\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"deepsea_tfb_prediction\"\n",
    "num_labels = 919\n",
    "max_length = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"✅ Libraries imported and configuration set!\")\n",
    "print(f\"🎯 Device: {device}\")\n",
    "print(f\"🧬 Model: {config_or_model}\")\n",
    "print(f\"📊 Labels: {num_labels} TF binding sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d25b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and test dataset for evaluation\n",
    "print(\"🔄 Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config_or_model)\n",
    "print(f\"✅ Tokenizer loaded\")\n",
    "\n",
    "print(\"📊 Loading test dataset...\")\n",
    "datasets = OmniDatasetForMultiLabelClassification.from_hub(\n",
    "    dataset_name_or_path=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    max_examples=100,  # Small subset for demonstration\n",
    "    force_padding=False\n",
    ")\n",
    "\n",
    "print(f\"🧪 Test dataset: {len(datasets['test'])} samples\")\n",
    "\n",
    "# For manual inference, we can load the model architecture and weights\n",
    "# In practice, you would load your trained checkpoint here\n",
    "print(\"🔄 Loading model for manual inference...\")\n",
    "model = OmniModelForMultiLabelSequenceClassification(\n",
    "    config_or_model,\n",
    "    tokenizer,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded and ready for manual inference!\")\n",
    "print(f\"📊 Model has {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada5ea1",
   "metadata": {},
   "source": [
    "#### 3.2. Loading the Fine-Tuned Model\n",
    "\n",
    "Now, we load the model. We first initialize the model architecture using `OmniModelForMultiLabelSequenceClassification` and then load our fine-tuned weights from the `best_model.pth` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration with ModelHub - matches complete tutorial approach\n",
    "print(\"🔮 Quick Inference with ModelHub:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use ModelHub for easy inference\n",
    "model_hub_inference = ModelHub.load(\"yangheng/ogb_tfb_finetuned\")\n",
    "\n",
    "# Test with a sample sequence\n",
    "test_sequence = \"GATTACAGATTACAGATTACA\" * 10  # Create a longer test sequence\n",
    "\n",
    "print(f\"🧬 Test sequence length: {len(test_sequence)} bp\")\n",
    "print(\"🔄 Running inference...\")\n",
    "\n",
    "# Make prediction\n",
    "result = model_hub_inference.inference(test_sequence)\n",
    "print(\"✅ Inference completed!\")\n",
    "\n",
    "if hasattr(result, 'shape'):\n",
    "    print(f\"📊 Output shape: {result.shape}\")\n",
    "    print(f\"🎯 Predicted {result.shape[-1]} TF binding probabilities\")\n",
    "else:\n",
    "    print(f\"📊 Result type: {type(result)}\")\n",
    "\n",
    "print(\"🚀 ModelHub provides easy one-line inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efda4a",
   "metadata": {},
   "source": [
    "#### 3.3. The Inference Loop\n",
    "\n",
    "With the model and dataloader ready, we can now loop through the test set, make predictions, and store the results. This is the core of manual inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual inference on test dataset\n",
    "print(\"🔍 Manual Inference on Test Dataset:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Get a few test samples for demonstration\n",
    "test_samples = [datasets['test'][i] for i in range(min(3, len(datasets['test'])))]\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"🔄 Processing test samples...\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        # Prepare input\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        true_labels = sample['labels']\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        \n",
    "        # Apply threshold for binary predictions\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        \n",
    "        print(f\"\\n📋 Sample {i+1}:\")\n",
    "        print(f\"   🧬 Sequence length: {input_ids.shape[1]} tokens\")\n",
    "        print(f\"   🎯 Predicted binding sites: {predictions.sum().item():.0f}/{len(predictions)}\")\n",
    "        print(f\"   🏷️ True binding sites: {true_labels.sum().item():.0f}/{len(true_labels)}\")\n",
    "        print(f\"   📈 Max probability: {probabilities.max().item():.3f}\")\n",
    "        print(f\"   📉 Min probability: {probabilities.min().item():.3f}\")\n",
    "        \n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_labels.append(true_labels.numpy())\n",
    "\n",
    "print(f\"\\n✅ Manual inference completed on {len(test_samples)} samples!\")\n",
    "print(\"🎯 This demonstrates the full inference pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69606ea",
   "metadata": {},
   "source": [
    "This final score gives you a reliable estimate of how well your model will perform in a real-world scenario on new genomic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e94569",
   "metadata": {},
   "source": [
    "### 4. Deployment Concepts: Serving Your Model\n",
    "\n",
    "A trained model in a notebook is great for research, but for real-world applications, you'll want to **deploy** it as a service. This typically means wrapping it in a web API. A popular choice for this is **FastAPI**.\n",
    "\n",
    "The concept is simple:\n",
    "1.  **Create a FastAPI App**: A simple Python script that defines API endpoints.\n",
    "2.  **Load the Model**: In the app's startup logic, you would load your fine-tuned model and tokenizer once (e.g., using the `ModelHub` or the manual method).\n",
    "3.  **Define a Prediction Endpoint**: Create an endpoint (e.g., `/predict`) that accepts a DNA sequence as input.\n",
    "4.  **Process and Predict**: Inside the endpoint function, you would call the model's inference method with the input sequence and return the prediction as a JSON response.\n",
    "\n",
    "While a full deployment tutorial is beyond our current scope, the `ModelHub` API is designed to make this transition as smooth as possible.\n",
    "\n",
    "### Summary and Conclusion\n",
    "\n",
    "This tutorial marks the end of our journey from a biological question to a fully trained and evaluated model. We have covered the complete lifecycle: data preparation, model initialization, training, and now, inference.\n",
    "\n",
    "You have learned how to:\n",
    "-   Understand the end-to-end inference pipeline.\n",
    "-   Use the high-level `ModelHub` for quick and easy predictions.\n",
    "-   Perform manual inference for greater control and batch processing.\n",
    "-   Evaluate the final model on a test set to get a definitive performance metric.\n",
    "-   Conceptualize how to deploy your model as a service.\n",
    "\n",
    "You are now equipped with the fundamental skills to tackle your own genomic prediction tasks using `OmniGenBench`. You can return to the main tutorial or explore the other examples in the repository to learn about different tasks and models. Happy modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b65068",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
