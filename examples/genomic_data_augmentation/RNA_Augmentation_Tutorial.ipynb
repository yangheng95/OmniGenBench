{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "702828e2486d156f"
  },
  {
   "cell_type": "markdown",
   "id": "eba1def0",
   "metadata": {},
   "source": [
    "### 1. **Setting Up the Environment**\n",
    "Before starting, you need to install the required Python packages:\n",
    "```bash\n",
    "pip install torch transformers autocuda tqdm\n",
    "```\n",
    "You will also need a pre-trained masked language model (MLM) that is compatible with your sequence data. The model should be hosted on Hugging Face or available locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f25c5",
   "metadata": {},
   "source": [
    "### 2. **Understanding the Parameters**\n",
    "When initializing the `OmniModelForAugmentation` class, you can configure several key parameters:\n",
    "- `model_name_or_path`: The Hugging Face model name or the local path to the pre-trained model.\n",
    "- `noise_ratio`: The proportion of tokens to mask in each sequence for augmentation (default is 0.15).\n",
    "- `max_length`: The maximum token length for input sequences (default is 1026).\n",
    "- `instance_num`: The number of augmented instances to generate for each input sequence (default is 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e1c24",
   "metadata": {},
   "source": [
    "### 3. **Example Usage**\n",
    "Letâ€™s walk through an example of how to use the `OmniModelForAugmentation` class.\n",
    "\n",
    "First, initialize the model by providing the model path and other augmentation parameters such as noise ratio, maximum sequence length, and instance number."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from omnigenbench import OmniModelForAugmentation\n",
    "\n",
    "# Initialize the augmentation model\n",
    "model = OmniModelForAugmentation(\n",
    "    model_name_or_path=\"anonymous8/OmniGenome-186M\",  # Pre-trained model\n",
    "    noise_ratio=0.2,  # 20% of the tokens will be masked\n",
    "    max_length=1026,  # Maximum token length\n",
    "    instance_num=3  # Generate 3 augmented instances per sequence\n",
    ")\n"
   ],
   "id": "7c69e556085cd36e"
  },
  {
   "cell_type": "markdown",
   "id": "906988c0",
   "metadata": {},
   "source": [
    "### Step 1: **Augment a Single Sequence**\n",
    "You can augment a single sequence directly by calling the `augment_sequence` method. This method will apply noise, predict masked tokens, and return the augmented sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single sequence augmentation\n",
    "augmented_sequence = model.augment_sequence(\"ATCTTGCATTGAAG\")\n",
    "print(f\"Augmented sequence: {augmented_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c290d9",
   "metadata": {},
   "source": [
    "### Step 2: **Augment Sequences from a File**\n",
    "To augment multiple sequences from a JSON file, you can use the `augment_from_file` method. This method reads the sequences from the file, applies augmentation, and saves the augmented sequences to another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for input and output\n",
    "input_file = \"toy_datasets/test.json\"\n",
    "output_file = \"toy_datasets/augmented_sequences.json\"\n",
    "\n",
    "# Augment sequences from the input file and save to the output file\n",
    "model.augment_from_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0704c93",
   "metadata": {},
   "source": [
    "The input file should be in JSON format, where each line contains a sequence, like this:\n",
    "\n",
    "```json\n",
    "{\"seq\": \"ATCTTGCATTGAAG\"}\n",
    "{\"seq\": \"GGTTTACAGTCCAA\"}\n",
    "```\n",
    "\n",
    "The output will be saved in the same format, with each augmented sequence written in a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d30d2f",
   "metadata": {},
   "source": [
    "### Step 3: **Configurable Parameters**\n",
    "The augmentation process allows you to configure various parameters, such as:\n",
    "- **`noise_ratio`**: Specifies the percentage of tokens that will be masked in the input sequence. The default value is `0.15` (i.e., 15% of tokens will be masked).\n",
    "- **`max_length`**: The maximum token length for the input sequences. The default is `1026`.\n",
    "- **`instance_num`**: The number of augmented instances to generate for each input sequence. The default is `1`, but you can increase this value to create multiple augmented versions of each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16a8f4",
   "metadata": {},
   "source": [
    "### Step 4: **Save Augmented Sequences**\n",
    "The `save_augmented_sequences` method saves the generated augmented sequences to a JSON file. Each line will contain one augmented sequence in the format `{\"aug_seq\": \"<augmented_sequence>\"}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e69a26",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The `OmniModelForAugmentation` class provides a simple and flexible interface for augmenting sequences using a masked language model. By adjusting the noise ratio, instance count, and other hyperparameters, you can create diverse augmented datasets to improve the performance of machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
