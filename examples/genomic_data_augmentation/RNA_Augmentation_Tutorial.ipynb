{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6a2991",
   "metadata": {},
   "source": [
    "# üß¨ Genomic Data Augmentation with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to perform **intelligent genomic data augmentation** using **OmniGenBench**. This guide will walk you through the process of generating high-quality synthetic genomic sequences that preserve biological patterns and improve model training performance.\n",
    "\n",
    "### 1. The Machine Learning Challenge: Why Genomic Data Augmentation?\n",
    "\n",
    "**Genomic data augmentation** is a critical technique in computational biology that addresses several fundamental challenges in genomic machine learning:\n",
    "\n",
    "- **Limited Training Data**: High-quality labeled genomic datasets are often small and expensive to generate\n",
    "- **Class Imbalance**: Rare genomic variants and functions are underrepresented in datasets\n",
    "- **Overfitting Prevention**: Augmentation increases dataset diversity and improves generalization\n",
    "- **Domain Adaptation**: Bridging gaps between different experimental conditions or species\n",
    "\n",
    "The power of genomic augmentation lies in its ability to:\n",
    "- **Generate Realistic Sequences**: Create biologically plausible variants while preserving functional patterns\n",
    "- **Expand Dataset Size**: Multiply available training data without additional experimental costs\n",
    "- **Improve Model Robustness**: Enhance model performance on unseen genomic variations\n",
    "- **Balance Datasets**: Address class imbalance issues in genomic classification tasks\n",
    "\n",
    "Applications across computational biology:\n",
    "- **Rare Variant Analysis**: Augment underrepresented mutation patterns for disease prediction\n",
    "- **Cross-Species Learning**: Generate bridge sequences for evolutionary studies\n",
    "- **Functional Annotation**: Create training data for poorly characterized genomic regions\n",
    "- **Model Validation**: Generate test sequences for robustness evaluation\n",
    "\n",
    "### 2. The Challenge: Biologically-Informed Sequence Generation\n",
    "\n",
    "Unlike random mutations, intelligent genomic augmentation must preserve:\n",
    "\n",
    "- **Functional Motifs**: Critical regulatory and coding sequences\n",
    "- **Structural Constraints**: Secondary structures and folding patterns\n",
    "- **Evolutionary Patterns**: Codon usage bias and phylogenetic relationships  \n",
    "- **Statistical Properties**: Nucleotide composition and k-mer frequencies\n",
    "\n",
    "**Augmentation Process:**\n",
    "\n",
    "| Original Sequence | Random Mutation | Intelligent Augmentation |\n",
    "|------------------|-----------------|-------------------------|\n",
    "| `ATGCGATCG` | `ATGCTATCG` (random) | `ATGCGATCC` (codon-aware) |\n",
    "| Functional | May break function | Preserves function |\n",
    "\n",
    "### 3. The Tool: Masked Language Models for Genomic Augmentation\n",
    "\n",
    "#### Foundation Model Understanding\n",
    "**OmniGenome** uses masked language modeling (MLM) for intelligent sequence augmentation. This approach:\n",
    "\n",
    "1. **Masks Strategic Positions**: Selectively mask nucleotides while preserving critical patterns\n",
    "2. **Predicts Biologically Plausible Alternatives**: Use pre-trained understanding to suggest realistic substitutions\n",
    "3. **Maintains Sequence Integrity**: Ensure augmented sequences remain biologically valid\n",
    "4. **Preserves Functional Patterns**: Keep important motifs and regulatory elements intact\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Genomic Augmentation\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for Genomic Data Augmentation\"\n",
    "        A[\"üì• Step 1: Setup and Configuration<br/>Configure augmentation parameters and models\"] --> B[\"üîß Step 2: Model Initialization<br/>Load pre-trained genomic foundation models\"]\n",
    "        B --> C[\"üéì Step 3: Sequence Augmentation<br/>Generate diverse, biologically-valid variants\"]\n",
    "        C --> D[\"üîÆ Step 4: Quality Assessment<br/>Validate and analyze augmented sequences\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start generating high-quality genomic training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1def0",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup and Configuration\n",
    "\n",
    "This first step focuses on setting up our genomic data augmentation environment and understanding the key parameters that control sequence generation quality.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required packages for intelligent genomic data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: omnigenbench requires Python 3.8+ and PyTorch 1.12+\n",
    "!pip install omnigenbench torch transformers tqdm scikit-learn matplotlib seaborn -U\n",
    "\n",
    "# Verify installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "try:\n",
    "    import omnigenbench\n",
    "    print(f\"‚úÖ OmniGenBench version: {omnigenbench.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import omnigenbench: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f25c5",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Next, we import the essential libraries for genomic data augmentation, including specialized tools for sequence analysis and quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23213f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import jaccard_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from omnigenbench import (\n",
    "    OmniModelForAugmentation,\n",
    "    ModelHub,\n",
    ")\n",
    "\n",
    "# Environment validation\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Validation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {__import__('sys').version.split()[0]}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Note: Running on CPU. Augmentation will be slower but functional.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Verify data directory exists\n",
    "data_dir = Path(\"toy_datasets\")\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory '{data_dir}' not found. \"\n",
    "        f\"Please ensure you're running this notebook from the correct directory.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e1c24",
   "metadata": {},
   "source": [
    "### 1.3: Understanding Augmentation Parameters\n",
    "\n",
    "Before we start augmentation, let's understand the key parameters that control the quality and diversity of generated sequences:\n",
    "\n",
    "#### Critical Parameters\n",
    "- **noise_ratio**: Proportion of tokens to mask and predict (0.15-0.25 typically optimal)\n",
    "  - Higher values = more variation but risk losing biological patterns\n",
    "  - Lower values = safer but less diversity\n",
    "- **instance_num**: Number of variants per original sequence\n",
    "  - Each variant uses a different random masking pattern\n",
    "- **max_length**: Maximum sequence length for processing\n",
    "  - Longer sequences require more GPU memory\n",
    "- **model_selection**: Choice of pre-trained genomic foundation model\n",
    "  - Larger models (186M) produce higher quality but slower\n",
    "  - Smaller models (52M) are faster but may be less accurate\n",
    "\n",
    "**Design Philosophy:**\n",
    "- **Reproducibility**: Set random seeds for deterministic results\n",
    "- **Biological Validity**: Augmentation preserves sequence properties\n",
    "- **Efficiency**: Batch processing for speed\n",
    "- **Transparency**: All parameters are explicit and documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69e556085cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Source of Truth (SSoT) for all augmentation configuration\n",
    "# Modify these values to customize augmentation behavior\n",
    "\n",
    "AUGMENTATION_CONFIG = {\n",
    "    # Model configuration\n",
    "    \"model_name\": \"yangheng/OmniGenome-52M\",\n",
    "    \n",
    "    # Augmentation hyperparameters\n",
    "    \"noise_ratio\": 0.20,         # Mask 20% of tokens per sequence\n",
    "    \"max_length\": 512,           # Maximum sequence length (adjust based on your data)\n",
    "    \"instance_num\": 3,           # Generate 3 variants per original sequence\n",
    "    \"batch_size\": 8,             # Batch size for MLM forward pass (reduce if OOM)\n",
    "    \n",
    "    # Reproducibility\n",
    "    \"seed\": 42,                  # Random seed for reproducible results\n",
    "    \n",
    "    # File paths (relative to notebook directory)\n",
    "    \"data_dir\": \"toy_datasets\",\n",
    "    \"train_file\": \"train.json\",\n",
    "    \"test_file\": \"test.json\",\n",
    "    \"output_file\": \"augmented_sequences.json\",\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(AUGMENTATION_CONFIG[\"seed\"])\n",
    "np.random.seed(AUGMENTATION_CONFIG[\"seed\"])\n",
    "\n",
    "# Display configuration\n",
    "print(\"üéØ Genomic Data Augmentation Configuration (SSoT):\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExpected output per sequence: {AUGMENTATION_CONFIG['instance_num']} variants\")\n",
    "print(f\"Approximate masking per variant: ~{int(AUGMENTATION_CONFIG['noise_ratio'] * 100)} nucleotides per 100nt\")\n",
    "print(f\"Model parameter count: ~52 million (efficient for CPU/GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906988c0",
   "metadata": {},
   "source": [
    "## üöÄ Step 2: Model Initialization\n",
    "\n",
    "Now let's initialize the genomic augmentation model. The `OmniModelForAugmentation` leverages pre-trained genomic foundation models to generate biologically-informed sequence variants.\n",
    "\n",
    "### Augmentation Model Features\n",
    "- **Intelligent Masking**: Strategic selection of positions for variation\n",
    "- **Contextual Prediction**: Uses surrounding sequence context for realistic substitutions  \n",
    "- **Batch Processing**: Efficient handling of multiple sequences\n",
    "- **Quality Control**: Built-in validation of augmented sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the genomic augmentation model\n",
    "print(\"üîß Initializing Genomic Data Augmentation Model...\")\n",
    "print(f\"Loading: {AUGMENTATION_CONFIG['model_name']}\")\n",
    "\n",
    "try:\n",
    "    augmentation_model = OmniModelForAugmentation(\n",
    "        config_or_model=AUGMENTATION_CONFIG[\"model_name\"],\n",
    "        noise_ratio=AUGMENTATION_CONFIG[\"noise_ratio\"],\n",
    "        max_length=AUGMENTATION_CONFIG[\"max_length\"],\n",
    "        instance_num=AUGMENTATION_CONFIG[\"instance_num\"],\n",
    "        batch_size=AUGMENTATION_CONFIG[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Augmentation model initialized successfully!\")\n",
    "    print(\"\\nüéØ Model Capabilities:\")\n",
    "    print(\"  [x] Intelligent sequence masking based on genomic patterns\")\n",
    "    print(\"  [x] Context-aware nucleotide prediction via MLM\")\n",
    "    print(\"  [x] Batch processing for computational efficiency\")\n",
    "    print(\"  [x] Preservation of biological sequence properties\")\n",
    "    print(f\"  [x] Configured for {AUGMENTATION_CONFIG['instance_num']} variants per sequence\")\n",
    "    \n",
    "    # Verify model is on correct device\n",
    "    device = next(augmentation_model.model.parameters()).device\n",
    "    print(f\"\\n‚úì Model device: {device}\")\n",
    "    print(f\"‚úì AMP enabled: {augmentation_model.use_amp}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model initialization failed: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"  1. Check internet connection (model downloads from HuggingFace Hub)\")\n",
    "    print(\"  2. Verify transformers version: pip install transformers>=4.25.0\")\n",
    "    print(\"  3. If behind proxy, set HF_ENDPOINT environment variable\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c290d9",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Sequence Augmentation\n",
    "\n",
    "Now comes the exciting part! We'll demonstrate different approaches to genomic data augmentation, from single sequences to batch processing of entire datasets.\n",
    "\n",
    "### Our Augmentation Strategy\n",
    "\n",
    "We'll explore multiple augmentation scenarios:\n",
    "\n",
    "1. **Single Sequence Augmentation**: Generate variants for individual sequences\n",
    "2. **Batch Augmentation**: Process multiple sequences efficiently\n",
    "3. **File-based Augmentation**: Handle large datasets from files\n",
    "4. **Quality-controlled Augmentation**: Ensure biological validity of outputs\n",
    "\n",
    "Let's start with single sequence augmentation to understand the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate single sequence augmentation with proper API usage\n",
    "test_sequences = {\n",
    "    \"Coding sequence\": \"ATGAAAGCCATTGAGAAGGCAAAACCCCGATGGTCCTTCGCGAA\",\n",
    "    \"UTR region\": \"AUUGAGAUGUUUGCCAUUUUGACCAUCUGACCUUUGCCAUC\",\n",
    "    \"Regulatory motif\": \"TATAAGCCGCGGTGACCTGCAG\",\n",
    "    \"Random sequence\": \"ATCGATCGATCGATCGATCG\"\n",
    "}\n",
    "\n",
    "print(\"üéì Demonstrating Single Sequence Augmentation\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Using augment() method - generates k variants per sequence\")\n",
    "print(\"Each variant uses different random masking patterns\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for seq_name, sequence in test_sequences.items():\n",
    "    print(f\"\\nüìä Augmenting: {seq_name}\")\n",
    "    print(f\"  Original ({len(sequence)}nt): {sequence}\")\n",
    "    \n",
    "    try:\n",
    "        # CORRECT API USAGE: augment(seq, k) automatically handles masking and prediction\n",
    "        # Generates k variants (default k=1, or uses instance_num from config)\n",
    "        augmented_sequences = augmentation_model.augment(seq=sequence, k=1)\n",
    "        \n",
    "        # augment() returns a list, even for k=1\n",
    "        if augmented_sequences and len(augmented_sequences) > 0:\n",
    "            augmented_seq = augmented_sequences[0]\n",
    "            print(f\"  Augmented:     {augmented_seq}\")\n",
    "            \n",
    "            # Analyze differences\n",
    "            min_len = min(len(sequence), len(augmented_seq))\n",
    "            differences = sum(1 for a, b in zip(sequence[:min_len], augmented_seq[:min_len]) if a != b)\n",
    "            \n",
    "            # Handle length differences (rare but possible)\n",
    "            len_diff = abs(len(sequence) - len(augmented_seq))\n",
    "            total_diff = differences + len_diff\n",
    "            \n",
    "            similarity = 1 - (total_diff / max(len(sequence), len(augmented_seq)))\n",
    "            \n",
    "            print(f\"\\n  üìà Analysis:\")\n",
    "            print(f\"    Changed positions: {differences}/{min_len}\")\n",
    "            if len_diff > 0:\n",
    "                print(f\"    Length difference: {len_diff} (‚ö†Ô∏è unusual)\")\n",
    "            print(f\"    Sequence similarity: {similarity:.1%}\")\n",
    "            print(f\"    Effective mutation rate: {differences/min_len:.1%}\")\n",
    "            \n",
    "            # GC content analysis\n",
    "            def gc_content(seq):\n",
    "                seq_upper = seq.upper()\n",
    "                gc_count = seq_upper.count('G') + seq_upper.count('C')\n",
    "                return gc_count / len(seq) if len(seq) > 0 else 0\n",
    "            \n",
    "            orig_gc = gc_content(sequence)\n",
    "            aug_gc = gc_content(augmented_seq)\n",
    "            gc_diff = aug_gc - orig_gc\n",
    "            \n",
    "            print(f\"    GC content: {orig_gc:.1%} ‚Üí {aug_gc:.1%} (Œî{gc_diff:+.1%})\")\n",
    "            \n",
    "            # Biological validity check\n",
    "            valid_nucs = set('ATCGUatcgu')\n",
    "            invalid_orig = sum(1 for c in sequence if c not in valid_nucs)\n",
    "            invalid_aug = sum(1 for c in augmented_seq if c not in valid_nucs)\n",
    "            \n",
    "            if invalid_orig > 0 or invalid_aug > 0:\n",
    "                print(f\"    ‚ö†Ô∏è  Invalid nucleotides: Orig={invalid_orig}, Aug={invalid_aug}\")\n",
    "            else:\n",
    "                print(f\"    ‚úì All nucleotides valid (ATCGU only)\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Warning: No augmented sequence returned\")\n",
    "            print(\"     This may indicate an issue with the model or input sequence\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Augmentation failed: {str(e)}\")\n",
    "        print(f\"     Error type: {type(e).__name__}\")\n",
    "        # Uncomment for debugging:\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "    \n",
    "    print(\"‚îÄ\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Single sequence augmentation demonstration complete!\")\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"  1. Use augment(seq, k) method for proper augmentation workflow\")\n",
    "print(\"  2. Method automatically handles masking and MLM prediction\")\n",
    "print(\"  3. Returns list of k augmented sequences (even for k=1)\")\n",
    "print(\"  4. GC content is generally preserved (¬±5% typical)\")\n",
    "print(\"  5. Mutation rate depends on noise_ratio parameter\")\n",
    "print(f\"  6. Current noise_ratio: {AUGMENTATION_CONFIG['noise_ratio']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e23864",
   "metadata": {},
   "source": [
    "### Batch Augmentation for Dataset Expansion\n",
    "\n",
    "Now let's demonstrate batch augmentation for processing multiple sequences efficiently. This is particularly useful for expanding training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate file-based batch augmentation\n",
    "input_file = Path(AUGMENTATION_CONFIG[\"data_dir\"]) / AUGMENTATION_CONFIG[\"train_file\"]\n",
    "output_file = Path(AUGMENTATION_CONFIG[\"data_dir\"]) / AUGMENTATION_CONFIG[\"output_file\"]\n",
    "\n",
    "print(\"üèóÔ∏è Demonstrating File-Based Batch Augmentation\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÇ Input:  {input_file}\")\n",
    "print(f\"üìÇ Output: {output_file}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Validation: Check input file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "try:\n",
    "    # Load original dataset to understand structure\n",
    "    print(\"\\n1Ô∏è‚É£ Loading original dataset...\")\n",
    "    with open(input_file, 'r') as f:\n",
    "        original_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(original_data)} sequences\")\n",
    "    \n",
    "    # Validate data format\n",
    "    required_keys = {\"seq\"}  # Must have \"seq\" key\n",
    "    if original_data:\n",
    "        sample = original_data[0]\n",
    "        if not required_keys.issubset(sample.keys()):\n",
    "            raise ValueError(\n",
    "                f\"Invalid JSON format. Expected keys: {required_keys}, \"\n",
    "                f\"Found: {set(sample.keys())}\"\n",
    "            )\n",
    "        print(f\"   ‚úì Data format validated\")\n",
    "        \n",
    "        # Show sample\n",
    "        sample_seq = sample[\"seq\"]\n",
    "        print(f\"\\n   üìù Sample original sequence:\")\n",
    "        print(f\"      seq: {sample_seq[:60]}...\" if len(sample_seq) > 60 else f\"      seq: {sample_seq}\")\n",
    "        if \"label\" in sample:\n",
    "            print(f\"      label: {sample['label'][:60]}...\" if len(str(sample['label'])) > 60 else f\"      label: {sample['label']}\")\n",
    "    \n",
    "    # Perform augmentation\n",
    "    print(f\"\\n2Ô∏è‚É£ Starting batch augmentation...\")\n",
    "    print(f\"   - Input sequences: {len(original_data)}\")\n",
    "    print(f\"   - Variants per sequence: {AUGMENTATION_CONFIG['instance_num']}\")\n",
    "    print(f\"   - Expected output: {len(original_data) * AUGMENTATION_CONFIG['instance_num']} sequences\")\n",
    "    print(f\"   - Batch size: {AUGMENTATION_CONFIG['batch_size']} (for MLM forward pass)\")\n",
    "    \n",
    "    # Call the augmentation method (with progress bar from tqdm)\n",
    "    augmentation_model.augment_from_file(\n",
    "        input_file=str(input_file),\n",
    "        output_file=str(output_file)\n",
    "    )\n",
    "    \n",
    "    # Verify output\n",
    "    print(f\"\\n3Ô∏è‚É£ Verifying output...\")\n",
    "    with open(output_file, 'r') as f:\n",
    "        augmented_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"   ‚úì Output file created: {output_file}\")\n",
    "    print(f\"   ‚úì Augmented sequences written: {len(augmented_data)}\")\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\n‚úÖ Augmentation Completed Successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä Results Summary:\")\n",
    "    print(f\"   Original sequences:    {len(original_data):>6}\")\n",
    "    print(f\"   Augmented sequences:   {len(augmented_data):>6}\")\n",
    "    print(f\"   Expansion ratio:       {len(augmented_data)/len(original_data):>6.1f}x\")\n",
    "    print(f\"   Output file size:      {output_file.stat().st_size / 1024:>6.1f} KB\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show sample augmented sequence\n",
    "    if augmented_data:\n",
    "        sample_aug = augmented_data[0]\n",
    "        print(f\"\\nüìù Sample augmented sequence:\")\n",
    "        aug_seq = sample_aug.get(\"aug_seq\", sample_aug.get(\"seq\", \"\"))\n",
    "        print(f\"   aug_seq: {aug_seq[:60]}...\" if len(aug_seq) > 60 else f\"   aug_seq: {aug_seq}\")\n",
    "    \n",
    "    # Quick quality check\n",
    "    print(f\"\\nüî¨ Quick Quality Check:\")\n",
    "    aug_lengths = [len(item.get(\"aug_seq\", item.get(\"seq\", \"\"))) for item in augmented_data[:100]]\n",
    "    orig_lengths = [len(item[\"seq\"]) for item in original_data[:100]]\n",
    "    \n",
    "    print(f\"   Original avg length: {np.mean(orig_lengths):.1f} ¬± {np.std(orig_lengths):.1f} nt\")\n",
    "    print(f\"   Augmented avg length: {np.mean(aug_lengths):.1f} ¬± {np.std(aug_lengths):.1f} nt\")\n",
    "    print(f\"   Length preservation: {'‚úì Good' if abs(np.mean(aug_lengths) - np.mean(orig_lengths)) < 5 else '‚ö†Ô∏è Check'}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå File not found: {str(e)}\")\n",
    "    print(\"   Ensure toy_datasets directory exists with train.json\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå JSON parsing error: {str(e)}\")\n",
    "    print(\"   Check input file format - expecting one JSON object per line\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Batch augmentation failed: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "print(\"\\nüí° Usage Notes:\")\n",
    "print(\"  - augment_from_file() expects JSON with 'seq' key\")\n",
    "print(\"  - Output uses 'aug_seq' key to distinguish from originals\")\n",
    "print(\"  - Original labels/metadata are NOT preserved automatically\")\n",
    "print(\"  - For training, you may need to merge original + augmented data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b736c",
   "metadata": {},
   "source": [
    "## üîÆ Step 4: Quality Assessment and Analysis\n",
    "\n",
    "The final step involves comprehensive analysis of our augmented sequences to ensure they maintain biological validity while providing useful diversity for training.\n",
    "\n",
    "### Quality Assessment Pipeline\n",
    "\n",
    "Our assessment includes:\n",
    "1. **Sequence Diversity Analysis**: Measure how different augmented sequences are from originals\n",
    "2. **Biological Property Conservation**: Check if important sequence characteristics are preserved\n",
    "3. **Statistical Validation**: Ensure augmented sequences follow expected genomic patterns\n",
    "4. **Functional Motif Preservation**: Verify that critical sequence elements remain intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive quality assessment of augmented sequences\n",
    "def analyze_sequence_properties(sequences, labels=None):\n",
    "    \"\"\"Analyze statistical properties of genomic sequences\"\"\"\n",
    "    \n",
    "    if not sequences or len(sequences) == 0:\n",
    "        return {\n",
    "            'num_sequences': 0,\n",
    "            'avg_length': 0,\n",
    "            'length_std': 0,\n",
    "            'gc_content': [],\n",
    "            'avg_gc_content': 0,\n",
    "            'gc_content_std': 0,\n",
    "            'nucleotide_composition': {}\n",
    "        }\n",
    "    \n",
    "    analysis = {\n",
    "        'num_sequences': len(sequences),\n",
    "        'avg_length': np.mean([len(seq) for seq in sequences]),\n",
    "        'length_std': np.std([len(seq) for seq in sequences]),\n",
    "        'gc_content': [],\n",
    "        'nucleotide_composition': {'A': [], 'T': [], 'G': [], 'C': [], 'U': []},\n",
    "    }\n",
    "    \n",
    "    for seq in sequences:\n",
    "        seq_upper = seq.upper()\n",
    "        length = len(seq_upper)\n",
    "        \n",
    "        if length == 0:\n",
    "            continue\n",
    "            \n",
    "        # GC content\n",
    "        gc = (seq_upper.count('G') + seq_upper.count('C')) / length\n",
    "        analysis['gc_content'].append(gc)\n",
    "        \n",
    "        # Nucleotide composition\n",
    "        for nuc in ['A', 'T', 'G', 'C', 'U']:\n",
    "            freq = seq_upper.count(nuc) / length\n",
    "            analysis['nucleotide_composition'][nuc].append(freq)\n",
    "    \n",
    "    # Convert to means and stds\n",
    "    if analysis['gc_content']:\n",
    "        analysis['avg_gc_content'] = np.mean(analysis['gc_content'])\n",
    "        analysis['gc_content_std'] = np.std(analysis['gc_content'])\n",
    "    else:\n",
    "        analysis['avg_gc_content'] = 0\n",
    "        analysis['gc_content_std'] = 0\n",
    "    \n",
    "    for nuc in analysis['nucleotide_composition']:\n",
    "        freqs = analysis['nucleotide_composition'][nuc]\n",
    "        if freqs:\n",
    "            analysis['nucleotide_composition'][nuc] = {\n",
    "                'mean': np.mean(freqs),\n",
    "                'std': np.std(freqs)\n",
    "            }\n",
    "        else:\n",
    "            analysis['nucleotide_composition'][nuc] = {'mean': 0, 'std': 0}\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"üî¨ Performing comprehensive quality assessment...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load both original and augmented data for comparison\n",
    "input_file_path = Path(AUGMENTATION_CONFIG[\"data_dir\"]) / AUGMENTATION_CONFIG[\"train_file\"]\n",
    "output_file_path = Path(AUGMENTATION_CONFIG[\"data_dir\"]) / AUGMENTATION_CONFIG[\"output_file\"]\n",
    "\n",
    "try:\n",
    "    # Original sequences\n",
    "    print(\"\\n1Ô∏è‚É£ Loading original sequences...\")\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        original_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    original_sequences = [item.get('seq', '') for item in original_data]\n",
    "    print(f\"   ‚úì Loaded {len(original_sequences)} original sequences\")\n",
    "    \n",
    "    # Augmented sequences\n",
    "    print(\"\\n2Ô∏è‚É£ Loading augmented sequences...\")\n",
    "    if output_file_path.exists():\n",
    "        with open(output_file_path, 'r') as f:\n",
    "            augmented_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "        augmented_sequences = [item.get('aug_seq', item.get('seq', '')) for item in augmented_data]\n",
    "        print(f\"   ‚úì Loaded {len(augmented_sequences)} augmented sequences\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Augmented file not found: {output_file_path}\")\n",
    "        print(\"   Generating sample augmented sequences for demonstration...\")\n",
    "        augmented_sequences = []\n",
    "        for seq in original_sequences[:5]:  # Augment first 5 sequences\n",
    "            try:\n",
    "                aug_seqs = augmentation_model.augment(seq, k=1)\n",
    "                if aug_seqs:\n",
    "                    augmented_sequences.extend(aug_seqs)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to augment sequence: {e}\")\n",
    "                augmented_sequences.append(seq)  # Fallback to original\n",
    "    \n",
    "    # Analyze both datasets\n",
    "    print(\"\\n3Ô∏è‚É£ Analyzing sequence properties...\")\n",
    "    original_analysis = analyze_sequence_properties(original_sequences)\n",
    "    augmented_analysis = analyze_sequence_properties(augmented_sequences)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéØ QUALITY ASSESSMENT RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nüìà Dataset Size Comparison:\")\n",
    "    print(f\"  Original sequences:    {original_analysis['num_sequences']:>6}\")\n",
    "    print(f\"  Augmented sequences:   {augmented_analysis['num_sequences']:>6}\")\n",
    "    if original_analysis['num_sequences'] > 0:\n",
    "        expansion_ratio = augmented_analysis['num_sequences'] / original_analysis['num_sequences']\n",
    "        print(f\"  Dataset expansion:     {expansion_ratio:>6.1f}x\")\n",
    "        print(f\"  Expected expansion:    {AUGMENTATION_CONFIG['instance_num']:>6}x (from config)\")\n",
    "    \n",
    "    print(\"\\nüìè Sequence Length Statistics:\")\n",
    "    print(f\"  Original:   {original_analysis['avg_length']:>6.1f} ¬± {original_analysis['length_std']:.1f} nt\")\n",
    "    print(f\"  Augmented:  {augmented_analysis['avg_length']:>6.1f} ¬± {augmented_analysis['length_std']:.1f} nt\")\n",
    "    len_preservation = abs(original_analysis['avg_length'] - augmented_analysis['avg_length'])\n",
    "    status = '‚úÖ Excellent' if len_preservation < 1.0 else '‚úì Good' if len_preservation < 5.0 else '‚ö†Ô∏è Check'\n",
    "    print(f\"  Preservation: {status} (Œî={len_preservation:.1f}nt)\")\n",
    "    \n",
    "    print(\"\\nüß¨ GC Content Analysis:\")\n",
    "    print(f\"  Original:   {original_analysis['avg_gc_content']:>6.1%} ¬± {original_analysis['gc_content_std']:.1%}\")\n",
    "    print(f\"  Augmented:  {augmented_analysis['avg_gc_content']:>6.1%} ¬± {augmented_analysis['gc_content_std']:.1%}\")\n",
    "    gc_diff = abs(original_analysis['avg_gc_content'] - augmented_analysis['avg_gc_content'])\n",
    "    gc_status = '‚úÖ Excellent' if gc_diff < 0.02 else '‚úì Good' if gc_diff < 0.05 else '‚ö†Ô∏è Check' if gc_diff < 0.1 else '‚ùå Large'\n",
    "    print(f\"  Preservation: {gc_status} (Œî={gc_diff:.1%})\")\n",
    "    \n",
    "    print(\"\\nüî§ Nucleotide Composition Comparison:\")\n",
    "    print(\"  Nucleotide  Original    Augmented   Difference  Status\")\n",
    "    print(\"  \" + \"‚îÄ\" * 60)\n",
    "    for nuc in ['A', 'T', 'U', 'G', 'C']:\n",
    "        orig_freq = original_analysis['nucleotide_composition'][nuc]['mean']\n",
    "        aug_freq = augmented_analysis['nucleotide_composition'][nuc]['mean']\n",
    "        diff = aug_freq - orig_freq\n",
    "        if orig_freq > 0.01 or aug_freq > 0.01:  # Only show significant nucleotides\n",
    "            status = '‚úÖ' if abs(diff) < 0.02 else '‚úì' if abs(diff) < 0.05 else '‚ö†Ô∏è' if abs(diff) < 0.10 else '‚ùå'\n",
    "            print(f\"      {nuc}       {orig_freq:>6.1%}      {aug_freq:>6.1%}      {diff:>+6.1%}     {status}\")\n",
    "    \n",
    "    # Sequence diversity analysis\n",
    "    if len(original_sequences) > 0 and len(augmented_sequences) > 0:\n",
    "        print(\"\\nüé≤ Sequence Diversity Assessment:\")\n",
    "        \n",
    "        # Sample sequences for comparison\n",
    "        sample_size = min(10, len(original_sequences), len(augmented_sequences))\n",
    "        orig_sample = original_sequences[:sample_size]\n",
    "        aug_sample = augmented_sequences[:sample_size]\n",
    "        \n",
    "        # Calculate pairwise similarities within each set\n",
    "        def pairwise_similarity(sequences):\n",
    "            similarities = []\n",
    "            for i in range(len(sequences)):\n",
    "                for j in range(i+1, len(sequences)):\n",
    "                    seq1, seq2 = sequences[i], sequences[j]\n",
    "                    min_len = min(len(seq1), len(seq2))\n",
    "                    if min_len > 0:\n",
    "                        matches = sum(1 for a, b in zip(seq1[:min_len], seq2[:min_len]) if a == b)\n",
    "                        similarity = matches / min_len\n",
    "                        similarities.append(similarity)\n",
    "            return np.mean(similarities) if similarities else 0\n",
    "        \n",
    "        orig_diversity = 1 - pairwise_similarity(orig_sample)\n",
    "        aug_diversity = 1 - pairwise_similarity(aug_sample)\n",
    "        \n",
    "        print(f\"  Original set diversity:   {orig_diversity:>6.1%}\")\n",
    "        print(f\"  Augmented set diversity:  {aug_diversity:>6.1%}\")\n",
    "        \n",
    "        if aug_diversity > orig_diversity * 0.95:\n",
    "            print(\"  ‚úÖ Augmentation maintained or increased diversity\")\n",
    "        elif aug_diversity > orig_diversity * 0.8:\n",
    "            print(\"  ‚úì Augmentation maintained reasonable diversity\") \n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Augmentation may have reduced diversity\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå File not found: {str(e)}\")\n",
    "    print(\"   Ensure the data directory and files exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Quality assessment failed: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "\n",
    "print(f\"\\nüéâ Quality assessment completed!\")\n",
    "print(\"\\nüöÄ Your augmented dataset is ready for:\")\n",
    "print(\"  ‚Ä¢ Training data expansion and class balancing\")\n",
    "print(\"  ‚Ä¢ Model robustness improvement and regularization\")\n",
    "print(\"  ‚Ä¢ Cross-validation and generalization testing\")\n",
    "print(\"  ‚Ä¢ Domain adaptation and transfer learning\")\n",
    "print(\"  ‚Ä¢ Rare variant analysis and representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a99df",
   "metadata": {},
   "source": [
    "## üéâ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on genomic data augmentation with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end workflow for intelligent genomic data augmentation. Specifically, you have:\n",
    "\n",
    "1. **Understood the \"Why\"**: Gained appreciation for the importance of data augmentation in genomic machine learning and how intelligent augmentation preserves biological patterns while increasing diversity.\n",
    "\n",
    "2. **Mastered the 4-Step Workflow**:\n",
    "   - **Step 1: Setup and Configuration**: You learned how to configure augmentation parameters and understand their impact on sequence generation quality.\n",
    "   - **Step 2: Model Initialization**: You saw how to leverage pre-trained genomic foundation models for context-aware sequence augmentation.\n",
    "   - **Step 3: Sequence Augmentation**: You implemented both single sequence and batch augmentation strategies for different use cases.\n",
    "   - **Step 4: Quality Assessment**: You performed comprehensive analysis to validate the biological validity and diversity of augmented sequences.\n",
    "\n",
    "3. **Advanced Capabilities**: You explored:\n",
    "   - Intelligent masking strategies that preserve important sequence patterns\n",
    "   - Context-aware nucleotide prediction for realistic variations\n",
    "   - Batch processing for efficient dataset expansion\n",
    "   - Quality control metrics for validating augmented sequences\n",
    "   - Statistical analysis of sequence properties and diversity\n",
    "\n",
    "### Next Steps and Applications\n",
    "\n",
    "Your augmented datasets can now be applied to:\n",
    "- **Training Data Enhancement**: Expand small or imbalanced genomic datasets\n",
    "- **Model Robustness**: Improve generalization through increased sequence diversity\n",
    "- **Rare Variant Analysis**: Generate synthetic examples of underrepresented patterns\n",
    "- **Cross-Domain Learning**: Bridge gaps between different genomic contexts\n",
    "- **Validation Studies**: Create test sets for evaluating model robustness\n",
    "\n",
    "### Best Practices for Genomic Augmentation\n",
    "\n",
    "1. **Parameter Tuning**: Start with noise_ratio=0.15-0.25 and adjust based on your specific application\n",
    "2. **Quality Validation**: Always assess biological property conservation in augmented sequences\n",
    "3. **Diversity Balance**: Ensure augmentation increases diversity without breaking biological constraints\n",
    "4. **Domain Specificity**: Consider the specific genomic context (coding, regulatory, etc.) when setting parameters\n",
    "5. **Iterative Refinement**: Use validation metrics to fine-tune augmentation strategies\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "Explore our other tutorials to expand your genomic AI toolkit:\n",
    "- **[mRNA Degradation Prediction](../mRNA_degrad_rate_regression/)**: Apply augmented data to stability prediction\n",
    "- **[RNA Secondary Structure Prediction](../rna_secondary_structure_prediction/)**: Use augmentation for structure modeling\n",
    "- **[Translation Efficiency Prediction](../translation_efficiency_prediction/)**: Enhance training data for efficiency prediction\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and tools to effectively augment genomic datasets for your machine learning research. Intelligent data augmentation is a powerful technique for advancing genomic AI!\n",
    "\n",
    "**Happy augmenting and discovering! üß¨‚ú®**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0704c93",
   "metadata": {},
   "source": [
    "### Advanced Usage: Understanding the Augmentation API\n",
    "\n",
    "The `OmniModelForAugmentation` provides three levels of API for different use cases:\n",
    "\n",
    "#### High-Level API (Recommended)\n",
    "```python\n",
    "# Generate k augmented variants (automatic masking + prediction)\n",
    "augmented_seqs = augmentation_model.augment(seq=\"AUGCGAUCGAG\", k=3)\n",
    "# Returns: list of 3 augmented sequences\n",
    "```\n",
    "\n",
    "#### Batch Processing API\n",
    "```python\n",
    "# Process multiple sequences efficiently\n",
    "sequences = [\"ATCGATCG\", \"GCTAGCTA\", \"CGATCGAT\"]\n",
    "all_augmented = augmentation_model.augment_sequences(sequences)\n",
    "# Returns: list of len(sequences) * k augmented sequences\n",
    "```\n",
    "\n",
    "#### File-Based API (For Large Datasets)\n",
    "```python\n",
    "# Process entire datasets from JSON files\n",
    "augmentation_model.augment_from_file(\n",
    "    input_file=\"train.json\",    # Each line: {\"seq\": \"ATCG...\"}\n",
    "    output_file=\"augmented.json\"  # Each line: {\"aug_seq\": \"ATCG...\"}\n",
    ")\n",
    "```\n",
    "\n",
    "#### Low-Level API (Advanced Users Only)\n",
    "```python\n",
    "# Step 1: Manually apply masking\n",
    "masked_seq = augmentation_model.apply_noise_to_sequence(\"AUGCGAUCGAG\")\n",
    "print(f\"Masked: {masked_seq}\")  # e.g., \"AUG[MASK]GA[MASK]CGAG\"\n",
    "\n",
    "# Step 2: Predict masked positions (expects already-masked sequence)\n",
    "augmented_seq = augmentation_model.augment_sequence(masked_seq)\n",
    "print(f\"Predicted: {augmented_seq}\")\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Important API Notes:**\n",
    "- `augment(seq, k)` - **Recommended** high-level API (auto-masks + predicts)\n",
    "- `augment_sequences(seqs)` - Batch processing with automatic buffering\n",
    "- `augment_from_file(input, output)` - File-based processing with progress bar\n",
    "- `apply_noise_to_sequence(seq)` - Low-level: masking only\n",
    "- `augment_sequence(masked_seq)` - Low-level: prediction only (expects pre-masked input)\n",
    "\n",
    "**Expected Input/Output Formats:**\n",
    "\n",
    "Input JSON (one object per line):\n",
    "```json\n",
    "{\"seq\": \"ATCTTGCATTGAAG\"}\n",
    "{\"seq\": \"GGTTTACAGTCCAA\"}\n",
    "```\n",
    "\n",
    "Output JSON (one object per line):\n",
    "```json\n",
    "{\"aug_seq\": \"ATCTTGCATAGAAG\"}\n",
    "{\"aug_seq\": \"GGTTTACAATCCAA\"}\n",
    "```\n",
    "\n",
    "**Note:** The low-level API (`apply_noise_to_sequence` + `augment_sequence`) is for advanced users who need custom masking strategies. Most users should use the high-level `augment(seq, k)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d30d2f",
   "metadata": {},
   "source": [
    "### Configuration Parameter Reference\n",
    "\n",
    "**Key hyperparameters and their effects:**\n",
    "\n",
    "| Parameter | Type | Default | Range | Effect |\n",
    "|-----------|------|---------|-------|--------|\n",
    "| `noise_ratio` | float | 0.15 | 0.05-0.30 | Proportion of tokens masked per sequence |\n",
    "| `max_length` | int | 1026 | 50-2048 | Maximum sequence length (longer = more GPU memory) |\n",
    "| `instance_num` | int | 1 | 1-10 | Variants generated per input sequence |\n",
    "| `batch_size` | int | 32 | 1-128 | MLM forward pass batch size (reduce if OOM) |\n",
    "| `use_amp` | bool | auto | - | Automatic mixed precision (auto-detects CUDA) |\n",
    "\n",
    "**Tuning Guidelines:**\n",
    "- **For diversity**: Increase `noise_ratio` to 0.25-0.30\n",
    "- **For safety**: Decrease `noise_ratio` to 0.10-0.15  \n",
    "- **For speed**: Increase `batch_size` (if GPU memory allows)\n",
    "- **For rare variants**: Increase `instance_num` to 5-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16a8f4",
   "metadata": {},
   "source": [
    "### Integrating Augmentation into Training Pipelines\n",
    "\n",
    "**Option 1: Pre-augment dataset (recommended for static datasets)**\n",
    "```python\n",
    "# Augment once, save to disk, reuse across training runs\n",
    "augmentation_model.augment_from_file(\"train.json\", \"train_augmented.json\")\n",
    "\n",
    "# Later in training code\n",
    "train_dataset = OmniDatasetForSequenceClassification(\n",
    "    data_file=\"train_augmented.json\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "```\n",
    "\n",
    "**Option 2: On-the-fly augmentation (recommended for large datasets)**\n",
    "```python\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_data, augmentor, aug_ratio=0.5):\n",
    "        self.data = base_data\n",
    "        self.augmentor = augmentor\n",
    "        self.aug_ratio = aug_ratio  # 50% augmented, 50% original\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if np.random.rand() < self.aug_ratio:\n",
    "            aug_seq = self.augmentor.augment(sample[\"seq\"], k=1)[0]\n",
    "            return {\"seq\": aug_seq, \"label\": sample[\"label\"]}\n",
    "        return sample\n",
    "```\n",
    "\n",
    "**Option 3: Hybrid approach (balanced augmentation)**\n",
    "```python\n",
    "# Keep originals + add augmented variants\n",
    "original_data = load_data(\"train.json\")\n",
    "augmented_seqs = augmentor.augment_sequences([d[\"seq\"] for d in original_data])\n",
    "\n",
    "combined_data = original_data + [\n",
    "    {\"seq\": aug_seq, \"label\": original_data[i // k][\"label\"]}\n",
    "    for i, aug_seq in enumerate(augmented_seqs)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e69a26",
   "metadata": {},
   "source": [
    "## üìö References and Further Reading\n",
    "\n",
    "### Academic Foundation\n",
    "1. **Masked Language Modeling**: Devlin et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "2. **Genomic Foundation Models**: Ji et al. (2021). \"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome\"\n",
    "3. **Data Augmentation Theory**: Shorten & Khoshgoftaar (2019). \"A survey on Image Data Augmentation for Deep Learning\"\n",
    "\n",
    "### OmniGenBench Documentation\n",
    "- **API Reference**: [omnigenbench.readthedocs.io](https://omnigenbench.readthedocs.io)\n",
    "- **Model Hub**: [huggingface.co/yangheng](https://huggingface.co/yangheng)\n",
    "- **GitHub Repository**: [github.com/yangheng95/OmniGenBench](https://github.com/yangheng95/OmniGenBench)\n",
    "\n",
    "### Related Tutorials\n",
    "- `mRNA_degrad_rate_regression/` - Applying augmented data to regression tasks\n",
    "- `rna_secondary_structure_prediction/` - Structure-aware augmentation\n",
    "- `translation_efficiency_prediction/` - Functional sequence augmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Reproducibility Checklist:**\n",
    "- ‚úÖ Environment validated (Python/PyTorch/CUDA versions)\n",
    "- ‚úÖ Random seeds set for deterministic results  \n",
    "- ‚úÖ Configuration centralized in SSoT (AUGMENTATION_CONFIG)\n",
    "- ‚úÖ All file paths validated before processing\n",
    "- ‚úÖ Error handling with informative messages\n",
    "- ‚úÖ Output verification with statistical checks\n",
    "- ‚úÖ API usage documented with examples\n",
    "\n",
    "**Troubleshooting Common Issues:**\n",
    "- **\"CUDA out of memory\"**: Reduce `batch_size` or `max_length`\n",
    "- **\"Model download timeout\"**: Check HuggingFace Hub connectivity\n",
    "- **\"Invalid nucleotides\"**: Verify input sequences contain only ATCGU\n",
    "- **\"Empty output file\"**: Check input JSON format (must have \"seq\" key)\n",
    "\n",
    "---\n",
    "\n",
    "¬© 2025 OmniGenBench Project. Licensed under MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
