{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702828e2486d156f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6a2991",
   "metadata": {},
   "source": [
    "# 🧬 Genomic Data Augmentation with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to perform **intelligent genomic data augmentation** using **OmniGenBench**. This guide will walk you through the process of generating high-quality synthetic genomic sequences that preserve biological patterns and improve model training performance.\n",
    "\n",
    "### 1. The Machine Learning Challenge: Why Genomic Data Augmentation?\n",
    "\n",
    "**Genomic data augmentation** is a critical technique in computational biology that addresses several fundamental challenges in genomic machine learning:\n",
    "\n",
    "- **Limited Training Data**: High-quality labeled genomic datasets are often small and expensive to generate\n",
    "- **Class Imbalance**: Rare genomic variants and functions are underrepresented in datasets\n",
    "- **Overfitting Prevention**: Augmentation increases dataset diversity and improves generalization\n",
    "- **Domain Adaptation**: Bridging gaps between different experimental conditions or species\n",
    "\n",
    "The power of genomic augmentation lies in its ability to:\n",
    "- **Generate Realistic Sequences**: Create biologically plausible variants while preserving functional patterns\n",
    "- **Expand Dataset Size**: Multiply available training data without additional experimental costs\n",
    "- **Improve Model Robustness**: Enhance model performance on unseen genomic variations\n",
    "- **Balance Datasets**: Address class imbalance issues in genomic classification tasks\n",
    "\n",
    "Applications across computational biology:\n",
    "- **Rare Variant Analysis**: Augment underrepresented mutation patterns for disease prediction\n",
    "- **Cross-Species Learning**: Generate bridge sequences for evolutionary studies\n",
    "- **Functional Annotation**: Create training data for poorly characterized genomic regions\n",
    "- **Model Validation**: Generate test sequences for robustness evaluation\n",
    "\n",
    "### 2. The Challenge: Biologically-Informed Sequence Generation\n",
    "\n",
    "Unlike random mutations, intelligent genomic augmentation must preserve:\n",
    "\n",
    "- **Functional Motifs**: Critical regulatory and coding sequences\n",
    "- **Structural Constraints**: Secondary structures and folding patterns\n",
    "- **Evolutionary Patterns**: Codon usage bias and phylogenetic relationships  \n",
    "- **Statistical Properties**: Nucleotide composition and k-mer frequencies\n",
    "\n",
    "**Augmentation Process:**\n",
    "\n",
    "| Original Sequence | Random Mutation | Intelligent Augmentation |\n",
    "|------------------|-----------------|-------------------------|\n",
    "| `ATGCGATCG` | `ATGCTATCG` (random) | `ATGCGATCC` (codon-aware) |\n",
    "| Functional | May break function | Preserves function |\n",
    "\n",
    "### 3. The Tool: Masked Language Models for Genomic Augmentation\n",
    "\n",
    "#### Foundation Model Understanding\n",
    "**OmniGenome** uses masked language modeling (MLM) for intelligent sequence augmentation. This approach:\n",
    "\n",
    "1. **Masks Strategic Positions**: Selectively mask nucleotides while preserving critical patterns\n",
    "2. **Predicts Biologically Plausible Alternatives**: Use pre-trained understanding to suggest realistic substitutions\n",
    "3. **Maintains Sequence Integrity**: Ensure augmented sequences remain biologically valid\n",
    "4. **Preserves Functional Patterns**: Keep important motifs and regulatory elements intact\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Genomic Augmentation\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for Genomic Data Augmentation\"\n",
    "        A[\"📥 Step 1: Setup and Configuration<br/>Configure augmentation parameters and models\"] --> B[\"🔧 Step 2: Model Initialization<br/>Load pre-trained genomic foundation models\"]\n",
    "        B --> C[\"🎓 Step 3: Sequence Augmentation<br/>Generate diverse, biologically-valid variants\"]\n",
    "        C --> D[\"🔮 Step 4: Quality Assessment<br/>Validate and analyze augmented sequences\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start generating high-quality genomic training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1def0",
   "metadata": {},
   "source": [
    "## 🚀 Step 1: Setup and Configuration\n",
    "\n",
    "This first step focuses on setting up our genomic data augmentation environment and understanding the key parameters that control sequence generation quality.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required packages for intelligent genomic data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omnigenbench torch transformers tqdm scikit-learn matplotlib seaborn -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f25c5",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Next, we import the essential libraries for genomic data augmentation, including specialized tools for sequence analysis and quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23213f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.metrics import jaccard_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omnigenbench import (\n",
    "    OmniModelForAugmentation,\n",
    "    ModelHub,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e1c24",
   "metadata": {},
   "source": [
    "### 1.3: Understanding Augmentation Parameters\n",
    "\n",
    "Before we start augmentation, let's understand the key parameters that control the quality and diversity of generated sequences:\n",
    "\n",
    "#### Critical Parameters\n",
    "- **noise_ratio**: Proportion of tokens to mask (0.15-0.25 typically optimal)\n",
    "- **instance_num**: Number of variants per original sequence\n",
    "- **max_length**: Maximum sequence length for processing\n",
    "- **model_selection**: Choice of pre-trained genomic foundation model\n",
    "\n",
    "These parameters balance between sequence diversity and biological validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69e556085cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration for genomic data augmentation\n",
    "augmentation_config = {\n",
    "    \"model_name\": \"yangheng/OmniGenome-52M\",\n",
    "    \"noise_ratio\": 0.2,      # 20% of tokens will be masked\n",
    "    \"max_length\": 512,       # Maximum sequence length\n",
    "    \"instance_num\": 3,       # Generate 3 variants per sequence\n",
    "    \"batch_size\": 8,         # Batch processing size\n",
    "}\n",
    "\n",
    "print(\"🎯 Genomic Data Augmentation Configuration:\")\n",
    "print(f\"  Model: {augmentation_config['model_name']}\")\n",
    "print(f\"  Noise ratio: {augmentation_config['noise_ratio']:.1%}\")\n",
    "print(f\"  Max length: {augmentation_config['max_length']}\")\n",
    "print(f\"  Instances per sequence: {augmentation_config['instance_num']}\")\n",
    "print(f\"  Batch size: {augmentation_config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906988c0",
   "metadata": {},
   "source": [
    "## 🚀 Step 2: Model Initialization\n",
    "\n",
    "Now let's initialize the genomic augmentation model. The `OmniModelForAugmentation` leverages pre-trained genomic foundation models to generate biologically-informed sequence variants.\n",
    "\n",
    "### Augmentation Model Features\n",
    "- **Intelligent Masking**: Strategic selection of positions for variation\n",
    "- **Contextual Prediction**: Uses surrounding sequence context for realistic substitutions  \n",
    "- **Batch Processing**: Efficient handling of multiple sequences\n",
    "- **Quality Control**: Built-in validation of augmented sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the genomic augmentation model\n",
    "print(\"🔧 Initializing Genomic Data Augmentation Model...\")\n",
    "\n",
    "augmentation_model = OmniModelForAugmentation(\n",
    "    config_or_model=augmentation_config[\"model_name\"],\n",
    "    noise_ratio=augmentation_config[\"noise_ratio\"],\n",
    "    max_length=augmentation_config[\"max_length\"],\n",
    "    instance_num=augmentation_config[\"instance_num\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Augmentation model initialized successfully!\")\n",
    "print(\"🎯 Model capabilities:\")\n",
    "print(\"  - Intelligent sequence masking based on genomic patterns\")\n",
    "print(\"  - Context-aware nucleotide prediction\")\n",
    "print(\"  - Batch processing for efficiency\")\n",
    "print(\"  - Preservation of biological sequence properties\")\n",
    "print(f\"  - Configured for {augmentation_config['instance_num']} variants per sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c290d9",
   "metadata": {},
   "source": [
    "## 🚀 Step 3: Sequence Augmentation\n",
    "\n",
    "Now comes the exciting part! We'll demonstrate different approaches to genomic data augmentation, from single sequences to batch processing of entire datasets.\n",
    "\n",
    "### Our Augmentation Strategy\n",
    "\n",
    "We'll explore multiple augmentation scenarios:\n",
    "\n",
    "1. **Single Sequence Augmentation**: Generate variants for individual sequences\n",
    "2. **Batch Augmentation**: Process multiple sequences efficiently\n",
    "3. **File-based Augmentation**: Handle large datasets from files\n",
    "4. **Quality-controlled Augmentation**: Ensure biological validity of outputs\n",
    "\n",
    "Let's start with single sequence augmentation to understand the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate single sequence augmentation\n",
    "test_sequences = {\n",
    "    \"Coding sequence\": \"ATGAAAGCCATTGAGAAGGCAAAACCCCGATGGTCCTTCGCGAA\",\n",
    "    \"UTR region\": \"AUUGAGAUGUUUGCCAUUUUGACCAUCUGACCUUUGCCAUC\",\n",
    "    \"Regulatory motif\": \"TATAAGCCGCGGTGACCTGCAG\",\n",
    "    \"Random sequence\": \"ATCGATCGATCGATCGATCG\"\n",
    "}\n",
    "\n",
    "print(\"🎓 Demonstrating single sequence augmentation...\")\n",
    "print(\"⚡ Using intelligent masking and contextual prediction\")\n",
    "\n",
    "for seq_name, sequence in test_sequences.items():\n",
    "    print(f\"\\n📊 Augmenting: {seq_name}\")\n",
    "    print(f\"  Original: {sequence}\")\n",
    "    print(f\"  Length: {len(sequence)} nucleotides\")\n",
    "    \n",
    "    try:\n",
    "        # Generate augmented variants\n",
    "        augmented_sequence = augmentation_model.augment_sequence(sequence)\n",
    "        \n",
    "        print(f\"  Augmented: {augmented_sequence}\")\n",
    "        \n",
    "        # Analyze differences\n",
    "        differences = sum(1 for a, b in zip(sequence, augmented_sequence) if a != b)\n",
    "        similarity = (len(sequence) - differences) / len(sequence)\n",
    "        \n",
    "        print(f\"  📈 Analysis:\")\n",
    "        print(f\"    Changed positions: {differences}/{len(sequence)}\")\n",
    "        print(f\"    Sequence similarity: {similarity:.1%}\")\n",
    "        print(f\"    Effective noise ratio: {differences/len(sequence):.1%}\")\n",
    "        \n",
    "        # GC content analysis\n",
    "        def gc_content(seq):\n",
    "            return (seq.upper().count('G') + seq.upper().count('C')) / len(seq)\n",
    "        \n",
    "        orig_gc = gc_content(sequence)\n",
    "        aug_gc = gc_content(augmented_sequence)\n",
    "        print(f\"    GC content: {orig_gc:.1%} → {aug_gc:.1%} (Δ{aug_gc-orig_gc:+.1%})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Augmentation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"─\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e23864",
   "metadata": {},
   "source": [
    "### Batch Augmentation for Dataset Expansion\n",
    "\n",
    "Now let's demonstrate batch augmentation for processing multiple sequences efficiently. This is particularly useful for expanding training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate file-based augmentation using existing toy dataset\n",
    "input_file = \"toy_datasets/train.json\"\n",
    "output_file = \"toy_datasets/augmented_sequences.json\"\n",
    "\n",
    "print(\"🏗️ Demonstrating file-based batch augmentation...\")\n",
    "print(f\"📂 Input file: {input_file}\")\n",
    "print(f\"📂 Output file: {output_file}\")\n",
    "\n",
    "try:\n",
    "    # Load original dataset to understand structure\n",
    "    with open(input_file, 'r') as f:\n",
    "        original_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"📊 Original dataset: {len(original_data)} sequences\")\n",
    "    \n",
    "    # Show sample original sequence\n",
    "    if original_data:\n",
    "        sample = original_data[0]\n",
    "        print(f\"📝 Sample original sequence:\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 50:\n",
    "                print(f\"  {key}: {value[:50]}...\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Perform augmentation using the model's file processing method\n",
    "    print(f\"\\n🎓 Starting batch augmentation...\")\n",
    "    print(f\"  - Processing {len(original_data)} sequences\")\n",
    "    print(f\"  - Generating {augmentation_config['instance_num']} variants each\")\n",
    "    print(f\"  - Expected output: {len(original_data) * augmentation_config['instance_num']} sequences\")\n",
    "    \n",
    "    # Call the augmentation method\n",
    "    augmentation_model.augment_from_file(\n",
    "        input_file=input_file,\n",
    "        output_file=output_file\n",
    "    )\n",
    "    \n",
    "    # Verify output\n",
    "    with open(output_file, 'r') as f:\n",
    "        augmented_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"✅ Augmentation completed!\")\n",
    "    print(f\"📊 Results:\")\n",
    "    print(f\"  - Original sequences: {len(original_data)}\")\n",
    "    print(f\"  - Augmented sequences: {len(augmented_data)}\")\n",
    "    print(f\"  - Expansion ratio: {len(augmented_data)/len(original_data):.1f}x\")\n",
    "    \n",
    "    # Show sample augmented sequence\n",
    "    if augmented_data:\n",
    "        sample_aug = augmented_data[0]\n",
    "        print(f\"\\n📝 Sample augmented sequence:\")\n",
    "        for key, value in sample_aug.items():\n",
    "            if isinstance(value, str) and len(value) > 50:\n",
    "                print(f\"  {key}: {value[:50]}...\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Batch augmentation failed: {str(e)}\")\n",
    "    print(\"This might be due to file format or model compatibility issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b736c",
   "metadata": {},
   "source": [
    "## 🔮 Step 4: Quality Assessment and Analysis\n",
    "\n",
    "The final step involves comprehensive analysis of our augmented sequences to ensure they maintain biological validity while providing useful diversity for training.\n",
    "\n",
    "### Quality Assessment Pipeline\n",
    "\n",
    "Our assessment includes:\n",
    "1. **Sequence Diversity Analysis**: Measure how different augmented sequences are from originals\n",
    "2. **Biological Property Conservation**: Check if important sequence characteristics are preserved\n",
    "3. **Statistical Validation**: Ensure augmented sequences follow expected genomic patterns\n",
    "4. **Functional Motif Preservation**: Verify that critical sequence elements remain intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive quality assessment of augmented sequences\n",
    "def analyze_sequence_properties(sequences, labels=None):\n",
    "    \"\"\"Analyze statistical properties of genomic sequences\"\"\"\n",
    "    \n",
    "    analysis = {\n",
    "        'num_sequences': len(sequences),\n",
    "        'avg_length': np.mean([len(seq) for seq in sequences]),\n",
    "        'length_std': np.std([len(seq) for seq in sequences]),\n",
    "        'gc_content': [],\n",
    "        'nucleotide_composition': {'A': [], 'T': [], 'G': [], 'C': [], 'U': []},\n",
    "    }\n",
    "    \n",
    "    for seq in sequences:\n",
    "        seq_upper = seq.upper()\n",
    "        length = len(seq_upper)\n",
    "        \n",
    "        # GC content\n",
    "        gc = (seq_upper.count('G') + seq_upper.count('C')) / length if length > 0 else 0\n",
    "        analysis['gc_content'].append(gc)\n",
    "        \n",
    "        # Nucleotide composition\n",
    "        for nuc in ['A', 'T', 'G', 'C', 'U']:\n",
    "            freq = seq_upper.count(nuc) / length if length > 0 else 0\n",
    "            analysis['nucleotide_composition'][nuc].append(freq)\n",
    "    \n",
    "    # Convert to means and stds\n",
    "    analysis['avg_gc_content'] = np.mean(analysis['gc_content'])\n",
    "    analysis['gc_content_std'] = np.std(analysis['gc_content'])\n",
    "    \n",
    "    for nuc in analysis['nucleotide_composition']:\n",
    "        freqs = analysis['nucleotide_composition'][nuc]\n",
    "        analysis['nucleotide_composition'][nuc] = {\n",
    "            'mean': np.mean(freqs),\n",
    "            'std': np.std(freqs)\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"🔬 Performing comprehensive quality assessment...\")\n",
    "\n",
    "# Load both original and augmented data for comparison\n",
    "try:\n",
    "    # Original sequences\n",
    "    with open(\"toy_datasets/train.json\", 'r') as f:\n",
    "        original_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    original_sequences = [item.get('sequence', item.get('seq', '')) for item in original_data]\n",
    "    \n",
    "    # Augmented sequences (if they exist)\n",
    "    try:\n",
    "        with open(\"toy_datasets/augmented_sequences.json\", 'r') as f:\n",
    "            augmented_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "        augmented_sequences = [item.get('sequence', item.get('seq', '')) for item in augmented_data]\n",
    "    except:\n",
    "        # Generate some examples for analysis if file doesn't exist\n",
    "        print(\"Generating sample augmented sequences for analysis...\")\n",
    "        augmented_sequences = []\n",
    "        for seq in original_sequences[:5]:  # Augment first 5 sequences\n",
    "            try:\n",
    "                aug_seq = augmentation_model.augment_sequence(seq)\n",
    "                augmented_sequences.append(aug_seq)\n",
    "            except:\n",
    "                augmented_sequences.append(seq)  # Fallback to original\n",
    "    \n",
    "    # Analyze both datasets\n",
    "    print(\"📊 Analyzing original sequences...\")\n",
    "    original_analysis = analyze_sequence_properties(original_sequences)\n",
    "    \n",
    "    print(\"📊 Analyzing augmented sequences...\")\n",
    "    augmented_analysis = analyze_sequence_properties(augmented_sequences)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    print(\"\\n🎯 Quality Assessment Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"📈 Dataset Size Comparison:\")\n",
    "    print(f\"  Original sequences: {original_analysis['num_sequences']}\")\n",
    "    print(f\"  Augmented sequences: {augmented_analysis['num_sequences']}\")\n",
    "    if original_analysis['num_sequences'] > 0:\n",
    "        expansion_ratio = augmented_analysis['num_sequences'] / original_analysis['num_sequences']\n",
    "        print(f\"  Dataset expansion: {expansion_ratio:.1f}x\")\n",
    "    \n",
    "    print(\"\\n📏 Sequence Length Statistics:\")\n",
    "    print(f\"  Original: {original_analysis['avg_length']:.1f} ± {original_analysis['length_std']:.1f}\")\n",
    "    print(f\"  Augmented: {augmented_analysis['avg_length']:.1f} ± {augmented_analysis['length_std']:.1f}\")\n",
    "    \n",
    "    print(\"\\n🧬 GC Content Analysis:\")\n",
    "    print(f\"  Original: {original_analysis['avg_gc_content']:.1%} ± {original_analysis['gc_content_std']:.1%}\")\n",
    "    print(f\"  Augmented: {augmented_analysis['avg_gc_content']:.1%} ± {augmented_analysis['gc_content_std']:.1%}\")\n",
    "    gc_diff = abs(original_analysis['avg_gc_content'] - augmented_analysis['avg_gc_content'])\n",
    "    print(f\"  Difference: {gc_diff:.1%} ({'✅ Good' if gc_diff < 0.05 else '⚠️ Check' if gc_diff < 0.1 else '❌ Large'})\")\n",
    "    \n",
    "    print(\"\\n🔤 Nucleotide Composition Comparison:\")\n",
    "    for nuc in ['A', 'T', 'U', 'G', 'C']:\n",
    "        orig_freq = original_analysis['nucleotide_composition'][nuc]['mean']\n",
    "        aug_freq = augmented_analysis['nucleotide_composition'][nuc]['mean']\n",
    "        diff = abs(orig_freq - aug_freq)\n",
    "        if orig_freq > 0.01 or aug_freq > 0.01:  # Only show significant nucleotides\n",
    "            status = '✅' if diff < 0.02 else '⚠️' if diff < 0.05 else '❌'\n",
    "            print(f\"  {nuc}: {orig_freq:.1%} → {aug_freq:.1%} ({diff:+.1%}) {status}\")\n",
    "    \n",
    "    # Sequence diversity analysis\n",
    "    if len(original_sequences) > 0 and len(augmented_sequences) > 0:\n",
    "        print(\"\\n🎲 Sequence Diversity Assessment:\")\n",
    "        \n",
    "        # Sample sequences for comparison\n",
    "        sample_size = min(10, len(original_sequences), len(augmented_sequences))\n",
    "        orig_sample = original_sequences[:sample_size]\n",
    "        aug_sample = augmented_sequences[:sample_size]\n",
    "        \n",
    "        # Calculate pairwise similarities within each set\n",
    "        def pairwise_similarity(sequences):\n",
    "            similarities = []\n",
    "            for i in range(len(sequences)):\n",
    "                for j in range(i+1, len(sequences)):\n",
    "                    seq1, seq2 = sequences[i], sequences[j]\n",
    "                    min_len = min(len(seq1), len(seq2))\n",
    "                    if min_len > 0:\n",
    "                        matches = sum(1 for a, b in zip(seq1[:min_len], seq2[:min_len]) if a == b)\n",
    "                        similarity = matches / min_len\n",
    "                        similarities.append(similarity)\n",
    "            return np.mean(similarities) if similarities else 0\n",
    "        \n",
    "        orig_diversity = 1 - pairwise_similarity(orig_sample)\n",
    "        aug_diversity = 1 - pairwise_similarity(aug_sample)\n",
    "        \n",
    "        print(f\"  Original set diversity: {orig_diversity:.1%}\")\n",
    "        print(f\"  Augmented set diversity: {aug_diversity:.1%}\")\n",
    "        \n",
    "        if aug_diversity > orig_diversity:\n",
    "            print(\"  ✅ Augmentation increased sequence diversity\")\n",
    "        elif aug_diversity > orig_diversity * 0.8:\n",
    "            print(\"  ⚠️ Augmentation maintained reasonable diversity\") \n",
    "        else:\n",
    "            print(\"  ❌ Augmentation may have reduced diversity\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Quality assessment failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\n🎉 Quality assessment completed!\")\n",
    "print(\"🚀 Your augmented dataset is ready for:\")\n",
    "print(\"  - Training data expansion and class balancing\")\n",
    "print(\"  - Model robustness improvement\")\n",
    "print(\"  - Cross-validation and generalization testing\")\n",
    "print(\"  - Domain adaptation and transfer learning\")\n",
    "print(\"  - Rare variant analysis and representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a99df",
   "metadata": {},
   "source": [
    "## 🎉 Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on genomic data augmentation with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end workflow for intelligent genomic data augmentation. Specifically, you have:\n",
    "\n",
    "1. **Understood the \"Why\"**: Gained appreciation for the importance of data augmentation in genomic machine learning and how intelligent augmentation preserves biological patterns while increasing diversity.\n",
    "\n",
    "2. **Mastered the 4-Step Workflow**:\n",
    "   - **Step 1: Setup and Configuration**: You learned how to configure augmentation parameters and understand their impact on sequence generation quality.\n",
    "   - **Step 2: Model Initialization**: You saw how to leverage pre-trained genomic foundation models for context-aware sequence augmentation.\n",
    "   - **Step 3: Sequence Augmentation**: You implemented both single sequence and batch augmentation strategies for different use cases.\n",
    "   - **Step 4: Quality Assessment**: You performed comprehensive analysis to validate the biological validity and diversity of augmented sequences.\n",
    "\n",
    "3. **Advanced Capabilities**: You explored:\n",
    "   - Intelligent masking strategies that preserve important sequence patterns\n",
    "   - Context-aware nucleotide prediction for realistic variations\n",
    "   - Batch processing for efficient dataset expansion\n",
    "   - Quality control metrics for validating augmented sequences\n",
    "   - Statistical analysis of sequence properties and diversity\n",
    "\n",
    "### Next Steps and Applications\n",
    "\n",
    "Your augmented datasets can now be applied to:\n",
    "- **Training Data Enhancement**: Expand small or imbalanced genomic datasets\n",
    "- **Model Robustness**: Improve generalization through increased sequence diversity\n",
    "- **Rare Variant Analysis**: Generate synthetic examples of underrepresented patterns\n",
    "- **Cross-Domain Learning**: Bridge gaps between different genomic contexts\n",
    "- **Validation Studies**: Create test sets for evaluating model robustness\n",
    "\n",
    "### Best Practices for Genomic Augmentation\n",
    "\n",
    "1. **Parameter Tuning**: Start with noise_ratio=0.15-0.25 and adjust based on your specific application\n",
    "2. **Quality Validation**: Always assess biological property conservation in augmented sequences\n",
    "3. **Diversity Balance**: Ensure augmentation increases diversity without breaking biological constraints\n",
    "4. **Domain Specificity**: Consider the specific genomic context (coding, regulatory, etc.) when setting parameters\n",
    "5. **Iterative Refinement**: Use validation metrics to fine-tune augmentation strategies\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "Explore our other tutorials to expand your genomic AI toolkit:\n",
    "- **[mRNA Degradation Prediction](../mRNA_degrad_rate_regression/)**: Apply augmented data to stability prediction\n",
    "- **[RNA Secondary Structure Prediction](../rna_secondary_structure_prediction/)**: Use augmentation for structure modeling\n",
    "- **[Translation Efficiency Prediction](../translation_efficiency_prediction/)**: Enhance training data for efficiency prediction\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and tools to effectively augment genomic datasets for your machine learning research. Intelligent data augmentation is a powerful technique for advancing genomic AI!\n",
    "\n",
    "**Happy augmenting and discovering! 🧬✨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for input and output\n",
    "input_file = \"toy_datasets/test.json\"\n",
    "output_file = \"toy_datasets/augmented_sequences.json\"\n",
    "\n",
    "# Augment sequences from the input file and save to the output file\n",
    "model.augment_from_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0704c93",
   "metadata": {},
   "source": [
    "The input file should be in JSON format, where each line contains a sequence, like this:\n",
    "\n",
    "```json\n",
    "{\"seq\": \"ATCTTGCATTGAAG\"}\n",
    "{\"seq\": \"GGTTTACAGTCCAA\"}\n",
    "```\n",
    "\n",
    "The output will be saved in the same format, with each augmented sequence written in a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d30d2f",
   "metadata": {},
   "source": [
    "### Step 3: **Configurable Parameters**\n",
    "The augmentation process allows you to configure various parameters, such as:\n",
    "- **`noise_ratio`**: Specifies the percentage of tokens that will be masked in the input sequence. The default value is `0.15` (i.e., 15% of tokens will be masked).\n",
    "- **`max_length`**: The maximum token length for the input sequences. The default is `1026`.\n",
    "- **`instance_num`**: The number of augmented instances to generate for each input sequence. The default is `1`, but you can increase this value to create multiple augmented versions of each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16a8f4",
   "metadata": {},
   "source": [
    "### Step 4: **Save Augmented Sequences**\n",
    "The `save_augmented_sequences` method saves the generated augmented sequences to a JSON file. Each line will contain one augmented sequence in the format `{\"aug_seq\": \"<augmented_sequence>\"}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e69a26",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The `OmniModelForAugmentation` class provides a simple and flexible interface for augmenting sequences using a masked language model. By adjusting the noise ratio, instance count, and other hyperparameters, you can create diverse augmented datasets to improve the performance of machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
