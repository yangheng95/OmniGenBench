{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d1de8e",
   "metadata": {},
   "source": [
    "# ðŸ”® Automated Inference with OmniGenBench AutoInfer\n",
    "\n",
    "Welcome to this comprehensive tutorial on **automated inference** using OmniGenBench's AutoInfer functionality. This guide will walk you through making predictions on genomic sequences using pre-trained and fine-tuned models.\n",
    "\n",
    "### 1. The Computational Challenge: Making Predictions at Scale\n",
    "\n",
    "**AutoInfer** streamlines the process of making predictions on genomic sequences by providing:\n",
    "- **Unified Interface**: Single API for all model types and tasks\n",
    "- **Batch Processing**: Efficient handling of thousands of sequences\n",
    "- **Flexible Input**: Support for various file formats (JSON, CSV, TXT)\n",
    "- **Rich Output**: Predictions, probabilities, and confidence scores\n",
    "\n",
    "Applications across genomic analysis:\n",
    "- **Transcription Factor Binding**: Predict TF binding sites in regulatory regions\n",
    "- **Translation Efficiency**: Estimate mRNA translation rates\n",
    "- **Promoter Detection**: Identify promoter regions in genomic sequences\n",
    "- **RNA Structure**: Predict secondary structure elements\n",
    "- **Variant Effect**: Assess functional impact of genetic variants\n",
    "\n",
    "### 2. The Data: From Sequences to Predictions\n",
    "\n",
    "AutoInfer handles various input formats:\n",
    "\n",
    "| Input Type | Format | Use Case |\n",
    "|-----------|--------|----------|\n",
    "| Single sequence | String | Quick predictions |\n",
    "| Multiple sequences | Comma-separated | Small batches |\n",
    "| JSON file | `{\"sequences\": [...]}` | Structured data |\n",
    "| CSV file | `sequence,label,...` | Tabular data |\n",
    "| Text file | One per line | Simple lists |\n",
    "\n",
    "### 3. The Tool: Pre-trained and Fine-tuned Models\n",
    "\n",
    "#### Model Types\n",
    "- **Foundation Models**: General genomic understanding (e.g., OmniGenome-186M)\n",
    "- **Fine-tuned Models**: Task-specific models (e.g., TFB prediction, TE prediction)\n",
    "- **Custom Models**: Your own fine-tuned models\n",
    "\n",
    "All models support:\n",
    "1. **Sequence Classification**: Single or multi-label classification\n",
    "2. **Token Classification**: Per-nucleotide predictions\n",
    "3. **Regression**: Continuous value prediction\n",
    "4. **Embedding Extraction**: Vector representations\n",
    "\n",
    "### 4. The Workflow: 4-Step Guide to Inference\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for AutoInfer\"\n",
    "        A[\"ðŸ“¥ Step 1: Setup and Configuration<br/>Prepare environment and data\"] --> B[\"ðŸ”§ Step 2: Load Model<br/>Initialize pre-trained or fine-tuned model\"]\n",
    "        B --> C[\"ðŸŽ“ Step 3: Run Inference<br/>Make predictions on sequences\"]\n",
    "        C --> D[\"ðŸ”® Step 4: Analysis and Export<br/>Analyze results and save outputs\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start making predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c982b1",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Setup and Configuration\n",
    "\n",
    "First, let's set up our environment and prepare sample data for inference.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "Install required packages for genomic inference."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c586323",
   "metadata": {},
   "source": [
    "!pip install omnigenbench torch transformers -U"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be3158d5",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Import essential libraries for inference and analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "742e7216",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import OmniGenBench components\n",
    "from omnigenbench import (\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForTokenClassification,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[SUCCESS] Libraries imported successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0eeb3f1",
   "metadata": {},
   "source": [
    "### 1.3: Prepare Sample Data\n",
    "\n",
    "Let's create various types of sample data to demonstrate different inference scenarios.\n",
    "\n",
    "#### Data Types:\n",
    "- **Promoter sequences**: For binary classification\n",
    "- **Regulatory elements**: For multi-class classification\n",
    "- **Long genomic regions**: For token-level predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "cedb6017",
   "metadata": {},
   "source": [
    "# Sample genomic sequences for inference\n",
    "sample_sequences = {\n",
    "    \"promoter_candidates\": [\n",
    "        \"ATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",\n",
    "        \"GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC\",\n",
    "        \"TATATATATATATATATATATATATATATATATATATATATATATATAT\",\n",
    "        \"ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC\",\n",
    "        \"CGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGAT\",\n",
    "    ],\n",
    "    \"regulatory_elements\": [\n",
    "        \"CGGCGCGCCATATAAGCATCGAGCGCGCACGTGCGCTGCGCGCGCGCTACGCGCGCATGTGCGCGCACGTACGCGCG\",\n",
    "        \"GCGCGCGCACGTGCGCACGTGCGCGCACGTGCGCGCGCACGTGCGCGCACGTGCGCGCACGTGCGCGCACGTGCGCGC\",\n",
    "        \"GCGCGCCACCAATGCGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCC\",\n",
    "    ],\n",
    "    \"long_sequences\": [\n",
    "        \"ATCGATCGATCG\" * 20,  # 240 bp\n",
    "        \"GCTAGCTAGCTA\" * 20,  # 240 bp\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create JSON format data\n",
    "json_data = {\n",
    "    \"sequences\": sample_sequences[\"promoter_candidates\"]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"inference_sequences.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Create JSON with metadata\n",
    "json_with_metadata = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"sequence\": seq,\n",
    "            \"gene_id\": f\"GENE_{i:03d}\",\n",
    "            \"description\": f\"Sample sequence {i+1}\",\n",
    "            \"organism\": \"Arabidopsis thaliana\"\n",
    "        }\n",
    "        for i, seq in enumerate(sample_sequences[\"promoter_candidates\"])\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"inference_with_metadata.json\", \"w\") as f:\n",
    "    json.dump(json_with_metadata, f, indent=2)\n",
    "\n",
    "# Create CSV format data\n",
    "csv_data = pd.DataFrame({\n",
    "    \"sequence\": sample_sequences[\"promoter_candidates\"],\n",
    "    \"gene_id\": [f\"GENE_{i:03d}\" for i in range(len(sample_sequences[\"promoter_candidates\"]))],\n",
    "    \"organism\": \"Arabidopsis thaliana\"\n",
    "})\n",
    "csv_data.to_csv(\"inference_sequences.csv\", index=False)\n",
    "\n",
    "# Create text file (one sequence per line)\n",
    "with open(\"inference_sequences.txt\", \"w\") as f:\n",
    "    for seq in sample_sequences[\"promoter_candidates\"]:\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(\"[SUCCESS] Sample data files created:\")\n",
    "print(\"  - inference_sequences.json\")\n",
    "print(\"  - inference_with_metadata.json\")\n",
    "print(\"  - inference_sequences.csv\")\n",
    "print(\"  - inference_sequences.txt\")\n",
    "print(f\"\\n[INFO] Total sequences prepared: {len(sample_sequences['promoter_candidates'])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52b43533",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 2: Load Model\n",
    "\n",
    "Now let's load a pre-trained or fine-tuned model for inference.\n",
    "\n",
    "### Model Loading Options\n",
    "\n",
    "We'll demonstrate three approaches:\n",
    "1. **Load from HuggingFace Hub**: Use publicly available fine-tuned models\n",
    "2. **Load from local path**: Use your own trained models\n",
    "3. **Use ModelHub**: Simplified loading with automatic configuration\n",
    "\n",
    "### 2.1: Load Fine-tuned Model (Recommended)\n",
    "\n",
    "Let's use a fine-tuned model for transcription factor binding prediction."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab1fff7c",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "inference_config = {\n",
    "    \"model_name\": \"yangheng/ogb_tfb_finetuned\",  # Fine-tuned TFB model\n",
    "    \"batch_size\": 16,\n",
    "    \"device\": \"cuda\",  # or \"cpu\"\n",
    "}\n",
    "\n",
    "print(\"[INFO] Loading fine-tuned model for inference...\")\n",
    "print(f\"  Model: {inference_config['model_name']}\")\n",
    "print(f\"  Device: {inference_config['device']}\")\n",
    "\n",
    "# Load model using ModelHub (simplified approach)\n",
    "model = ModelHub.load(\n",
    "    inference_config[\"model_name\"],\n",
    "    device=inference_config[\"device\"],\n",
    ")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model = model.eval()\n",
    "\n",
    "print(\"\\n[SUCCESS] Model loaded successfully!\")\n",
    "print(f\"  Model type: {type(model).__name__}\")\n",
    "print(f\"  Ready for inference: {not model.training}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49ea95b2",
   "metadata": {},
   "source": [
    "### 2.2: Alternative - Load from Local Path\n",
    "\n",
    "If you have a locally trained model, you can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "id": "19b07d77",
   "metadata": {},
   "source": [
    "# Example: Load from local directory\n",
    "# Uncomment to use your own model\n",
    "\n",
    "# local_model_path = \"./my_trained_model\"\n",
    "# model = OmniModelForSequenceClassification.from_pretrained(\n",
    "#     local_model_path,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = model.eval()\n",
    "\n",
    "print(\"[INFO] To use a local model, uncomment the code above and specify the path.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "167eb219",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Step 3: Run Inference\n",
    "\n",
    "Now let's make predictions on our sample sequences using various methods.\n",
    "\n",
    "### Inference Methods:\n",
    "1. **Single Sequence**: Quick predictions on individual sequences\n",
    "2. **Batch Inference**: Efficient processing of multiple sequences\n",
    "3. **File-based Inference**: Process sequences from JSON/CSV files\n",
    "\n",
    "### 3.1: Single Sequence Inference"
   ]
  },
  {
   "cell_type": "code",
   "id": "add9e2f5",
   "metadata": {},
   "source": [
    "# Single sequence prediction\n",
    "single_sequence = \"ATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "\n",
    "print(\"[INFO] Running inference on single sequence...\")\n",
    "print(f\"  Sequence: {single_sequence[:40]}...\")\n",
    "print(f\"  Length: {len(single_sequence)} bp\")\n",
    "\n",
    "# Make prediction\n",
    "result = model.inference(single_sequence)\n",
    "\n",
    "print(\"\\n[SUCCESS] Prediction completed!\")\n",
    "print(f\"  Prediction: {result['predictions']}\")\n",
    "\n",
    "if 'probabilities' in result:\n",
    "    probs = result['probabilities']\n",
    "    print(f\"  Probabilities: {probs}\")\n",
    "    print(f\"  Confidence: {max(probs):.4f}\")\n",
    "\n",
    "if 'confidence' in result:\n",
    "    print(f\"  Confidence score: {float(result['confidence']):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ee80b68",
   "metadata": {},
   "source": [
    "### 3.2: Batch Inference\n",
    "\n",
    "For multiple sequences, batch inference is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "id": "76526740",
   "metadata": {},
   "source": [
    "# Batch inference on multiple sequences\n",
    "sequences = sample_sequences[\"promoter_candidates\"]\n",
    "\n",
    "print(f\"[INFO] Running batch inference on {len(sequences)} sequences...\")\n",
    "\n",
    "# Collect all predictions\n",
    "batch_results = []\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    result = model.inference(seq)\n",
    "    batch_results.append({\n",
    "        \"sequence_id\": i,\n",
    "        \"sequence\": seq[:30] + \"...\",  # Truncate for display\n",
    "        \"prediction\": result['predictions'],\n",
    "        \"confidence\": max(result['probabilities']) if 'probabilities' in result else None\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"\\n[SUCCESS] Batch inference completed!\")\n",
    "print(\"\\nResults:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for res in batch_results:\n",
    "    print(f\"Sequence {res['sequence_id']}: {res['sequence']}\")\n",
    "    print(f\"  Prediction: {res['prediction']}\")\n",
    "    if res['confidence']:\n",
    "        print(f\"  Confidence: {res['confidence']:.4f}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a73177a",
   "metadata": {},
   "source": [
    "### 3.3: File-based Inference\n",
    "\n",
    "Process sequences from JSON files with metadata preservation."
   ]
  },
  {
   "cell_type": "code",
   "id": "271dfc60",
   "metadata": {},
   "source": [
    "# Load sequences from JSON file\n",
    "print(\"[INFO] Loading sequences from JSON file...\")\n",
    "\n",
    "with open(\"inference_with_metadata.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"  Loaded {len(data['data'])} sequences with metadata\")\n",
    "\n",
    "# Run inference and preserve metadata\n",
    "results_with_metadata = []\n",
    "\n",
    "for item in data['data']:\n",
    "    sequence = item['sequence']\n",
    "    result = model.inference(sequence)\n",
    "    \n",
    "    # Combine prediction with metadata\n",
    "    results_with_metadata.append({\n",
    "        \"sequence\": sequence,\n",
    "        \"metadata\": {\n",
    "            \"gene_id\": item['gene_id'],\n",
    "            \"description\": item['description'],\n",
    "            \"organism\": item['organism']\n",
    "        },\n",
    "        \"predictions\": result['predictions'],\n",
    "        \"probabilities\": result.get('probabilities', []),\n",
    "    })\n",
    "\n",
    "print(\"\\n[SUCCESS] File-based inference completed!\")\n",
    "print(f\"  Processed {len(results_with_metadata)} sequences\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0647b619",
   "metadata": {},
   "source": [
    "### 3.4: CSV File Inference\n",
    "\n",
    "Process sequences from CSV files and create a results DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f40e520",
   "metadata": {},
   "source": [
    "# Load sequences from CSV\n",
    "print(\"[INFO] Loading sequences from CSV file...\")\n",
    "\n",
    "df = pd.read_csv(\"inference_sequences.csv\")\n",
    "print(f\"  Loaded {len(df)} sequences from CSV\")\n",
    "\n",
    "# Run inference on all sequences\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "for sequence in df['sequence']:\n",
    "    result = model.inference(sequence)\n",
    "    predictions.append(result['predictions'])\n",
    "    if 'probabilities' in result:\n",
    "        confidences.append(max(result['probabilities']))\n",
    "    else:\n",
    "        confidences.append(None)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df['prediction'] = predictions\n",
    "df['confidence'] = confidences\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"inference_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n[SUCCESS] CSV inference completed!\")\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"\\n[INFO] Results saved to: inference_results.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e253da63",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Advanced Usage: CLI Integration\n",
    "\n",
    "The same inference can be performed using the command-line interface.\n",
    "\n",
    "### CLI Examples\n",
    "\n",
    "Here are equivalent CLI commands for the operations we performed above:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccca1528",
   "metadata": {},
   "source": [
    "# Display CLI usage examples\n",
    "cli_examples = \"\"\"\n",
    "Command-Line Interface Examples:\n",
    "================================\n",
    "\n",
    "1. Single sequence inference:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence \"ATCGATCGATCGATCGATCGATCG\" \\\n",
    "     --output-file predictions.json\n",
    "\n",
    "2. Multiple sequences (comma-separated):\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence \"ATCGATCG,GCGCGCGC,TATATAT\" \\\n",
    "     --output-file predictions.json\n",
    "\n",
    "3. Batch inference from JSON:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --input-file inference_sequences.json \\\n",
    "     --batch-size 16 \\\n",
    "     --output-file results.json\n",
    "\n",
    "4. Inference from CSV:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --input-file inference_sequences.csv \\\n",
    "     --output-file predictions.json \\\n",
    "     --device cuda:0\n",
    "\n",
    "5. Inference from text file:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence inference_sequences.txt \\\n",
    "     --output-file predictions.json\n",
    "\"\"\"\n",
    "\n",
    "print(cli_examples)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b40d88b5",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Loading**:\n",
    "   - Use `ModelHub.load()` for simplified loading\n",
    "   - Always set model to evaluation mode: `model.eval()`\n",
    "   - Specify device explicitly for GPU acceleration\n",
    "\n",
    "2. **Inference Methods**:\n",
    "   - Single sequence: `model.inference(sequence)`\n",
    "   - Batch processing: Loop over sequences\n",
    "   - CLI: Use `ogb autoinfer` for production workflows\n",
    "\n",
    "3. **Performance Tips**:\n",
    "   - Use batch processing for multiple sequences\n",
    "   - Enable GPU when available\n",
    "   - Use mixed precision for faster inference\n",
    "   - Process sequences in parallel when possible\n",
    "\n",
    "4. **Output Formats**:\n",
    "   - JSON: Rich metadata and nested structures\n",
    "   - CSV: Tabular data for spreadsheet analysis\n",
    "   - Custom: Adapt to your downstream analysis needs\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "| Task | Model | Application |\n",
    "|------|-------|-------------|\n",
    "| TF Binding | `ogb_tfb_finetuned` | Predict transcription factor binding sites |\n",
    "| Translation Efficiency | `ogb_te_finetuned` | Estimate mRNA translation rates |\n",
    "| Promoter Detection | Custom fine-tuned | Identify promoter regions |\n",
    "| RNA Structure | Structure-specific | Predict secondary structure elements |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune your own models**: See `sequence_classification_tutorial`\n",
    "- **Batch processing**: Scale to thousands of sequences\n",
    "- **Integration**: Incorporate into bioinformatics pipelines\n",
    "- **Advanced analysis**: Combine with embedding extraction and attention visualization\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation**: `docs/GETTING_STARTED.md`\n",
    "- **CLI Reference**: `docs/cli.rst`\n",
    "- **Examples**: `examples/` directory\n",
    "- **Issues**: https://github.com/yangheng95/OmniGenBench/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821c11a",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup\n",
    "\n",
    "Remove temporary files created during the tutorial (optional)."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d84c293",
   "metadata": {},
   "source": [
    "# Uncomment to remove temporary files\n",
    "# import os\n",
    "\n",
    "# files_to_remove = [\n",
    "#     \"inference_sequences.json\",\n",
    "#     \"inference_with_metadata.json\",\n",
    "#     \"inference_sequences.csv\",\n",
    "#     \"inference_sequences.txt\",\n",
    "#     \"inference_results.json\",\n",
    "#     \"inference_results.csv\",\n",
    "#     \"inference_summary.json\",\n",
    "#     \"inference_analysis.png\",\n",
    "#     \"complete_inference_results.json\",\n",
    "# ]\n",
    "\n",
    "# for file in files_to_remove:\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)\n",
    "#         print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"[INFO] To clean up temporary files, uncomment the code above.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
