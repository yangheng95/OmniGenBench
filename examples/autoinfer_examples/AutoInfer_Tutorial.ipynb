{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d1de8e",
   "metadata": {},
   "source": [
    "# üîÆ Automated Inference with OmniGenBench AutoInfer\n",
    "\n",
    "Welcome to this comprehensive tutorial on **automated inference** using OmniGenBench's AutoInfer functionality. This guide will walk you through making predictions on genomic sequences using pre-trained and fine-tuned models.\n",
    "\n",
    "### 1. The Computational Challenge: Making Predictions at Scale\n",
    "\n",
    "**AutoInfer** streamlines the process of making predictions on genomic sequences by providing:\n",
    "- **Unified Interface**: Single API for all model types and tasks\n",
    "- **Batch Processing**: Efficient handling of thousands of sequences\n",
    "- **Flexible Input**: Support for various file formats (JSON, CSV, TXT)\n",
    "- **Rich Output**: Predictions, logits, and confidence scores\n",
    "\n",
    "Applications across genomic analysis:\n",
    "- **Transcription Factor Binding**: Predict TF binding sites in regulatory regions\n",
    "- **Translation Efficiency**: Estimate mRNA translation rates\n",
    "- **Promoter Detection**: Identify promoter regions in genomic sequences\n",
    "- **RNA Structure**: Predict secondary structure elements\n",
    "- **Variant Effect**: Assess functional impact of genetic variants\n",
    "\n",
    "### 2. The Data: From Sequences to Predictions\n",
    "\n",
    "AutoInfer handles various input formats:\n",
    "\n",
    "| Input Type | Format | Use Case |\n",
    "|-----------|--------|----------|\n",
    "| Single sequence | String | Quick predictions |\n",
    "| Multiple sequences | Comma-separated | Small batches |\n",
    "| JSON file | `{\"sequences\": [...]}` | Structured data |\n",
    "| CSV file | `sequence,label,...` | Tabular data |\n",
    "| Text file | One per line | Simple lists |\n",
    "\n",
    "### 3. The Tool: Pre-trained and Fine-tuned Models\n",
    "\n",
    "#### Model Types\n",
    "- **Foundation Models**: General genomic understanding (e.g., OmniGenome-186M)\n",
    "- **Fine-tuned Models**: Task-specific models (e.g., TFB prediction, TE prediction)\n",
    "- **Custom Models**: Your own fine-tuned models\n",
    "\n",
    "All models support:\n",
    "1. **Sequence Classification**: Single or multi-label classification\n",
    "2. **Token Classification**: Per-nucleotide predictions\n",
    "3. **Regression**: Continuous value prediction\n",
    "4. **Embedding Extraction**: Vector representations\n",
    "\n",
    "### 4. The Workflow: 6-Step Guide to Inference\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"6-Step Workflow for AutoInfer\"\n",
    "        A[\"üì• Step 1: Environment Setup<br/>Install dependencies\"] --> B[\"‚öôÔ∏è Step 2: Configuration<br/>Set device, seeds, batch size\"]\n",
    "        B --> C[\"üìä Step 3: Data Preparation<br/>Create/load input sequences\"]\n",
    "        C --> D[\"üîß Step 4: Load Model<br/>Initialize fine-tuned model\"]\n",
    "        D --> E[\"üéì Step 5: Run Inference<br/>Make predictions\"]\n",
    "        E --> F[\"\udcc8 Step 6: Analysis & Visualization<br/>Interpret and export results\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "    style E fill:#ffe0f0,stroke:#333,stroke-width:2px\n",
    "    style F fill:#f0ffe0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start making predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c982b1",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup and Configuration\n",
    "\n",
    "First, let's set up our environment and prepare sample data for inference.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "Install required packages for genomic inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c586323",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omnigenbench torch transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3158d5",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Import essential libraries for inference and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import OmniGenBench components\n",
    "from omnigenbench import (\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForTokenClassification,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[SUCCESS] Libraries imported successfully!\")\n",
    "print(f\"\\n[INFO] Environment Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e1cd5",
   "metadata": {},
   "source": [
    "### 1.3: Set Reproducibility Configuration\n",
    "\n",
    "Ensure reproducible results by fixing random seeds and configuring device management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Auto-detect device with fallback\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Automatically detect the best available device.\n",
    "    Returns 'cuda' if GPU is available, otherwise 'cpu'.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\"[INFO] GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(f\"[INFO] No GPU detected, using CPU\")\n",
    "    return device\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Configuration complete!\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eeb3f1",
   "metadata": {},
   "source": [
    "### 1.4: Prepare Sample Data\n",
    "\n",
    "Let's create various types of sample data to demonstrate different inference scenarios.\n",
    "\n",
    "#### Data Types:\n",
    "- **Promoter sequences**: For binary classification\n",
    "- **Regulatory elements**: For multi-class classification\n",
    "- **Long genomic regions**: For token-level predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample genomic sequences for inference\n",
    "sample_sequences = {\n",
    "    \"promoter_candidates\": [\n",
    "        \"ATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",\n",
    "        \"GCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC\",\n",
    "        \"TATATATATATATATATATATATATATATATATATATATATATATATAT\",\n",
    "        \"ATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGCATGC\",\n",
    "        \"CGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGAT\",\n",
    "    ],\n",
    "    \"regulatory_elements\": [\n",
    "        \"CGGCGCGCCATATAAGCATCGAGCGCGCACGTGCGCTGCGCGCGCGCTACGCGCGCATGTGCGCGCACGTACGCGCG\",\n",
    "        \"GCGCGCGCACGTGCGCACGTGCGCGCACGTGCGCGCGCACGTGCGCGCACGTGCGCGCACGTGCGCGCACGTGCGCGC\",\n",
    "        \"GCGCGCCACCAATGCGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCCACCATGTGCGCGCC\",\n",
    "    ],\n",
    "    \"long_sequences\": [\n",
    "        \"ATCGATCGATCG\" * 20,  # 240 bp\n",
    "        \"GCTAGCTAGCTA\" * 20,  # 240 bp\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create JSON format data\n",
    "json_data = {\n",
    "    \"sequences\": sample_sequences[\"promoter_candidates\"]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"inference_sequences.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Create JSON with metadata\n",
    "json_with_metadata = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"sequence\": seq,\n",
    "            \"gene_id\": f\"GENE_{i:03d}\",\n",
    "            \"description\": f\"Sample sequence {i+1}\",\n",
    "            \"organism\": \"Arabidopsis thaliana\"\n",
    "        }\n",
    "        for i, seq in enumerate(sample_sequences[\"promoter_candidates\"])\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"inference_with_metadata.json\", \"w\") as f:\n",
    "    json.dump(json_with_metadata, f, indent=2)\n",
    "\n",
    "# Create CSV format data\n",
    "csv_data = pd.DataFrame({\n",
    "    \"sequence\": sample_sequences[\"promoter_candidates\"],\n",
    "    \"gene_id\": [f\"GENE_{i:03d}\" for i in range(len(sample_sequences[\"promoter_candidates\"]))],\n",
    "    \"organism\": \"Arabidopsis thaliana\"\n",
    "})\n",
    "csv_data.to_csv(\"inference_sequences.csv\", index=False)\n",
    "\n",
    "# Create text file (one sequence per line)\n",
    "with open(\"inference_sequences.txt\", \"w\") as f:\n",
    "    for seq in sample_sequences[\"promoter_candidates\"]:\n",
    "        f.write(seq + \"\\n\")\n",
    "\n",
    "print(\"[SUCCESS] Sample data files created:\")\n",
    "print(\"  - inference_sequences.json\")\n",
    "print(\"  - inference_with_metadata.json\")\n",
    "print(\"  - inference_sequences.csv\")\n",
    "print(\"  - inference_sequences.txt\")\n",
    "print(f\"\\n[INFO] Total sequences prepared: {len(sample_sequences['promoter_candidates'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b43533",
   "metadata": {},
   "source": [
    "## üîß Step 2: Load Model\n",
    "\n",
    "Now let's load a pre-trained or fine-tuned model for inference.\n",
    "\n",
    "### Model Loading Options\n",
    "\n",
    "We'll demonstrate three approaches:\n",
    "1. **Load from HuggingFace Hub**: Use publicly available fine-tuned models\n",
    "2. **Load from local path**: Use your own trained models\n",
    "3. **Use ModelHub**: Simplified loading with automatic configuration\n",
    "\n",
    "### 2.1: Load Fine-tuned Model (Recommended)\n",
    "\n",
    "Let's use a fine-tuned model for transcription factor binding prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "inference_config = {\n",
    "    \"model_name\": \"yangheng/ogb_tfb_finetuned\",  # Fine-tuned TFB model\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,  # Use auto-detected device\n",
    "}\n",
    "\n",
    "print(\"[INFO] Loading fine-tuned model for inference...\")\n",
    "print(f\"  Model: {inference_config['model_name']}\")\n",
    "print(f\"  Device: {inference_config['device']}\")\n",
    "\n",
    "try:\n",
    "    # Load model using ModelHub (simplified approach)\n",
    "    model = ModelHub.load(\n",
    "        inference_config[\"model_name\"],\n",
    "        device=inference_config[\"device\"],\n",
    "    )\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model = model.eval()\n",
    "    \n",
    "    print(\"\\n[SUCCESS] Model loaded successfully!\")\n",
    "    print(f\"  Model type: {type(model).__name__}\")\n",
    "    print(f\"  Evaluation mode: {not model.training}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to load model: {e}\")\n",
    "    print(f\"[INFO] Please check:\")\n",
    "    print(f\"  1. Internet connection (for HuggingFace Hub download)\")\n",
    "    print(f\"  2. Model name is correct: {inference_config['model_name']}\")\n",
    "    print(f\"  3. Sufficient disk space for model caching\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea95b2",
   "metadata": {},
   "source": [
    "### 2.2: Alternative - Load from Local Path\n",
    "\n",
    "If you have a locally trained model, you can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b07d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load from local directory\n",
    "# Uncomment to use your own model\n",
    "\n",
    "# local_model_path = \"./my_trained_model\"\n",
    "# model = OmniModelForSequenceClassification.from_pretrained(\n",
    "#     local_model_path,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = model.eval()\n",
    "\n",
    "print(\"[INFO] To use a local model, uncomment the code above and specify the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eb219",
   "metadata": {},
   "source": [
    "## üéì Step 3: Run Inference\n",
    "\n",
    "Now let's make predictions on our sample sequences using various methods.\n",
    "\n",
    "### Inference Methods:\n",
    "1. **Single Sequence**: Quick predictions on individual sequences\n",
    "2. **Batch Inference**: Efficient processing of multiple sequences\n",
    "3. **File-based Inference**: Process sequences from JSON/CSV files\n",
    "\n",
    "### 3.1: Single Sequence Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sequence prediction\n",
    "single_sequence = \"ATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "\n",
    "print(\"[INFO] Running inference on single sequence...\")\n",
    "print(f\"  Sequence: {single_sequence[:40]}...\")\n",
    "print(f\"  Length: {len(single_sequence)} bp\")\n",
    "\n",
    "try:\n",
    "    # Make prediction\n",
    "    result = model.inference(single_sequence)\n",
    "    \n",
    "    print(\"\\n[SUCCESS] Prediction completed!\")\n",
    "    print(f\"\\n[INFO] Result structure:\")\n",
    "    print(f\"  Available keys: {list(result.keys())}\")\n",
    "    \n",
    "    # Extract prediction (string label for classification)\n",
    "    prediction = result['predictions']\n",
    "    print(f\"\\n  Prediction: {prediction}\")\n",
    "    \n",
    "    # Extract confidence (tensor scalar)\n",
    "    if 'confidence' in result:\n",
    "        confidence = float(result['confidence'])  # Convert tensor to Python float\n",
    "        print(f\"  Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Extract logits if available\n",
    "    if 'logits' in result:\n",
    "        logits = result['logits']\n",
    "        print(f\"  Logits shape: {logits.shape}\")\n",
    "        print(f\"  Logits values: {logits.detach().cpu().numpy()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Inference failed: {e}\")\n",
    "    print(f\"[INFO] Troubleshooting:\")\n",
    "    print(f\"  - Check sequence format (only A, T, C, G allowed)\")\n",
    "    print(f\"  - Verify model is in eval mode\")\n",
    "    print(f\"  - Check device memory availability\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee80b68",
   "metadata": {},
   "source": [
    "### 3.2: Batch Inference\n",
    "\n",
    "For multiple sequences, batch inference is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76526740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference on multiple sequences\n",
    "sequences = sample_sequences[\"promoter_candidates\"]\n",
    "\n",
    "print(f\"[INFO] Running batch inference on {len(sequences)} sequences...\")\n",
    "\n",
    "# Collect all predictions\n",
    "batch_results = []\n",
    "\n",
    "try:\n",
    "    for i, seq in enumerate(sequences):\n",
    "        result = model.inference(seq)\n",
    "        \n",
    "        # Extract relevant information\n",
    "        prediction = result['predictions']\n",
    "        confidence = float(result['confidence']) if 'confidence' in result else None\n",
    "        \n",
    "        batch_results.append({\n",
    "            \"sequence_id\": i,\n",
    "            \"sequence\": seq[:30] + \"...\",  # Truncate for display\n",
    "            \"prediction\": prediction,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n[SUCCESS] Batch inference completed!\")\n",
    "    print(f\"\\nResults ({len(batch_results)} sequences):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for res in batch_results:\n",
    "        print(f\"\\nSequence {res['sequence_id']}: {res['sequence']}\")\n",
    "        print(f\"  Prediction: {res['prediction']}\")\n",
    "        if res['confidence'] is not None:\n",
    "            print(f\"  Confidence: {res['confidence']:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    if all(r['confidence'] is not None for r in batch_results):\n",
    "        confidences = [r['confidence'] for r in batch_results]\n",
    "        print(f\"\\n[INFO] Confidence Statistics:\")\n",
    "        print(f\"  Mean: {np.mean(confidences):.4f}\")\n",
    "        print(f\"  Std:  {np.std(confidences):.4f}\")\n",
    "        print(f\"  Min:  {np.min(confidences):.4f}\")\n",
    "        print(f\"  Max:  {np.max(confidences):.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Batch inference failed: {e}\")\n",
    "    print(f\"[INFO] Processing stopped at sequence {i}/{len(sequences)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73177a",
   "metadata": {},
   "source": [
    "### 3.3: File-based Inference\n",
    "\n",
    "Process sequences from JSON files with metadata preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences from JSON file\n",
    "print(\"[INFO] Loading sequences from JSON file...\")\n",
    "\n",
    "try:\n",
    "    with open(\"inference_with_metadata.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"  Loaded {len(data['data'])} sequences with metadata\")\n",
    "    \n",
    "    # Run inference and preserve metadata\n",
    "    results_with_metadata = []\n",
    "    \n",
    "    for idx, item in enumerate(data['data']):\n",
    "        sequence = item['sequence']\n",
    "        result = model.inference(sequence)\n",
    "        \n",
    "        # Extract prediction and confidence\n",
    "        prediction = result['predictions']\n",
    "        confidence = float(result['confidence']) if 'confidence' in result else None\n",
    "        \n",
    "        # Convert logits to list for JSON serialization\n",
    "        logits_list = None\n",
    "        if 'logits' in result:\n",
    "            logits_list = result['logits'].detach().cpu().tolist()\n",
    "        \n",
    "        # Combine prediction with metadata\n",
    "        results_with_metadata.append({\n",
    "            \"sequence\": sequence,\n",
    "            \"metadata\": {\n",
    "                \"gene_id\": item['gene_id'],\n",
    "                \"description\": item['description'],\n",
    "                \"organism\": item['organism']\n",
    "            },\n",
    "            \"predictions\": prediction,\n",
    "            \"confidence\": confidence,\n",
    "            \"logits\": logits_list,\n",
    "        })\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 2 == 0 or (idx + 1) == len(data['data']):\n",
    "            print(f\"  Progress: {idx + 1}/{len(data['data'])} sequences processed\")\n",
    "    \n",
    "    print(\"\\n[SUCCESS] File-based inference completed!\")\n",
    "    print(f\"  Processed {len(results_with_metadata)} sequences\")\n",
    "    \n",
    "    # Display sample result\n",
    "    if results_with_metadata:\n",
    "        print(f\"\\n[INFO] Sample result (first sequence):\")\n",
    "        sample = results_with_metadata[0]\n",
    "        print(f\"  Gene ID: {sample['metadata']['gene_id']}\")\n",
    "        print(f\"  Prediction: {sample['predictions']}\")\n",
    "        print(f\"  Confidence: {sample['confidence']:.4f if sample['confidence'] else 'N/A'}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[ERROR] File 'inference_with_metadata.json' not found!\")\n",
    "    print(\"[INFO] Please run the data preparation cell (1.4) first\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] File-based inference failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647b619",
   "metadata": {},
   "source": [
    "### 3.4: CSV File Inference\n",
    "\n",
    "Process sequences from CSV files and create a results DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences from CSV\n",
    "print(\"[INFO] Loading sequences from CSV file...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"inference_sequences.csv\")\n",
    "    print(f\"  Loaded {len(df)} sequences from CSV\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Run inference on all sequences\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for idx, sequence in enumerate(df['sequence']):\n",
    "        result = model.inference(sequence)\n",
    "        \n",
    "        # Extract prediction and confidence\n",
    "        prediction = result['predictions']\n",
    "        confidence = float(result['confidence']) if 'confidence' in result else None\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        confidences.append(confidence)\n",
    "        \n",
    "        # Progress indicator for larger datasets\n",
    "        if (idx + 1) % 2 == 0 or (idx + 1) == len(df):\n",
    "            print(f\"  Progress: {idx + 1}/{len(df)} sequences processed\")\n",
    "    \n",
    "    # Add predictions to DataFrame\n",
    "    df['prediction'] = predictions\n",
    "    df['confidence'] = confidences\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"inference_results.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n[SUCCESS] CSV inference completed!\")\n",
    "    print(f\"\\n[INFO] Results DataFrame:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Display statistics\n",
    "    if df['confidence'].notna().any():\n",
    "        print(f\"\\n[INFO] Prediction Statistics:\")\n",
    "        print(f\"  Total sequences: {len(df)}\")\n",
    "        print(f\"  Mean confidence: {df['confidence'].mean():.4f}\")\n",
    "        print(f\"  Prediction distribution:\")\n",
    "        print(df['prediction'].value_counts().to_string(header=False))\n",
    "    \n",
    "    print(f\"\\n[INFO] Results saved to: {output_file}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[ERROR] File 'inference_sequences.csv' not found!\")\n",
    "    print(\"[INFO] Please run the data preparation cell (1.4) first\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "    print(f\"\\n[ERROR] Required column missing: {e}\")\n",
    "    print(\"[INFO] CSV file must contain a 'sequence' column\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] CSV inference failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Confidence Distribution\n",
    "if 'confidence' in df.columns and df['confidence'].notna().any():\n",
    "    ax1 = axes[0]\n",
    "    df['confidence'].hist(bins=20, ax=ax1, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Confidence Score', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Distribution of Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "    ax1.axvline(df['confidence'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df['confidence'].mean():.3f}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Prediction Distribution\n",
    "ax2 = axes[1]\n",
    "prediction_counts = df['prediction'].value_counts()\n",
    "prediction_counts.plot(kind='bar', ax=ax2, color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('Prediction Class', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Distribution of Predictions', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"[SUCCESS] Visualization saved to 'inference_analysis.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n[INFO] Inference Summary:\")\n",
    "print(f\"{'Metric':<25} {'Value':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total sequences':<25} {len(df):<15}\")\n",
    "print(f\"{'Unique predictions':<25} {df['prediction'].nunique():<15}\")\n",
    "if 'confidence' in df.columns:\n",
    "    print(f\"{'Mean confidence':<25} {df['confidence'].mean():<15.4f}\")\n",
    "    print(f\"{'Std confidence':<25} {df['confidence'].std():<15.4f}\")\n",
    "    print(f\"{'High confidence (>0.9)':<25} {(df['confidence'] > 0.9).sum():<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f20d8",
   "metadata": {},
   "source": [
    "## üìä Step 4: Visualization and Analysis\n",
    "\n",
    "Let's visualize the inference results to better understand model predictions and confidence distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253da63",
   "metadata": {},
   "source": [
    "## üéØ Step 5: CLI Integration\n",
    "\n",
    "The same inference can be performed using the command-line interface.\n",
    "\n",
    "### CLI Examples\n",
    "\n",
    "Here are equivalent CLI commands for the operations we performed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CLI usage examples\n",
    "cli_examples = \"\"\"\n",
    "Command-Line Interface Examples:\n",
    "================================\n",
    "\n",
    "1. Single sequence inference:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence \"ATCGATCGATCGATCGATCGATCG\" \\\n",
    "     --output-file predictions.json\n",
    "\n",
    "2. Multiple sequences (comma-separated):\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence \"ATCGATCG,GCGCGCGC,TATATAT\" \\\n",
    "     --output-file predictions.json\n",
    "\n",
    "3. Batch inference from JSON:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --input-file inference_sequences.json \\\n",
    "     --batch-size 16 \\\n",
    "     --output-file results.json\n",
    "\n",
    "4. Inference from CSV:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --input-file inference_sequences.csv \\\n",
    "     --output-file predictions.json \\\n",
    "     --device cuda:0\n",
    "\n",
    "5. Inference from text file:\n",
    "   ogb autoinfer \\\n",
    "     --model yangheng/ogb_tfb_finetuned \\\n",
    "     --sequence inference_sequences.txt \\\n",
    "     --output-file predictions.json\n",
    "\"\"\"\n",
    "\n",
    "print(cli_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d88b5",
   "metadata": {},
   "source": [
    "## üéì Step 6: Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Loading**:\n",
    "   - Use `ModelHub.load()` for simplified loading\n",
    "   - Always set model to evaluation mode: `model.eval()`\n",
    "   - Use auto-detected device for cross-platform compatibility\n",
    "   - Wrap loading in try-except for error handling\n",
    "\n",
    "2. **Inference API Return Format** (CRITICAL):\n",
    "   ```python\n",
    "   result = model.inference(sequence)\n",
    "   # Returns dict with keys:\n",
    "   # - 'predictions': str (single) or list (batch) of class labels\n",
    "   # - 'logits': Tensor[num_labels] - raw model outputs\n",
    "   # - 'confidence': Tensor(scalar) - max probability (use float() to convert)\n",
    "   # - 'last_hidden_state': Tensor - for downstream analysis\n",
    "   \n",
    "   # NO 'probabilities' key! Use logits or confidence instead\n",
    "   ```\n",
    "\n",
    "3. **Device Management**:\n",
    "   - Auto-detect device: `torch.cuda.is_available()`\n",
    "   - Provide CPU fallback for portability\n",
    "   - Print device information for transparency\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Wrap critical operations in try-except blocks\n",
    "   - Provide actionable error messages\n",
    "   - Validate inputs before processing\n",
    "\n",
    "5. **Reproducibility**:\n",
    "   - Set random seeds: `torch.manual_seed(SEED)`\n",
    "   - Fix CUDA determinism: `torch.backends.cudnn.deterministic = True`\n",
    "   - Document environment: PyTorch version, CUDA version\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "| Task | Model | Application |\n",
    "|------|-------|-------------|\n",
    "| TF Binding | `ogb_tfb_finetuned` | Predict transcription factor binding sites |\n",
    "| Translation Efficiency | `ogb_te_finetuned` | Estimate mRNA translation rates |\n",
    "| Promoter Detection | Custom fine-tuned | Identify promoter regions |\n",
    "| RNA Structure | Structure-specific | Predict secondary structure elements |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Batch Processing**: Process sequences in batches for efficiency\n",
    "- **GPU Acceleration**: Use CUDA when available (10-50x speedup)\n",
    "- **Memory Management**: Clear cache between large batches\n",
    "  ```python\n",
    "  if torch.cuda.is_available():\n",
    "      torch.cuda.empty_cache()\n",
    "  ```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune your own models**: See `sequence_classification_tutorial.ipynb`\n",
    "- **Batch processing at scale**: Process thousands of sequences efficiently\n",
    "- **Pipeline integration**: Incorporate into bioinformatics workflows\n",
    "- **Advanced analysis**: Combine with embedding extraction and attention visualization\n",
    "\n",
    "### Common Pitfalls & Solutions\n",
    "\n",
    "| Problem | Cause | Solution |\n",
    "|---------|-------|----------|\n",
    "| `KeyError: 'probabilities'` | Using incorrect API key | Use `'confidence'` or `'logits'` instead |\n",
    "| `RuntimeError: CUDA out of memory` | Batch too large | Reduce batch size or use CPU |\n",
    "| `TypeError: can't convert cuda:0 device type tensor to numpy` | Tensor on GPU | Use `.cpu()` before `.numpy()` |\n",
    "| Results vary between runs | No random seed | Set `torch.manual_seed(SEED)` |\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation**: `docs/GETTING_STARTED.md`\n",
    "- **CLI Reference**: `docs/cli.rst`\n",
    "- **Examples**: `examples/` directory\n",
    "- **API Reference**: https://omnigenbench.readthedocs.io\n",
    "- **Issues**: https://github.com/yangheng95/OmniGenBench/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821c11a",
   "metadata": {},
   "source": [
    "## üßπ Cleanup\n",
    "\n",
    "Remove temporary files created during the tutorial (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to remove temporary files\n",
    "# import os\n",
    "\n",
    "# files_to_remove = [\n",
    "#     \"inference_sequences.json\",\n",
    "#     \"inference_with_metadata.json\",\n",
    "#     \"inference_sequences.csv\",\n",
    "#     \"inference_sequences.txt\",\n",
    "#     \"inference_results.json\",\n",
    "#     \"inference_results.csv\",\n",
    "#     \"inference_summary.json\",\n",
    "#     \"inference_analysis.png\",\n",
    "#     \"complete_inference_results.json\",\n",
    "# ]\n",
    "\n",
    "# for file in files_to_remove:\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)\n",
    "#         print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"[INFO] To clean up temporary files, uncomment the code above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
