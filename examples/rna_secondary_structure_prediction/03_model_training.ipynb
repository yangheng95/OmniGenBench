{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_section",
   "metadata": {},
   "source": [
    "# ðŸ§¬ RNA SSP Tutorial 3/4: Model Training - Teaching the Model to Predict\n",
    "\n",
    "Welcome to the third tutorial in our series. So far, we have:\n",
    "1. Prepared our dataset ([01_data_preparation.ipynb](01_data_preparation.ipynb))\n",
    "2. Initialized our model ([02_model_initialization.ipynb](02_model_initialization.ipynb))\n",
    "\n",
    "Now we have all the ingredients ready. In this tutorial, we will combine them to **train** the model. Training is the process of teaching the model to make accurate predictions by showing it examples from our dataset and adjusting its internal parameters.\n",
    "\n",
    "This tutorial will cover:\n",
    "1. **The Concept of Supervised Training**: What it means to train a model\n",
    "2. **Trainers in OmniGenBench**: Different training engines available\n",
    "3. **The Training Process**: Step-by-step configuration and execution\n",
    "4. **Evaluation and Results**: Measuring performance\n",
    "\n",
    "By the end of this tutorial, you will have a fine-tuned model saved on your disk, ready for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_concept",
   "metadata": {},
   "source": [
    "### 1. The Concept of Supervised Training\n",
    "\n",
    "At its heart, **supervised training** is like teaching a student with a textbook and an answer key. We show the model an input (an RNA sequence), let it make a prediction, and then compare its prediction to the correct answer (the structure labels). The difference is quantified by a **loss function**. The goal is to minimize this loss.\n",
    "\n",
    "This process is iterative and happens in a **training loop**, which consists of four main steps:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"1. Forward Pass<br/>(Make Prediction)\"] --> B[\"2. Loss Calculation<br/>(Measure Error)\"]\n",
    "    B --> C[\"3. Backward Pass<br/>(Compute Gradients)\"]\n",
    "    C --> D[\"4. Optimizer Step<br/>(Update Weights)\"]\n",
    "    D --> A\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#e8f5e8\n",
    "    style D fill:#fff3e0\n",
    "```\n",
    "\n",
    "To manage this entire process, we need:\n",
    "- A **Model** to be trained\n",
    "- **DataLoaders** to supply batches of data\n",
    "- A **Loss Function** to measure error (Cross-Entropy for classification)\n",
    "- An **Optimizer** to update weights (typically AdamW)\n",
    "- **Evaluation Metrics** (F1-score, Accuracy) to assess performance\n",
    "\n",
    "A **Trainer** encapsulates this entire training loop, providing a clean interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_section",
   "metadata": {},
   "source": [
    "### 2. Trainers in OmniGenBench\n",
    "\n",
    "`OmniGenBench` provides flexible training options:\n",
    "\n",
    "| Trainer Class | Key Feature | When to Use |\n",
    "| --- | --- | --- |\n",
    "| `Trainer` | Native PyTorch | Simple, single-GPU training |\n",
    "| **`AccelerateTrainer`** | **Distributed via `accelerate`** | **Recommended - Scales from single to multi-GPU** |\n",
    "| `HFTrainer` | Hugging Face integration | If using HF ecosystem heavily |\n",
    "\n",
    "For this tutorial, we use **`AccelerateTrainer`** - it's powerful, scalable, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation_section",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ The Training Process in Action\n",
    "\n",
    "Let's put everything together and train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install omnigenbench -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForTokenClassification,\n",
    "    OmniDatasetForTokenClassification,\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸŽ¯ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "### Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and dataset configuration\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"RNA-SSP-Archive2\"\n",
    "\n",
    "# Label mapping\n",
    "label2id = {\"(\": 0, \")\": 1, \".\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Training parameters\n",
    "max_length = 512\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "max_examples = 1000  # For quick testing; set to None for full dataset\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"ogb_rna_structure_finetuned\"\n",
    "\n",
    "print(\"âœ… Configuration complete!\")\n",
    "print(f\"ðŸ“Š Model: {model_name_or_path}\")\n",
    "print(f\"ðŸ“Š Dataset: {dataset_name}\")\n",
    "print(f\"ðŸ“Š Batch size: {batch_size}\")\n",
    "print(f\"ðŸ“Š Epochs: {num_epochs}\")\n",
    "print(f\"ðŸ“Š Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_section",
   "metadata": {},
   "source": [
    "### Step 3: Load Data\n",
    "\n",
    "We'll reuse the data loading code from Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "print(f\"âœ… Tokenizer loaded: {model_name_or_path}\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"ðŸ—ï¸ Loading datasets...\")\n",
    "datasets = OmniDatasetForTokenClassification.from_hub(\n",
    "    dataset_name_or_path=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    label2id=label2id,\n",
    "    max_examples=max_examples,\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Datasets loaded: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "### Step 4: Initialize Model\n",
    "\n",
    "We'll reuse the model initialization from Tutorial 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = OmniModelForTokenClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model initialized: {model_name_or_path}\")\n",
    "print(f\"ðŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_section",
   "metadata": {},
   "source": [
    "### Step 5: Define Evaluation Metrics\n",
    "\n",
    "We'll use F1-score and Accuracy to evaluate our model's performance. The `ignore_y=-100` parameter tells the metric to ignore special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "metric_functions = [\n",
    "    ClassificationMetric(ignore_y=-100).f1_score,\n",
    "    ClassificationMetric(ignore_y=-100).accuracy_score,\n",
    "]\n",
    "\n",
    "print(\"âœ… Evaluation metrics configured:\")\n",
    "print(\"  - F1-score (macro-averaged)\")\n",
    "print(\"  - Accuracy\")\n",
    "print(\"  - Special tokens (label=-100) will be ignored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer_section",
   "metadata": {},
   "source": [
    "### Step 6: Configure and Run Trainer\n",
    "\n",
    "Now we configure the `AccelerateTrainer` with all our components and launch training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    "    # Training parameters\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    # Optimization settings\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    # Logging and checkpointing\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured!\")\n",
    "print(f\"ðŸ“Š Training will run for {num_epochs} epochs\")\n",
    "print(f\"ðŸ“Š Model checkpoints will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_execution",
   "metadata": {},
   "source": [
    "### Step 7: Start Training\n",
    "\n",
    "This is where the magic happens! The trainer will:\n",
    "1. Run the training loop for the specified number of epochs\n",
    "2. Periodically evaluate on the validation set\n",
    "3. Save checkpoints\n",
    "4. Log metrics\n",
    "\n",
    "**Note**: This may take several minutes to hours depending on your hardware and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ“ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "metrics = trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_section",
   "metadata": {},
   "source": [
    "### Step 8: Review Training Results\n",
    "\n",
    "Let's examine the final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Final Training Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "### Step 9: Save the Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to: {output_dir}\")\n",
    "print(\"\\nðŸ“¦ Saved files:\")\n",
    "import os\n",
    "if os.path.exists(output_dir):\n",
    "    for file in os.listdir(output_dir):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_section",
   "metadata": {},
   "source": [
    "### Step 10: Test Set Evaluation (Optional)\n",
    "\n",
    "Evaluate the trained model on the test set to get final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"test\" in datasets:\n",
    "    print(\"ðŸ§ª Evaluating on test set...\")\n",
    "    test_metrics = trainer.evaluate(datasets[\"test\"])\n",
    "    \n",
    "    print(\"\\nðŸ“Š Test Set Performance:\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in test_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ No test set available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding_section",
   "metadata": {},
   "source": [
    "## ðŸ“Š Understanding the Metrics\n",
    "\n",
    "### F1-Score\n",
    "- Harmonic mean of precision and recall\n",
    "- Range: 0.0 (worst) to 1.0 (best)\n",
    "- Good for imbalanced classes (more `)` and `(` than `.`)\n",
    "\n",
    "### Accuracy\n",
    "- Percentage of correctly predicted tokens\n",
    "- Range: 0.0 (worst) to 1.0 (best)\n",
    "- Easy to interpret but can be misleading with imbalanced data\n",
    "\n",
    "### What are Good Scores?\n",
    "For RNA secondary structure prediction:\n",
    "- **F1 > 0.80**: Excellent\n",
    "- **F1 > 0.70**: Good\n",
    "- **F1 > 0.60**: Acceptable\n",
    "- **F1 < 0.60**: Needs improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips_section",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Training Tips and Tricks\n",
    "\n",
    "### Improving Performance\n",
    "\n",
    "1. **More Data**: Use full dataset (set `max_examples=None`)\n",
    "2. **More Epochs**: Train for 5-10 epochs instead of 3\n",
    "3. **Larger Model**: Try OmniGenome-186M or 418M\n",
    "4. **Learning Rate**: Experiment with 1e-5 to 5e-5\n",
    "5. **Batch Size**: Increase if you have GPU memory\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **Out of Memory** | Reduce batch_size or max_length |\n",
    "| **Training Too Slow** | Increase batch_size or use larger GPU |\n",
    "| **Overfitting** | Reduce num_epochs or add dropout |\n",
    "| **Underfitting** | Train longer or use larger model |\n",
    "| **Unstable Loss** | Reduce learning_rate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary and Next Steps\n",
    "\n",
    "In this tutorial, we:\n",
    "1. âœ… Understood the concept of supervised training\n",
    "2. âœ… Explored OmniGenBench trainer options\n",
    "3. âœ… Configured AccelerateTrainer with all components\n",
    "4. âœ… Trained the model on RNA structure data\n",
    "5. âœ… Evaluated performance with F1-score and accuracy\n",
    "6. âœ… Saved the trained model for future use\n",
    "\n",
    "### What We've Accomplished\n",
    "```python\n",
    "# Complete training in ~15 lines!\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    compute_metrics=metric_functions,\n",
    ")\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "- **AccelerateTrainer** handles the entire training loop\n",
    "- **Metrics** help us understand model performance\n",
    "- **Checkpointing** allows us to resume training\n",
    "- **Evaluation** on validation set prevents overfitting\n",
    "\n",
    "### Next: Model Inference\n",
    "Now that we have a trained model, proceed to **[04_model_inference.ipynb](04_model_inference.ipynb)** to:\n",
    "- Load the trained model\n",
    "- Make predictions on new RNA sequences\n",
    "- Validate predicted structures\n",
    "- Interpret and visualize results\n",
    "\n",
    "Time to put our model to work! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}