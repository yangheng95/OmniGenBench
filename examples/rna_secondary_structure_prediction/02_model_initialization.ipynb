{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_section",
   "metadata": {},
   "source": [
    "# üß¨ RNA SSP Tutorial 2/4: Model Initialization - From Task to Architecture\n",
    "\n",
    "In the previous tutorial, [01_data_preparation.ipynb](01_data_preparation.ipynb), we defined our biological task‚Äîpredicting RNA secondary structure‚Äîand prepared our data accordingly. We framed it as a **token classification** problem where each nucleotide receives a structural label.\n",
    "\n",
    "This crucial step of defining the task and data format dictates our next decision: **choosing the right model architecture**.\n",
    "\n",
    "> üìö **Learning Objectives**: Master model initialization patterns, understand foundation model concepts, and leverage OmniGenBench's intelligent defaults\n",
    "\n",
    "---\n",
    "\n",
    "## The Power of Pre-trained Models üöÄ\n",
    "\n",
    "The core idea behind fine-tuning is to leverage **pre-trained foundation models**. These models have already learned the fundamental \"language\" of genomes from vast amounts of unlabeled sequence data. This pre-training endows them with powerful, general-purpose understanding of genomic patterns.\n",
    "\n",
    "Our task is to take this general knowledge and specialize it for our specific problem: RNA secondary structure prediction.\n",
    "\n",
    "### üéØ Why Use Foundation Models?\n",
    "\n",
    "| Traditional Methods | Foundation Model Approach |\n",
    "|---------|------------|\n",
    "| üîß Requires hand-crafted features | ü§ñ Automatically learns representations |\n",
    "| üìö Relies on prior biological knowledge | üî¨ Discovers patterns from data |\n",
    "| üéØ Limited generalization ability | üåê Strong cross-task generalization |\n",
    "| üìä Needs large amounts of task-specific data | üí° Can fine-tune with small datasets |\n",
    "\n",
    "## Key Components: Model and Tokenizer\n",
    "\n",
    "This tutorial will guide you through selecting and initializing OmniGenome for RNA structure prediction. We will cover:\n",
    "\n",
    "1. **The OmniGenBench Model Zoo**: Available model architectures\n",
    "2. **The Principle of Model Selection**: Matching models to tasks\n",
    "3. **Model Architecture**: Understanding the \"base + task head\" design\n",
    "4. **Inputs and Outputs**: What the model expects and produces\n",
    "5. **Practical Implementation**: Initializing the model for our task\n",
    "\n",
    "By the end of this tutorial, you will understand how to configure OmniGenome for token-level predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_zoo_section",
   "metadata": {},
   "source": [
    "### 1. The OmniGenBench Model Zoo\n",
    "\n",
    "`OmniGenBench` provides a comprehensive framework with various model architectures. These are often referred to as \"task heads.\" When you use a pre-trained model, you combine a powerful **base model** with a smaller, task-specific **head**.\n",
    "\n",
    "Here is a summary of the main model classes available:\n",
    "\n",
    "| Model Class | Task Type | RNA SSP Relevance |\n",
    "| --- | --- | --- |\n",
    "| `OmniModelForSequenceClassification` | Sequence Classification | Classifying entire RNA molecules |\n",
    "| `OmniModelForMultiLabelSequenceClassification` | Multi-Label Classification | Multiple properties per sequence |\n",
    "| **`OmniModelForTokenClassification`** | **Token Classification** | **Per-nucleotide structure prediction (our task)** |\n",
    "| `OmniModelForSequenceRegression` | Sequence Regression | Predicting continuous values |\n",
    "| `OmniModelForTokenRegression` | Token Regression | Per-position continuous predictions |\n",
    "| `OmniModelForSeq2Seq` | Sequence-to-Sequence | Structure generation |\n",
    "\n",
    "**OmniGenome** models (52M, 186M, 418M parameters) are particularly powerful for genomic tasks because they were pre-trained on diverse DNA/RNA sequences, making them excel at pattern recognition across different sequence contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection_principle",
   "metadata": {},
   "source": [
    "### 2. The Principle of Model Selection\n",
    "\n",
    "The selection principle is straightforward: **match the model architecture to the machine learning task you defined**.\n",
    "\n",
    "In our case:\n",
    "- **Biological Problem**: Predicting if each nucleotide in an RNA sequence is paired or unpaired, and if paired, which side\n",
    "- **Data Format**: RNA sequence of variable length\n",
    "- **Label Format**: A label for each position: `(`, `)`, or `.`\n",
    "- **ML Task**: Since each position needs a label, this is **Token Classification**\n",
    "- **Model Choice**: `OmniModelForTokenClassification` with **OmniGenome** base model\n",
    "\n",
    "**Why OmniGenome for this task?**\n",
    "- Pre-trained on genomic sequences (DNA/RNA)\n",
    "- Understands sequence patterns and motifs\n",
    "- Captures contextual relationships between positions\n",
    "- Generalizes well across different RNA types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture_section",
   "metadata": {},
   "source": [
    "### 3. Model Architecture: Base Model + Task Head\n",
    "\n",
    "Let's visualize the architecture. At its core, our model consists of two parts:\n",
    "\n",
    "1. **OmniGenome Base Model**: A large, pre-trained transformer that reads RNA sequences (as tokens) and converts them into rich numerical representations (embeddings) that capture sequence patterns and contexts.\n",
    "\n",
    "2. **The Token Classification Head**: A smaller neural network (usually one or two linear layers) that takes the per-token embeddings and transforms them into 3-class predictions (one for each structural label).\n",
    "\n",
    "Here is a diagram illustrating this architecture:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Input\"\n",
    "        A[\"RNA Sequence<br/>AUGCCGUGC\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"Tokenization\"\n",
    "        B[\"Input Tokens<br/>[CLS], A, U, G, C, C, G, U, G, C, [SEP]\"]\n",
    "    end\n",
    "\n",
    "    subgraph \"OmniModelForTokenClassification\"\n",
    "        C(\"Base Model<br/>OmniGenome-52M\")\n",
    "        D(\"Classification Head<br/>Linear Layer + Softmax\")\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output\"\n",
    "        E[\"Per-Token Predictions<br/>[., (, (, (, ., ., ., ), ), .]\"]\n",
    "    end\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "    C -- \"Token Embeddings\" --> D\n",
    "    D --> E\n",
    "```\n",
    "\n",
    "The base model does the heavy lifting of understanding the sequence, while the head adapts that understanding to our specific predictive goal. During fine-tuning, we update the weights of both the head and (to a lesser extent) the base model to optimize for structure prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "io_section",
   "metadata": {},
   "source": [
    "### 4. Inputs and Outputs: A Look at the Data Flow\n",
    "\n",
    "Understanding what the model expects and returns is critical for debugging and interpretation.\n",
    "\n",
    "#### Model Inputs\n",
    "The model expects a **dictionary** with these keys:\n",
    "- `input_ids`: Tokenized sequence (tensor of integers)\n",
    "- `attention_mask`: Mask indicating real vs. padded tokens\n",
    "- `labels` (during training): Ground-truth labels for each position\n",
    "\n",
    "#### Model Outputs\n",
    "The model returns a **dictionary** with:\n",
    "- `logits`: Raw prediction scores for each token and class (shape: [batch, seq_len, num_labels])\n",
    "- `loss` (during training): Computed loss value\n",
    "- `predictions` (during inference): Predicted class IDs\n",
    "\n",
    "#### Example Flow\n",
    "```python\n",
    "# Input\n",
    "sequence = \"AUGCCGUGC\"\n",
    "# After tokenization: [CLS] A U G C C G U G C [SEP]\n",
    "# Model output logits shape: [1, 11, 3]\n",
    "# Predictions: [., ., (, (, (, ., ., ), ), ., .]\n",
    "# (Ignore [CLS] and [SEP] labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation_section",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Practical Implementation\n",
    "\n",
    "Now let's put theory into practice. We'll initialize the model in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_imports",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install omnigenbench -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForTokenClassification,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_step",
   "metadata": {},
   "source": [
    "### Step 2: Configuration\n",
    "\n",
    "Define our model and label configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "\n",
    "# Label mapping for RNA secondary structure\n",
    "label2id = {\n",
    "    \"(\": 0,  # Opening base pair\n",
    "    \")\": 1,  # Closing base pair\n",
    "    \".\": 2   # Unpaired nucleotide\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "print(f\"‚úÖ Configuration complete!\")\n",
    "print(f\"üìä Model: {model_name_or_path}\")\n",
    "print(f\"üìä Number of labels: {num_labels}\")\n",
    "print(f\"üìä Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_step",
   "metadata": {},
   "source": [
    "### Step 3: Initialize Tokenizer\n",
    "\n",
    "The tokenizer converts sequences into model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer matching the model\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {model_name_or_path}\")\n",
    "print(f\"üìä Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"üìä Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init_step",
   "metadata": {},
   "source": [
    "### Step 4: Initialize Model\n",
    "\n",
    "Now we create the model for token classification. This is remarkably simple with OmniGenBench!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for token classification\n",
    "model = OmniModelForTokenClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model initialized: {model_name_or_path}\")\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"  - Architecture: Token-level classification\")\n",
    "print(f\"  - Base model: OmniGenome-52M\")\n",
    "print(f\"  - Number of labels: {num_labels}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_model",
   "metadata": {},
   "source": [
    "### Step 5: Test Model with Sample Input\n",
    "\n",
    "Let's verify the model works by testing it on a sample sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample sequence\n",
    "sample_sequence = \"AUGCCGUGCAUUAA\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(\n",
    "    sample_sequence,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "print(f\"üìù Sample sequence: {sample_sequence}\")\n",
    "print(f\"üìä Tokenized input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"üìä Input IDs: {inputs['input_ids']}\")\n",
    "\n",
    "# Forward pass (no gradients needed for testing)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nüìä Model Outputs:\")\n",
    "print(f\"  - Logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"  - Logits: {outputs['logits']}\")\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs['logits'], dim=-1)[0]\n",
    "predicted_structure = \"\".join([id2label[pred.item()] for pred in predictions[1:-1]])  # Skip [CLS] and [SEP]\n",
    "\n",
    "print(f\"\\nüîÆ Predicted Structure:\")\n",
    "print(f\"  Sequence:  {sample_sequence}\")\n",
    "print(f\"  Structure: {predicted_structure}\")\n",
    "print(f\"\\nüí° Note: These are untrained predictions. After training, they will be accurate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_components",
   "metadata": {},
   "source": [
    "### Understanding Model Components\n",
    "\n",
    "Let's examine the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model components\n",
    "print(\"üèóÔ∏è Model Architecture:\")\n",
    "print(f\"\\nBase Model: {type(model.model).__name__}\")\n",
    "print(f\"Classification Head: {type(model.classifier).__name__}\")\n",
    "\n",
    "# Show classification head details\n",
    "print(f\"\\nüìä Classification Head Structure:\")\n",
    "print(model.classifier)\n",
    "\n",
    "print(f\"\\nüí° The classification head transforms the base model's embeddings into class predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## üìö Summary and Next Steps\n",
    "\n",
    "In this tutorial, we:\n",
    "1. ‚úÖ Understood the concept of foundation models and pre-training\n",
    "2. ‚úÖ Explored the OmniGenBench model zoo\n",
    "3. ‚úÖ Learned the principle of matching models to tasks\n",
    "4. ‚úÖ Understood the \"base model + task head\" architecture\n",
    "5. ‚úÖ Initialized OmniGenome for token classification\n",
    "6. ‚úÖ Tested the model with a sample sequence\n",
    "\n",
    "### What We've Accomplished\n",
    "```python\n",
    "# Model initialization in just 3 lines!\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "model = OmniModelForTokenClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "- **Foundation models** provide powerful pre-trained representations\n",
    "- **Task heads** adapt base models to specific problems\n",
    "- **Token classification** predicts a label for each position\n",
    "- **OmniGenBench** makes model initialization effortless\n",
    "\n",
    "### Next: Model Training\n",
    "Now that our model is initialized, proceed to **[03_model_training.ipynb](03_model_training.ipynb)** to:\n",
    "- Fine-tune the model on RNA structure data\n",
    "- Configure training parameters\n",
    "- Evaluate model performance\n",
    "- Save the trained model\n",
    "\n",
    "The model is ready to learn! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}