# ç¿»è¯‘æ•ˆç‡é¢„æµ‹ - BPPç‰¹å¾èåˆæ¨¡å‹

## ğŸ“‹ æ¦‚è¿°

æœ¬æ¨¡å—å®ç°äº†ä¸€ä¸ªåŸºäº **OmniGenBench** æ¡†æ¶çš„ç¿»è¯‘æ•ˆç‡ï¼ˆTranslation Efficiency, TEï¼‰é¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡èåˆ **RNAåºåˆ—ç‰¹å¾** å’Œ **ç¢±åŸºé…å¯¹æ¦‚ç‡ï¼ˆBase Pairing Probability, BPPï¼‰** ç»“æ„ç‰¹å¾æ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚

### ğŸ¯ æ ¸å¿ƒåˆ›æ–°

1. **ç»“æ„ç‰¹å¾èåˆ**: å°†RNAäºŒçº§ç»“æ„ä¿¡æ¯ï¼ˆBPPçŸ©é˜µï¼‰ä¸åºåˆ—embeddingsç»“åˆ
2. **ç«¯åˆ°ç«¯è®­ç»ƒ**: è”åˆä¼˜åŒ–æ‰€æœ‰ç»„ä»¶ï¼Œæ— éœ€é¢„è®­ç»ƒ
3. **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ›¿æ¢åŸºç¡€æ¨¡å‹æˆ–ä¿®æ”¹ç‰¹å¾å¤„ç†å™¨
4. **ç”Ÿäº§å°±ç»ª**: å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°å’Œæ¨ç†pipeline

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

```
è¾“å…¥: RNAåºåˆ—
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚                      â”‚
    â–¼                     â–¼                      â–¼
[Tokenizer]        [ViennaRNA]           [Sequence Info]
    â”‚                     â”‚                      â”‚
    â–¼                     â–¼                      â”‚
[Transformer]      [BPP Matrix]                 â”‚
    â”‚               512Ã—512                      â”‚
    â”‚                     â”‚                      â”‚
    â–¼                     â–¼                      â”‚
[CLS Token]        [CNN Processor]               â”‚
 Embedding          3-Layer CNN                  â”‚
  (768-d)          + Global Pool                 â”‚
    â”‚                  (128-d)                   â”‚
    â”‚                     â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
               â”‚                                 â”‚
               â–¼                                 â”‚
         [Feature Fusion]                        â”‚
         Gated MLP (256-d)                       â”‚
               â”‚                                 â”‚
               â–¼                                 â”‚
         [Classifier]                            â”‚
           2 classes                             â”‚
               â”‚                                 â”‚
               â–¼                                 â”‚
    Prediction: High TE / Low TE                 â”‚
```

### å…³é”®ç»„ä»¶

#### 1. **TEDatasetWithBPP** - è‡ªå®šä¹‰æ•°æ®é›†ç±»
- **åŠŸèƒ½**: åœ¨æ•°æ®åŠ è½½æ—¶è®¡ç®—BPPçŸ©é˜µ
- **ç‰¹ç‚¹**:
  - è‡ªåŠ¨åºåˆ—å¡«å……/æˆªæ–­åˆ°512é•¿åº¦
  - ä½¿ç”¨ViennaRNAé…åˆ†å‡½æ•°è®¡ç®—BPP
  - BPPç»“æœç¼“å­˜ï¼ˆæé€Ÿï¼‰
  - ä¼˜é›…çš„é”™è¯¯å¤„ç†

```python
class TEDatasetWithBPP(OmniDatasetForSequenceClassification):
    def compute_bpp_matrix(self, sequence: str) -> np.ndarray:
        """ä½¿ç”¨ViennaRNAè®¡ç®—ç¢±åŸºé…å¯¹æ¦‚ç‡çŸ©é˜µ"""
        fc = RNA.fold_compound(sequence)
        fc.pf()  # è®¡ç®—é…åˆ†å‡½æ•°
        bpp = fc.bpp()  # è·å–BPPçŸ©é˜µ
        # è½¬æ¢ä¸ºå¯¹ç§°numpyæ•°ç»„
        return bpp_matrix
    
    def prepare_input(self, instance, **kwargs):
        """å‡†å¤‡è¾“å…¥: åºåˆ—tokenization + BPPè®¡ç®—"""
        # 1. æå–åºåˆ—å¹¶è½¬æ¢T->U
        # 2. å¡«å……/æˆªæ–­åˆ°512
        # 3. è®¡ç®—BPPçŸ©é˜µ
        # 4. Tokenizeåºåˆ—
        # 5. è¿”å›æ‰€æœ‰ç‰¹å¾
```

#### 2. **BPPProcessor** - BPPç‰¹å¾æå–å™¨
- **æ¶æ„**: 3å±‚CNN + å…¨å±€å¹³å‡æ± åŒ–
- **è¾“å…¥**: `[batch, 1, 512, 512]` BPPçŸ©é˜µ
- **è¾“å‡º**: `[batch, 128]` ç»“æ„ç‰¹å¾å‘é‡

```python
self.cnn = nn.Sequential(
    # Layer 1: 512Ã—512 -> 256Ã—256
    nn.Conv2d(1, 32, kernel_size=3, padding=1),
    nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
    
    # Layer 2: 256Ã—256 -> 128Ã—128
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
    
    # Layer 3: 128Ã—128 -> 1Ã—1 (global pool)
    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.BatchNorm2d(128), nn.ReLU(),
    nn.AdaptiveAvgPool2d((1, 1)),
)
```

#### 3. **FeatureFusion** - é—¨æ§ç‰¹å¾èåˆæ¨¡å—
- **æœºåˆ¶**: è‡ªé€‚åº”é—¨æ§èåˆï¼ˆGated Fusionï¼‰
- **å…¬å¼**: `output = gate * seq_features + (1 - gate) * bpp_features`
- **ä¼˜åŠ¿**: æ¨¡å‹è‡ªåŠ¨å­¦ä¹ ä¸¤ç§ç‰¹å¾çš„æƒé‡

```python
class FeatureFusion(nn.Module):
    def forward(self, seq_features, bpp_features):
        # æŠ•å½±åˆ°å…±åŒç©ºé—´
        seq_proj = self.seq_proj(seq_features)
        bpp_proj = self.bpp_proj(bpp_features)
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate = self.gate(torch.cat([seq_features, bpp_features], dim=1))
        
        # é—¨æ§èåˆ
        fused = gate * seq_proj + (1 - gate) * bpp_proj
        
        # MLPå¤„ç†
        return self.fusion_mlp(fused)
```

#### 4. **TEModelWithBPP** - å®Œæ•´æ¨¡å‹
- **ç»§æ‰¿**: `OmniModelForSequenceClassification`
- **å…¼å®¹æ€§**: æ”¯æŒæ‰€æœ‰OmniGenBenchåŸºç¡€æ¨¡å‹
- **ç‰¹ç‚¹**: ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# å®‰è£…OmniGenBench
pip install omnigenbench -U

# å®‰è£…ViennaRNA (å¿…éœ€)
conda install -c bioconda viennarna

# æˆ–è€…ä½¿ç”¨pip (Linux/Mac)
pip install ViennaRNA
```

### æ–¹æ³•1: ä¸€é”®è®­ç»ƒï¼ˆæ¨èï¼‰

```python
from te_bpp_model import train_te_model

# ä½¿ç”¨ä½ çš„æ•°æ®è®­ç»ƒ
results = train_te_model(
    model_name="yangheng/PlantRNA-FM",  # æˆ–å…¶ä»–åŸºç¡€æ¨¡å‹
    train_file="data/train.json",
    valid_file="data/valid.json",
    test_file="data/test.json",
    max_length=512,
    batch_size=16,
    epochs=30,
    learning_rate=2e-5,
    output_dir="my_te_model"
)

print(f"Test F1 Score: {results['f1_score']:.4f}")
```

### æ–¹æ³•2: ä½¿ç”¨æ¼”ç¤ºæ•°æ®æµ‹è¯•

```python
from te_bpp_model import train_te_model

# è‡ªåŠ¨åˆ›å»ºæ¼”ç¤ºæ•°æ®é›†å¹¶è®­ç»ƒ
results = train_te_model(
    model_name="yangheng/PlantRNA-FM",
    use_demo=True,  # ä½¿ç”¨åˆæˆæ•°æ®
    batch_size=8,
    epochs=10,
    output_dir="te_demo_model"
)
```

### æ–¹æ³•3: è‡ªå®šä¹‰è®­ç»ƒæµç¨‹

```python
from te_bpp_model import TEModelWithBPP, TEDatasetWithBPP
from omnigenbench import OmniTokenizer, AccelerateTrainer, ClassificationMetric

# 1. åˆå§‹åŒ–
tokenizer = OmniTokenizer.from_pretrained("yangheng/PlantRNA-FM")
label2id = {"0": 0, "1": 1}

# 2. åŠ è½½æ•°æ®é›†
train_dataset = TEDatasetWithBPP(
    "train.json",
    tokenizer=tokenizer,
    max_length=512,
    label2id=label2id
)
valid_dataset = TEDatasetWithBPP("valid.json", tokenizer, 512, label2id=label2id)
test_dataset = TEDatasetWithBPP("test.json", tokenizer, 512, label2id=label2id)

# 3. åˆ›å»ºæ¨¡å‹
model = TEModelWithBPP(
    config_or_model="yangheng/PlantRNA-FM",
    tokenizer=tokenizer,
    num_labels=2,
    label2id=label2id
)

# 4. è®­ç»ƒ
trainer = AccelerateTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    test_dataset=test_dataset,
    compute_metrics=[ClassificationMetric().f1_score],
    batch_size=8,
    num_train_epochs=20,
    learning_rate=2e-5
)

metrics = trainer.train()
```

---

## ğŸ“Š æ•°æ®æ ¼å¼

### è¾“å…¥æ ¼å¼: JSON Lines

æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«`sequence`å’Œ`label`å­—æ®µï¼š

```json
{"sequence": "AUGCAUGCAUGCGCGCGCGC...", "label": 1}
{"sequence": "UUUAAAGGGCCCUUUAAAGGG...", "label": 0}
```

- **sequence**: RNAåºåˆ—ï¼ˆA, U, G, C, Nï¼‰
  - æ”¯æŒT/Uè‡ªåŠ¨è½¬æ¢
  - è‡ªåŠ¨å¡«å……åˆ°512é•¿åº¦ï¼ˆçŸ­åºåˆ—ç”¨'N'å¡«å……ï¼‰
  - è‡ªåŠ¨æˆªæ–­åˆ°512é•¿åº¦ï¼ˆé•¿åºåˆ—ï¼‰
  
- **label**: 
  - `0`: ä½ç¿»è¯‘æ•ˆç‡ï¼ˆLow TEï¼‰
  - `1`: é«˜ç¿»è¯‘æ•ˆç‡ï¼ˆHigh TEï¼‰

### æ•°æ®é›†ç»“æ„

```
translation_efficiency_prediction/
â”œâ”€â”€ train.json      # è®­ç»ƒé›† (æ¨è: 1000+ samples)
â”œâ”€â”€ valid.json      # éªŒè¯é›† (æ¨è: 200+ samples)
â””â”€â”€ test.json       # æµ‹è¯•é›† (æ¨è: 200+ samples)
```

### åˆ›å»ºä½ è‡ªå·±çš„æ•°æ®é›†

```python
import json

# å‡†å¤‡æ•°æ®
data = [
    {"sequence": "AUGCAUGC...", "label": 1},
    {"sequence": "UUUAAAGGG...", "label": 0},
    # ... æ›´å¤šæ ·æœ¬
]

# ä¿å­˜ä¸ºJSON Linesæ ¼å¼
with open("train.json", "w") as f:
    for item in data:
        f.write(json.dumps(item) + "\n")
```

---

## ğŸ”§ æ¨¡å‹æ¨ç†

### å•åºåˆ—é¢„æµ‹

```python
from omnigenbench import ModelHub

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = ModelHub.load("my_te_model")

# é¢„æµ‹
sequence = "AUGCAUGCAUGCGCGCGCGC" * 20
outputs = model.inference({"sequence": sequence})

prediction = outputs["predictions"][0]  # 0 or 1
confidence = outputs["confidence"]       # 0.0 - 1.0

print(f"Prediction: {'High TE' if prediction == 1 else 'Low TE'}")
print(f"Confidence: {confidence:.4f}")
```

### æ‰¹é‡é¢„æµ‹

```python
sequences = [
    "AUGCAUGCAUGC...",
    "UUUAAAGGGCCC...",
    "GCGCGCGCGCGC..."
]

for seq in sequences:
    outputs = model.inference({"sequence": seq})
    print(f"Sequence: {seq[:30]}...")
    print(f"  -> Prediction: {outputs['predictions'][0]}")
    print(f"  -> Confidence: {outputs['confidence']:.4f}\n")
```

### æå–ç‰¹å¾ç”¨äºä¸‹æ¸¸åˆ†æ

```python
model.eval()
with torch.no_grad():
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        bpp_matrix=bpp_matrix
    )
    
    sequence_features = outputs["sequence_features"]  # [batch, 768]
    bpp_features = outputs["bpp_features"]            # [batch, 128]
    logits = outputs["logits"]                        # [batch, 2]
```

---

## ğŸ¨ é«˜çº§å®šåˆ¶

### 1. æ›´æ¢åŸºç¡€æ¨¡å‹

```python
# ä½¿ç”¨å…¶ä»–æ¤ç‰©åŸºå› ç»„æ¨¡å‹
model = TEModelWithBPP(
    config_or_model="yangheng/OmniGenome-186M",  # é€šç”¨åŸºå› ç»„æ¨¡å‹
    tokenizer=tokenizer,
    num_labels=2
)

# ä½¿ç”¨DNABERT
model = TEModelWithBPP(
    config_or_model="zhihan1996/DNA_bert_6",
    tokenizer=tokenizer,
    num_labels=2
)
```

### 2. è°ƒæ•´BPPå¤„ç†å™¨

```python
# ä½¿ç”¨æ›´æ·±çš„CNN
class DeepBPPProcessor(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            # æ·»åŠ æ›´å¤šå±‚...
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )
    
    def forward(self, bpp_matrix):
        return self.cnn(bpp_matrix).flatten(1)

# æ›¿æ¢é»˜è®¤å¤„ç†å™¨
model.bpp_processor = DeepBPPProcessor()
```

### 3. ä¿®æ”¹èåˆç­–ç•¥

```python
# ç®€å•æ‹¼æ¥ï¼ˆä¸ä½¿ç”¨é—¨æ§ï¼‰
class SimpleFusion(nn.Module):
    def __init__(self, seq_dim, bpp_dim, output_dim):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(seq_dim + bpp_dim, 512),
            nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(512, output_dim),
            nn.ReLU()
        )
    
    def forward(self, seq_feat, bpp_feat):
        return self.mlp(torch.cat([seq_feat, bpp_feat], dim=1))

# ä½¿ç”¨
model.fusion = SimpleFusion(768, 128, 256)
```

### 4. å¤šä»»åŠ¡å­¦ä¹ 

```python
class MultiTaskTEModel(TEModelWithBPP):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ·»åŠ è¾…åŠ©ä»»åŠ¡headï¼ˆä¾‹å¦‚ï¼šé¢„æµ‹è¡¨è¾¾é‡ï¼‰
        self.expression_head = nn.Linear(256, 1)
    
    def forward(self, *args, **kwargs):
        outputs = super().forward(*args, **kwargs)
        # æ·»åŠ è¾…åŠ©ä»»åŠ¡è¾“å‡º
        fused_features = self.fusion(
            outputs["sequence_features"],
            outputs["bpp_features"]
        )
        outputs["expression_pred"] = self.expression_head(fused_features)
        return outputs
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒåŠ é€Ÿ

```python
# 1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
trainer = AccelerateTrainer(
    model=model,
    train_dataset=train_dataset,
    mixed_precision="fp16",  # æˆ– "bf16" (A100+)
    batch_size=16,           # å¢å¤§batch size
)

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchï¼‰
trainer = AccelerateTrainer(
    model=model,
    train_dataset=train_dataset,
    batch_size=4,
    gradient_accumulation_steps=4,  # æœ‰æ•ˆbatch=16
)

# 3. å¤šGPUè®­ç»ƒ
# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
trainer = AccelerateTrainer(
    model=model,
    train_dataset=train_dataset,
)
```

### BPPè®¡ç®—ä¼˜åŒ–

```python
# ä½¿ç”¨å¹¶è¡Œè®¡ç®—BPPï¼ˆé€‚åˆå¤§æ•°æ®é›†é¢„å¤„ç†ï¼‰
from multiprocessing import Pool

def precompute_bpp_matrices(sequences, num_workers=8):
    """é¢„è®¡ç®—æ‰€æœ‰BPPçŸ©é˜µå¹¶ä¿å­˜"""
    with Pool(num_workers) as pool:
        bpp_matrices = pool.map(compute_bpp_matrix, sequences)
    return bpp_matrices

# ä¿å­˜é¢„è®¡ç®—çš„BPP
import pickle
with open("bpp_cache.pkl", "wb") as f:
    pickle.dump(bpp_matrices, f)
```

### å†…å­˜ä¼˜åŒ–

```python
# å¯¹äºé•¿åºåˆ—æ•°æ®é›†
dataset = TEDatasetWithBPP(
    "train.json",
    tokenizer=tokenizer,
    max_length=512,  # å‡å°‘max_length
    label2id=label2id
)

# æ¸…ç†BPPç¼“å­˜
dataset._bpp_cache.clear()
```

---

## ğŸ§ª å®éªŒå»ºè®®

### è¶…å‚æ•°æœç´¢

```python
# æ¨èçš„è¶…å‚æ•°èŒƒå›´
hyperparams = {
    "learning_rate": [1e-5, 2e-5, 5e-5],
    "batch_size": [8, 16, 32],
    "bpp_dim": [64, 128, 256],
    "fusion_dim": [128, 256, 512],
}

# ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–éšæœºæœç´¢
best_f1 = 0
best_config = None

for lr in hyperparams["learning_rate"]:
    for bs in hyperparams["batch_size"]:
        model = TEModelWithBPP(...)
        results = train_te_model(
            learning_rate=lr,
            batch_size=bs,
            epochs=20
        )
        if results["f1_score"] > best_f1:
            best_f1 = results["f1_score"]
            best_config = {"lr": lr, "batch_size": bs}
```

### æ¶ˆèå®éªŒ

```python
# 1. ä»…ä½¿ç”¨åºåˆ—ç‰¹å¾ï¼ˆbaselineï¼‰
class SequenceOnlyModel(OmniModelForSequenceClassification):
    def forward(self, input_ids, attention_mask, bpp_matrix=None, labels=None):
        # å¿½ç•¥bpp_matrix
        outputs = self.model(input_ids, attention_mask)
        logits = self.classifier(outputs.last_hidden_state[:, 0, :])
        # ... è®¡ç®—loss
        return {"loss": loss, "logits": logits}

# 2. ä»…ä½¿ç”¨BPPç‰¹å¾
class BPPOnlyModel(nn.Module):
    def forward(self, bpp_matrix, labels=None):
        bpp_features = self.bpp_processor(bpp_matrix)
        logits = self.classifier(bpp_features)
        # ... è®¡ç®—loss
        return {"loss": loss, "logits": logits}

# 3. å®Œæ•´æ¨¡å‹ï¼ˆåºåˆ— + BPPï¼‰
# ä½¿ç”¨ TEModelWithBPP

# æ¯”è¾ƒä¸‰è€…æ€§èƒ½
```

---

## ğŸ“š æŠ€æœ¯ç»†èŠ‚

### BPPçŸ©é˜µè¯¦è§£

**å®šä¹‰**: BPP[i][j] è¡¨ç¤ºä½ç½®iå’Œjä¹‹é—´å½¢æˆç¢±åŸºé…å¯¹çš„æ¦‚ç‡

**è®¡ç®—æ–¹æ³•**: ä½¿ç”¨ViennaRNAçš„é…åˆ†å‡½æ•°ç®—æ³•
```python
fc = RNA.fold_compound(sequence)
fc.pf()  # è®¡ç®—Boltzmanné…åˆ†å‡½æ•° Z = Î£_s exp(-E(s)/RT)
bpp = fc.bpp()  # BPP[i][j] = P(i-j paired) = Î£_s Î´_{ij}(s) exp(-E(s)/RT) / Z
```

**æ€§è´¨**:
- å¯¹ç§°çŸ©é˜µ: BPP[i][j] = BPP[j][i]
- å€¼åŸŸ: [0, 1]
- å¯¹è§’çº¿ä¸º0ï¼ˆè‡ªèº«ä¸èƒ½é…å¯¹ï¼‰
- é«˜å€¼è¡¨ç¤ºè¯¥é…å¯¹åœ¨çƒ­åŠ›å­¦ä¸Šç¨³å®š

**ç”Ÿç‰©å­¦æ„ä¹‰**:
- åæ˜ RNAäºŒçº§ç»“æ„çš„çµæ´»æ€§
- ä¸ç¿»è¯‘æ•ˆç‡ç›¸å…³ï¼ˆç»“æ„å½±å“æ ¸ç³–ä½“ç»“åˆå’Œç§»åŠ¨ï¼‰
- æ•è·5' UTRå’ŒCDSèµ·å§‹åŒºçš„ç»“æ„ç‰¹å¾

### æ¨¡å‹å®¹é‡åˆ†æ

```python
from te_bpp_model import TEModelWithBPP
from omnigenbench import OmniTokenizer

tokenizer = OmniTokenizer.from_pretrained("yangheng/PlantRNA-FM")
model = TEModelWithBPP("yangheng/PlantRNA-FM", tokenizer, num_labels=2)

# è®¡ç®—å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# é¢„æœŸè¾“å‡º (PlantRNA-FM)
# Total parameters: ~35,000,000
# Trainable parameters: ~35,000,000
```

**ç»„ä»¶å‚æ•°åˆ†è§£**:
- PlantRNA-FM base: ~35M
- BPP Processor (CNN): ~200K
- Feature Fusion: ~500K
- Classifier: ~512

### è®¡ç®—å¤æ‚åº¦

**è®­ç»ƒæ—¶é—´å¤æ‚åº¦**:
- BPPè®¡ç®—: O(NÂ³) per sequence (ViennaRNA)
- Transformer: O(NÂ²Â·D) per forward pass
- CNN: O(NÂ²Â·KÂ²Â·C) (K=kernel size, C=channels)
- Total: O(NÂ³ + NÂ²Â·D) dominated by BPP

**æ¨ç†æ—¶é—´** (512é•¿åº¦åºåˆ—, single GPU):
- BPPè®¡ç®—: ~0.5-1s
- Model forward: ~0.01s
- Total: ~0.5-1s per sequence

**å†…å­˜ä½¿ç”¨** (512é•¿åº¦, batch=8):
- BPP matrices: 8 Ã— 512Â² Ã— 4 bytes = ~8MB
- Model activations: ~2GB
- Total: ~2-3GB GPU memory

---

## ğŸ› æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. ViennaRNAå®‰è£…å¤±è´¥

**ç—‡çŠ¶**: `ImportError: No module named 'RNA'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# macOS/Linux
conda install -c bioconda viennarna

# Windows (éœ€è¦WSL)
# 1. å®‰è£…WSL: https://docs.microsoft.com/en-us/windows/wsl/install
# 2. åœ¨WSLä¸­å®‰è£…conda
# 3. conda install -c bioconda viennarna
```

#### 2. BPPè®¡ç®—å¤±è´¥

**ç—‡çŠ¶**: `Warning: Failed to compute BPP`

**åŸå› **: åºåˆ—åŒ…å«æ— æ•ˆå­—ç¬¦æˆ–å¤ªçŸ­

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥åºåˆ—
sequence = sequence.upper().replace("T", "U")
valid_chars = set("AUGCN")
if not all(c in valid_chars for c in sequence):
    print(f"Invalid characters in sequence")

# è¿‡æ»¤æ— æ•ˆåºåˆ—
filtered_data = [
    item for item in data 
    if len(item["sequence"]) >= 50 and 
       all(c in "AUGCTN" for c in item["sequence"].upper())
]
```

#### 3. CUDAå†…å­˜ä¸è¶³

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: å‡å°batch size
trainer = AccelerateTrainer(model=model, batch_size=4)

# æ–¹æ¡ˆ2: å‡å°åºåˆ—é•¿åº¦
dataset = TEDatasetWithBPP(..., max_length=256)

# æ–¹æ¡ˆ3: ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.model.gradient_checkpointing_enable()

# æ–¹æ¡ˆ4: ä½¿ç”¨CPUï¼ˆé€Ÿåº¦æ…¢ï¼‰
device = torch.device("cpu")
model = model.to(device)
```

#### 4. è®­ç»ƒä¸æ”¶æ•›

**ç—‡çŠ¶**: Lossä¸ä¸‹é™æˆ–æ³¢åŠ¨

**è¯Šæ–­**:
```python
# æ£€æŸ¥æ•°æ®å¹³è¡¡
from collections import Counter
labels = [item["label"] for item in train_data]
print(Counter(labels))  # åº”è¯¥æ¥è¿‘1:1

# æ£€æŸ¥å­¦ä¹ ç‡
# å°è¯•é™ä½å­¦ä¹ ç‡
trainer = AccelerateTrainer(learning_rate=1e-5)

# æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm()}")
```

---

## ğŸ“– å‚è€ƒèµ„æ–™

### ç›¸å…³è®ºæ–‡

1. **ViennaRNA Package**
   - Lorenz, R. et al. (2011). ViennaRNA Package 2.0. Algorithms Mol Biol, 6, 26.
   - [Link](https://almob.biomedcentral.com/articles/10.1186/1748-7188-6-26)

2. **Translation Efficiency Prediction**
   - Sample, P. J. et al. (2019). Human 5' UTR design and variant effect prediction from a massively parallel translation assay. Nat Biotechnol, 37(7), 803-809.

3. **RNA Structure and Translation**
   - Bevilacqua, P. C. et al. (2016). Structures mRNA: A New Frontier in RNA Biology and Drug Targets. Nat Rev Mol Cell Biol, 17(7), 436-451.

### OmniGenBenchæ–‡æ¡£

- **ä¸»é¡µ**: [https://github.com/yangheng95/OmniGenBench](https://github.com/yangheng95/OmniGenBench)
- **æ–‡æ¡£**: [https://omnigenbench.readthedocs.io/](https://omnigenbench.readthedocs.io/)
- **ç¤ºä¾‹**: [https://github.com/yangheng95/OmniGenBench/tree/master/examples](https://github.com/yangheng95/OmniGenBench/tree/master/examples)

### åŸºç¡€æ¨¡å‹

- **PlantRNA-FM**: [yangheng/PlantRNA-FM](https://huggingface.co/yangheng/PlantRNA-FM)
- **OmniGenome**: [yangheng/OmniGenome-186M](https://huggingface.co/yangheng/OmniGenome-186M)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

### æ”¹è¿›å»ºè®®
- [ ] æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»ï¼ˆåŒæ—¶é¢„æµ‹TEå’Œç¨³å®šæ€§ï¼‰
- [ ] æ·»åŠ æ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆæ˜¾ç¤ºå“ªäº›é…å¯¹å¯¹é¢„æµ‹æœ€é‡è¦ï¼‰
- [ ] é›†æˆå…¶ä»–ç»“æ„é¢„æµ‹æ–¹æ³•ï¼ˆRNAfold, Mfoldï¼‰
- [ ] æ”¯æŒå¯å˜é•¿åº¦åºåˆ—ï¼ˆæ— éœ€paddingï¼‰
- [ ] æ·»åŠ æ¨¡å‹è§£é‡Šæ€§åˆ†æå·¥å…·

---

## ğŸ“„ è®¸å¯è¯

æœ¬ä»£ç éµå¾ª MIT Licenseï¼Œè¯¦è§ [LICENSE](../../LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ“§ è”ç³»æ–¹å¼

- **ä½œè€…**: YANG, HENG (æ¨æ’)
- **é‚®ç®±**: hy345@exeter.ac.uk
- **ä¸»é¡µ**: [https://yangheng95.github.io](https://yangheng95.github.io)
- **GitHub**: [@yangheng95](https://github.com/yangheng95)

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ1æ—¥
