{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c44b76",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Translation Efficiency Prediction with Foundation Model\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to predict **Translation Efficiency (TE)** from mRNA sequences using **OmniGenBench** and **PlantRNA-FM** (Plant RNA Foundation Model). This guide demonstrates the practical application of plant-specialized genomic deep learning for rice translation efficiency prediction.\n",
    "\n",
    "> ðŸ“š **Prerequisite**: If you are new to OmniGenBench, it is highly recommended to first study the **[Fundamental Concepts Tutorial](https://github.com/yangheng95/OmniGenBench/blob/master/examples/00_fundamental_concepts.ipynb)**, which covers general knowledge such as language model concepts, machine learning task classification, and foundation model principlesâ€”particularly how PlantRNA-FM works.\n",
    "\n",
    "### 1. The Biological Challenge: Translation Efficiency in Plants\n",
    "\n",
    "**Translation** is one of the most fundamental processes in molecular biology - it's the mechanism by which cells read mRNA sequences and synthesize proteins. **Translation Efficiency (TE)** in plants quantifies how effectively this process occurs for a given plant mRNA molecule, directly impacting protein production levels in plant cells.\n",
    "\n",
    "Understanding and predicting TE in plants has profound implications across multiple domains:\n",
    "- **Synthetic Biology & Crop Engineering**: Designing optimized gene circuits for precise protein expression in transgenic plants\n",
    "- **Agricultural Biotechnology**: Engineering crop plants for enhanced nutritional content (e.g., Golden Rice, protein-enriched grains)\n",
    "- **Stress Tolerance**: Understanding how plants regulate translation under stress conditions (drought, heat, salinity)\n",
    "- **Yield Improvement**: Optimizing expression of yield-related genes through codon optimization and UTR engineering\n",
    "- **Plant Defense**: Enhancing production of defense proteins and secondary metabolites\n",
    "\n",
    "However, experimentally measuring TE across thousands of plant mRNA sequences is time-consuming and costly. This is where **PlantRNA-FM** (published in *Nature Machine Intelligence*, 35M parameters), a plant-specialized foundation model, can provide transformative solutions by learning plant-specific translation patterns with remarkable efficiency.\n",
    "\n",
    "### 2. The Data: Rice Translation Efficiency Dataset\n",
    "\n",
    "To train our predictive model, we utilize a carefully curated dataset from *Oryza sativa* (rice), a model organism in plant biology.\n",
    "\n",
    "- **What it contains**: mRNA sequences with experimentally determined translation efficiency measurements\n",
    "- **What it labels**: Each sequence is classified as either \"High TE\" (1) or \"Low TE\" (0) based on ribosome profiling data\n",
    "- **Our Goal**: Train a model that can accurately classify any rice mRNA sequence by its translation efficiency potential\n",
    "\n",
    "**Dataset Structure:**\n",
    "\n",
    "| sequence | label | \n",
    "|---------|-------|\n",
    "| AUGGCCAUUGUAAUUGGCCGACUUGA... | 1 (High TE) | \n",
    "| AUGGCUACUAGCUAGCUAGCUAGC...    | 0 (Low TE) | \n",
    "| ...                                | ...  | \n",
    "\n",
    "Find the dataset template in **[Dataset Template](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/05_advanced_dataset_creation.ipynb)** and customize it as needed for your experiments.\n",
    "\n",
    "### 3. Quick Start: Translation Efficiency Prediction Workflow\n",
    "\n",
    "This tutorial demonstrates the practical application of the **[Fundamental Concepts](https://github.com/yangheng95/OmniGenBench/blob/master/examples/00_fundamental_concepts.ipynb)** to a specific biological problem. We'll use the standard 4-step OmniGenBench workflow:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7a1481a1d95c348",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"4-step workflow.png\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cfd5145",
   "metadata": {},
   "source": [
    "\n",
    "**Translation Efficiency Prediction** is a **sequence classification** task where we predict binary labels (High TE vs Low TE) for plant mRNA sequences. We'll use `OmniModelForSequenceClassification` with **PlantRNA-FM** to leverage plant-specific codon usage patterns and RNA structural features.\n",
    "\n",
    "### 4. Tutorial Structure\n",
    "\n",
    "1. **[Data Preparation](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/01_data_preparation.ipynb)**: Download and preprocess the rice translation efficiency dataset\n",
    "2. **[Model Initialization](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/02_model_initialization.ipynb)**: Load PlantRNA-FM and set it up for binary classification of plant mRNA sequences\n",
    "3. **[Training Implementation](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/03_model_training.ipynb)**: Fine-tune PlantRNA-FM using rice TE data and validate its performance\n",
    "4. **[Inference: Make Predictions](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/04_model_inference.ipynb)**: Use the trained model to predict translation efficiency on new plant mRNA sequences\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59944a",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Data Preparation\n",
    "\n",
    "This first step is all about getting our data ready for in-silico analysis. It involves four key parts:\n",
    "1.  **Environment Setup**: Installing and importing the necessary libraries.\n",
    "2.  **Configuration**: Defining all our important parameters in one place.\n",
    "3.  **Data Acquisition**: Downloading and preparing the raw dataset.\n",
    "4.  **Data Loading**: Creating a pipeline to efficiently feed data to the model.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library, `transformers` provides the underlying model architecture, and the other packages are utilities for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5573019",
   "metadata": {},
   "source": [
    "!pip install omnigenbench -U"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fc72ffc",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and interacting with the operating system.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU. This makes a huge difference when working with large models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "id": "07902125",
   "metadata": {},
   "source": [
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForSequenceClassification,\n",
    "    OmniModelForSequenceClassification,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e7baaf1",
   "metadata": {},
   "source": [
    "### 1.2: Global Configuration\n",
    "\n",
    "To make our tutorial easy to modify and understand, we'll centralize all important parameters in this section. This is a best practice in software development that makes experiments more reproducible.\n",
    "\n",
    "#### Key Parameters\n",
    "-   **Dataset**: We define the local path and download URL for our dataset.\n",
    "-   **Model**: We select which pre-trained foundation model to use. For this tutorial, we'll start with `PlantRNA-FM` (Hugging Face: `yangheng/PlantRNA-FM`) because it's fast and efficient, making it perfect for learning and prototyping.\n",
    "\n",
    "This centralized approach allows you to easily experiment with different settings (e.g., a larger model, a different learning rate) without having to hunt through the code.\n",
    "\n",
    "#### Note\n",
    "Almost all the parameters here are standard in machine learning workflows and have a default value that works well if you don't set them. Don't worry if some of these terms are unfamiliar right now. We'll explain each one as we go along."
   ]
  },
  {
   "cell_type": "code",
   "id": "e68bacd4",
   "metadata": {},
   "source": [
    "model_name_or_path = \"yangheng/PlantRNA-FM\"  # Plant RNA Foundation Model\n",
    "dataset_name = \"translation_efficiency_prediction\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "07bec55e",
   "metadata": {},
   "source": [
    "### 1.3: Data Acquisition\n",
    "\n",
    "With our environment configured, it's time to download the translation efficiency dataset for rice. The function below automates this process by:\n",
    "1.  Checking if the data already exists.\n",
    "2.  Downloading the dataset from the OmniGenBench Hub if needed.\n",
    "3.  Extracting the files.\n",
    "4.  Cleaning up the temporary zip file.\n",
    "\n",
    "This ensures we have the `train.json`, `valid.json`, and `test.json` files ready for the next stage. These files represent the standard splits for training, validating, and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b41c321",
   "metadata": {},
   "source": [
    "# Model and Tokenizer\n",
    "\n",
    "# We define the label mapping in the training\n",
    "label2id = {\"0\": 0, \"1\": 1}  # 0: Low TE, 1: High TE\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name_or_path=\"translation_efficiency_prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    label2id=label2id,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93f3fef9",
   "metadata": {},
   "source": [
    "### 1.4: Dataset Loading with OmniGenBench\n",
    "\n",
    "With OmniGenBench, data loading is significantly simplified! The framework automatically handles:\n",
    "\n",
    "#### Automatic Data Processing\n",
    "The `OmniDatasetForSequenceClassification` class automatically:\n",
    "1. **Downloads and processes** the dataset from our curated collection\n",
    "2. **Handles sequence preprocessing** including truncation, padding, and tokenization\n",
    "3. **Manages binary classification formatting** for translation efficiency prediction\n",
    "4. **Creates train/validation/test splits** ready for training\n",
    "\n",
    "This streamlined approach eliminates the need for custom dataset classes while maintaining full flexibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "9eb859d6",
   "metadata": {},
   "source": [
    "print(f\"ðŸ“Š Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86ab694a",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Initialization with PlantRNA-FM\n",
    "\n",
    "With our data pipeline in place, it's time to set up **PlantRNA-FM** (Plant RNA Foundation Model). Instead of building a model from scratch, we'll load the pre-trained PlantRNA-FM and adapt it for our rice translation efficiency task.\n",
    "\n",
    "This process involves three key components:\n",
    "1.  **The Tokenizer**: Converts plant RNA sequences into a numerical format PlantRNA-FM can process. It's crucial that we use the tokenizer specifically designed for PlantRNA-FM.\n",
    "2.  **PlantRNA-FM Base Model**: This plant-specialized model has learned fundamental patterns of plant RNA sequences, codon usage, and regulatory elements from extensive plant transcriptome data.\n",
    "3.  **The Classification Head**: We add a classification head on top of PlantRNA-FM that maps plant RNA representations to our binary labels (High TE vs Low TE).\n",
    "\n",
    "The `OmniModelForSequenceClassification` class seamlessly combines PlantRNA-FM with the task-specific classification head for rice translation efficiency prediction."
   ]
  },
  {
   "cell_type": "code",
   "id": "10959394",
   "metadata": {},
   "source": [
    "# === Model Initialization with PlantRNA-FM ===\n",
    "# Using PlantRNA-FM for plant-specific translation efficiency prediction\n",
    "\n",
    "model = OmniModelForSequenceClassification(\n",
    "    model_name_or_path,  # PlantRNA-FM\n",
    "    tokenizer,\n",
    "    num_labels=len(list(label2id.keys())),  # Binary classification: Low TE vs High TE\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded PlantRNA-FM for rice translation efficiency prediction\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd2e14e5",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Fine-tuning PlantRNA-FM\n",
    "\n",
    "This is the most exciting part! With our data and PlantRNA-FM ready, we can now begin the **fine-tuning** process. During training, PlantRNA-FM will adapt its plant-specific knowledge to learn the relationship between rice mRNA sequence features and translation efficiency.\n",
    "\n",
    "The `AccelerateTrainer` from `omnigenbench` handles the training process efficiently, allowing us to fine-tune PlantRNA-FM with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "id": "215efa9a",
   "metadata": {},
   "source": [
    "metric_functions = [ClassificationMetric().f1_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    ")\n",
    "print(\"ðŸŽ“ Starting training...\")\n",
    "\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_te_finetuned\")\n",
    "\n",
    "print('Metrics:', metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42b1f81d",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Model Inference and Interpretation\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting translation efficiency on new mRNA sequences. This process is called **inference**.\n",
    "\n",
    "### The Inference Pipeline\n",
    "\n",
    "Our inference pipeline consists of a few key steps:\n",
    "1. **Load the Model**: We load the best-performing model that was saved during training.\n",
    "2. **Process the Input**: We take new mRNA sequences and apply the same preprocessing steps we used for our training data (truncating/padding and tokenizing).\n",
    "3. **Run Prediction**: We feed the processed sequence to the model and get its predictions. We use `torch.no_grad()` to disable gradient calculations, which makes inference faster and uses less memory.\n",
    "4. **Interpret the Results**: The model's raw output is a probability score. We'll interpret these to make them more understandable, identifying whether sequences have high or low translation efficiency and with what level of confidence.\n",
    "\n",
    "To demonstrate, we'll test our model on a few sample sequences and print out a user-friendly summary of the results. This shows how the model can be used in a real-world application to analyze sequences of interest."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c23c904",
   "metadata": {},
   "source": [
    "\n",
    "inference_model = ModelHub.load(\"yangheng/ogb_te_finetuned\")\n",
    "\n",
    "sample_sequences = {\n",
    "    \"Optimized sequence\": \"AAACCAACAAAATGCAGTAGAAGTACTCTCGAGCTATAGTCGCGACGTGCTGCCCCGCAGGAGTACAGTAGTAGTACAACGTAAGCGGGAGCAACAGACTCCCCCCCTGCAACCCACTGTGCCTGTGCCCTCGACGCGTCTCCGTCGCTTTGGCAAATGTCACGTACATATTACCGTCTCAGGCTCTCAGCCATGCTCCCTACCACCCCTGCAGCGAAGCAAAAGCCACGCACGCGGCGCCTGACATGTAACAGGACTAGACCATCTTGTTCATTTCCCGCACCCCCTCCTCTCCTCTTCCTCCATCTGCCTCTTTAAAACAGTAAAAATAACCGTGCATCCCCTGGGCAAAATCTCTCCCATACATACACTACAGCGGCGAACCTTTCCTTATTCTCGCAACGCCTCGGTAACGGGCAGCGCCTGCTCCGCGCCGCGGTTGCGAGTTCGGGAAGGCGGCCGGAGTCGCGGGGAGGAGAGGGAGGATTCGATCGGCCAGA\",\n",
    "    \"Suboptimal sequence\": \"TGGAGATGGGCAGATGGCACACAAAACATGAATAGAAAACCCAAAAGGAAGGATGAAAAAAACACACACACACACACACACAAAACACAGAGAGAGAGAGAGAGAGAGCGAGAAAAGAAAAGAAAAAACCAATTCTTTTGGTCTCTTCCCTCTCCGTTTGTCGTGTCGAAGCCTTTGCCCCCACCACCTCCTCCTCTCCTCTCCCTTCCTCCCCTCCTCCCCATCTCGCTCTCCTCCCTCCTCTCTCCTCTCCTCGTCTCCTCTTCCTCTCCATTCCATTGGCCATTCCATTCCATTCCACCCCCCATGAAACCCCAAACCCTCGTCGGCCTCGCCGCGCTCGCGTAGCGCACCCGCCCTTCTCCTCTCGCCGGTGGTCCGCCGCCAGCCTCCCCCCACCCGATCCCGCCGCCCCCCCCGCCTTCACCCCGCCCACGCGGACGCATCCGATCCCGCCGCATCGCCGCGCGGGGGGGGGGGGGGGGGGGGGGGGGAGGGCACG\",\n",
    "    \"Random sequence\": \"AUGC\" * (128 // 4),\n",
    "}\n",
    "for seq_name, sequence in sample_sequences.items():\n",
    "    outputs = inference_model.inference(sequence)\n",
    "\n",
    "    # â€”â€” Result Interpretation â€”â€”\n",
    "    prediction = outputs['predictions']\n",
    "    confidence = outputs['confidence']\n",
    "    print(f\"  - Predicted Translation Efficiency: {prediction} (Confidence: {confidence:.2f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c8fd227",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on translation efficiency prediction with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end application of genomic deep learning, demonstrating how the concepts from the **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)** apply to a real biological problem. Specifically, you have:\n",
    "\n",
    "1. **Applied Task Formulation**: Successfully framed translation efficiency prediction as a sequence classification problem\n",
    "2. **Mastered the 4-Step Workflow**:\n",
    "   - **Step 1: Data Preparation**: Acquired, processed, and loaded the rice translation efficiency dataset\n",
    "   - **Step 2: Model Initialization**: Set up OmniModelForSequenceClassification for binary classification\n",
    "   - **Step 3: Model Training**: Fine-tuned the model using best practices and appropriate evaluation metrics\n",
    "   - **Step 4: Model Inference**: Generated predictions on new mRNA sequences and interpreted results\n",
    "3. **Understood Practical Application**: Gained hands-on experience with a biologically relevant prediction task\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "Now that you've mastered translation efficiency prediction, you can:\n",
    "\n",
    "#### ðŸ§¬ **Explore Other Sequence Classification Tasks**\n",
    "- **Promoter Recognition**: Identify regulatory sequences\n",
    "- **Subcellular Localization**: Predict protein cellular destinations  \n",
    "- **Functional Annotation**: Classify protein or RNA functions\n",
    "\n",
    "#### ðŸ“Š **Try Different Task Types**\n",
    "- **Sequence Regression**: Gene expression level prediction\n",
    "- **Token Classification**: Binding site identification\n",
    "- **Multi-label Classification**: Multi-functional sequence prediction\n",
    "\n",
    "#### ðŸ”¬ **Advanced Techniques**\n",
    "- **Custom Dataset Creation**: Use the [Advanced Dataset Creation Tutorial](./05_advanced_dataset_creation.ipynb)\n",
    "- **Model Comparison**: Benchmark different foundation models\n",
    "- **Hyperparameter Optimization**: Fine-tune model performance\n",
    "- **Biological Validation**: Compare predictions with experimental data\n",
    "\n",
    "### ðŸ“š Resources\n",
    "\n",
    "- **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)**: Review core concepts anytime\n",
    "- **[OmniGenBench Documentation](https://omnigenbench.readthedocs.io/)**: Complete API reference\n",
    "- **[GitHub Repository](https://github.com/yangheng95/OmniGenBench)**: Source code and community discussions\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c665",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
