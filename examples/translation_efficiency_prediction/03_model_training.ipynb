{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36941386",
   "metadata": {},
   "source": [
    "# Model Training Tutorial: Fine-tuning Your Genomic AI Model ğŸ‹ï¸\n",
    "\n",
    "Welcome to the most exciting part of our tutorial series! In the previous two tutorials, we prepared our data and initialized our model. Now it's time for **Model Training**.\n",
    "\n",
    "> ğŸ’¡ **Learning Objectives**: Understand supervised learning principles, master different trainer usage, complete end-to-end model fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8d326",
   "metadata": {},
   "source": [
    "## What is Supervised Fine-tuning? ğŸ“\n",
    "\n",
    "**Supervised fine-tuning** is the process of adapting a pre-trained model to a specific labeled dataset. The model \"learns\" through the following cycle:\n",
    "\n",
    "```\n",
    "1. ğŸ“Š Make predictions on sequences in the training set\n",
    "2. ğŸ¯ Compare predictions with true labels  \n",
    "3. ğŸ“‰ Calculate \"error\" or \"loss\"\n",
    "4. ğŸ”§ Adjust internal weights to reduce future errors\n",
    "```\n",
    "\n",
    "This cycle is repeated for all sequences in the training data across multiple \"epochs\".\n",
    "\n",
    "### ğŸ§  Why is Fine-tuning So Effective?\n",
    "\n",
    "| Training Stage | Knowledge Learned | Analogy |\n",
    "|---------|------------|------|\n",
    "| **Pre-training** | General patterns of genomic language | ğŸ“š Learned to \"read\" genomes |\n",
    "| **Fine-tuning** | Task-specific specialized knowledge | ğŸ¯ Learned to \"understand\" translation efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723c2fb",
   "metadata": {},
   "source": [
    "## Training Components in OmniGenBench ğŸ”§\n",
    "\n",
    "To start training, we need to assemble several key components:\n",
    "\n",
    "### 1. ğŸ—‚ï¸ **Datasets and DataLoaders**\n",
    "Wrap our training, validation, and test data into PyTorch `DataLoader`s that efficiently provide batched data to the model.\n",
    "\n",
    "### 2. ğŸ“Š **Evaluation Metrics**\n",
    "Define how we measure success. For classification tasks, we'll use the **F1 score** which balances precision and recall.\n",
    "\n",
    "### 3. âš™ï¸ **Optimizer**\n",
    "The algorithm that updates model weights. We'll use the popular **AdamW** optimizer.\n",
    "\n",
    "### 4. ğŸ¯ **Trainer**\n",
    "OmniGenBench provides a powerful `Trainer` class that orchestrates the entire training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48740d",
   "metadata": {},
   "source": [
    "## ğŸš€ OmniGenBench Trainer Selection Guide\n",
    "\n",
    "OmniGenBench provides multiple trainer backends to meet different needs:\n",
    "\n",
    "| Trainer Type | Base Technology | Main Advantages | Best Use Cases |\n",
    "|-----------|----------|----------|------------|\n",
    "| **`Trainer`** (PyTorch Native) | Pure PyTorch | ğŸŸ¢ Transparent and understandable<br>ğŸŸ¢ Full control | ğŸ¯ Learning and understanding<br>ğŸ¯ Single GPU training |\n",
    "| **`AccelerateTrainer`** | ğŸ¤— Accelerate | ğŸŸ¡ Seamless scaling<br>ğŸŸ¡ Distributed-friendly | ğŸ¯ Multi-GPU/TPU<br>ğŸ¯ Large-scale training |\n",
    "| **`HFTrainer`** | ğŸ¤— Trainer | ğŸ”´ Feature-rich<br>ğŸ”´ Complete ecosystem | ğŸ¯ HF users<br>ğŸ¯ Standardized workflows |\n",
    "\n",
    "**In this tutorial, we use `AccelerateTrainer`** - it matches the complete tutorial exactly and provides excellent performance.\n",
    "\n",
    "> ğŸ’­ **Selection Principles**: For this tutorial, we use `AccelerateTrainer` to match the complete tutorial workflow exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c47afb",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Environment Setup and Data Preparation\n",
    "\n",
    "First, let's set up our environment and rebuild the necessary components from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install omnigenbench -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d52cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniDatasetForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2933d",
   "metadata": {},
   "source": [
    "### ğŸ“Š Configure Training Parameters\n",
    "\n",
    "Let's define all training hyperparameters. This centralized configuration matches our complete tutorial exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration - matches complete tutorial exactly\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "label2id = {\"0\": 0, \"1\": 1}  # 0: Low TE, 1: High TE\n",
    "\n",
    "print(\"ğŸ“‹ Training configuration initialized!\")\n",
    "print(f\"  ğŸ¤– Model: {config_or_model}\")\n",
    "print(f\"  ğŸ·ï¸ Labels: {label2id}\")\n",
    "print(f\"  ğŸ¯ Task: Translation Efficiency Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c29614",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ Assemble Training Components\n",
    "\n",
    "Now, let's create all the objects needed for training, exactly as in the complete tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f625be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load tokenizer - matches complete tutorial\n",
    "print(\"ğŸ”„ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config_or_model)\n",
    "print(f\"âœ… Tokenizer loaded: {config_or_model}\")\n",
    "\n",
    "# 2. Load datasets - matches complete tutorial exactly\n",
    "print(\"ğŸ“Š Loading datasets...\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name_or_path=\"translation_efficiency_prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Datasets loaded: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize model - matches complete tutorial exactly\n",
    "print(\"ğŸ¤– Initializing model...\")\n",
    "model = OmniModelForSequenceClassification(\n",
    "    config_or_model,\n",
    "    tokenizer,\n",
    "    num_labels=2,  # Binary classification: Low TE vs High TE\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Model initialized! Parameter count: {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c5998",
   "metadata": {},
   "source": [
    "## ğŸš€ Start Training with AccelerateTrainer!\n",
    "\n",
    "Now we'll use the same training approach as the complete tutorial. The `AccelerateTrainer` will automatically handle:\n",
    "\n",
    "- âœ… Moving data and model to the correct device (GPU or CPU)\n",
    "- âœ… Iterating through training data for the specified number of epochs\n",
    "- âœ… Calculating loss and updating model weights\n",
    "- âœ… Evaluating the model on the validation set after each epoch\n",
    "- âœ… Logging performance metrics\n",
    "- âœ… Saving the best-performing model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a52989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training - exactly as in complete tutorial\n",
    "print(\"ğŸš€ Setting up training with AccelerateTrainer...\")\n",
    "\n",
    "metric_functions = [ClassificationMetric().f1_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Starting training...\")\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_te_finetuned\")  # Matches complete tutorial\n",
    "\n",
    "print('Metrics:', metrics)\n",
    "print(\"\\nğŸ‰ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162ed15",
   "metadata": {},
   "source": [
    "## ğŸ¯ Understanding Training Results\n",
    "\n",
    "After training completes, you'll see metrics that help you understand your model's performance:\n",
    "\n",
    "### ğŸ“Š Key Metrics to Watch:\n",
    "- **F1 Score**: Balances precision and recall (higher is better)\n",
    "- **Accuracy**: Overall classification accuracy\n",
    "- **Loss**: How \"wrong\" the model's predictions are (lower is better)\n",
    "\n",
    "### ğŸ¯ What Good Results Look Like:\n",
    "- **F1 Score > 0.7**: Good performance\n",
    "- **F1 Score > 0.8**: Excellent performance\n",
    "- **Stable validation metrics**: Model is learning generalizable patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training summary\n",
    "print(\"ğŸ“ˆ Training Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"âœ… Model successfully fine-tuned for translation efficiency prediction\")\n",
    "print(f\"ğŸ“Š Dataset: Rice mRNA sequences with experimental TE labels\")\n",
    "print(f\"ğŸ¯ Task: Binary classification (High TE vs Low TE)\")\n",
    "print(f\"ğŸ’¾ Model saved as: 'ogb_te_finetuned'\")\n",
    "print(f\"ğŸš€ Ready for inference on new sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e47d61",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary and Next Steps\n",
    "\n",
    "ğŸ‰ Congratulations! You have successfully completed the model training tutorial. Let's review what we've accomplished:\n",
    "\n",
    "### âœ… **Skills Mastered**\n",
    "\n",
    "âœ… **Understood supervised fine-tuning**: How pre-trained models learn specific tasks  \n",
    "âœ… **Mastered the AccelerateTrainer**: Professional training with minimal code  \n",
    "âœ… **Learned training best practices**: Proper data loading, metric selection, model saving  \n",
    "âœ… **Completed end-to-end training**: From raw data to trained model  \n",
    "âœ… **Matched complete tutorial workflow**: Consistent with the main tutorial  \n",
    "\n",
    "**Your model is now \"intelligent\"!** ğŸ§ âœ¨\n",
    "\n",
    "Through fine-tuning, we have transformed a general genomic foundation model into a translation efficiency prediction specialist. This trained model has been saved and is ready for making predictions on new mRNA sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ What's Next...\n",
    "\n",
    "In the final tutorial, we will explore:\n",
    "- ğŸ”® **Model inference**: Using your trained model to predict new sequences\n",
    "- ğŸ“Š **Result interpretation**: Understanding and validating predictions\n",
    "- ğŸŒ **Deployment options**: From research to production applications\n",
    "- ğŸš€ **Real-world usage**: Applying your model to biological research\n",
    "\n",
    "**Ready to put your trained model to work?**\n",
    "\n",
    "> ğŸŠ **Milestone**: You are now a qualified genomic AI trainer!\n",
    "\n",
    "ğŸ‘‰ **Final Step**: Open [04_model_inference.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/04_model_inference.ipynb) to complete your learning journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf55d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
