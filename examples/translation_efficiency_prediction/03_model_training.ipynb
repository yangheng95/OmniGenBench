{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36941386",
   "metadata": {},
   "source": [
    "# Model Training Tutorial: Fine-tuning Your Genomic AI Model 🏋️\n",
    "\n",
    "Welcome to the most exciting part of our tutorial series! In the previous two tutorials, we prepared our data and initialized our model. Now it's time for **Model Training**.\n",
    "\n",
    "> 💡 **Learning Objectives**: Understand supervised learning principles, master different trainer usage, complete end-to-end model fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8d326",
   "metadata": {},
   "source": [
    "## What is Supervised Fine-tuning? 🎓\n",
    "\n",
    "**Supervised fine-tuning** is the process of adapting a pre-trained model to a specific labeled dataset. The model \"learns\" through the following cycle:\n",
    "\n",
    "```\n",
    "1. 📊 Make predictions on sequences in the training set\n",
    "2. 🎯 Compare predictions with true labels  \n",
    "3. 📉 Calculate \"error\" or \"loss\"\n",
    "4. 🔧 Adjust internal weights to reduce future errors\n",
    "```\n",
    "\n",
    "This cycle is repeated for all sequences in the training data across multiple \"epochs\".\n",
    "\n",
    "### 🧠 Why is Fine-tuning So Effective?\n",
    "\n",
    "| Training Stage | Knowledge Learned | Analogy |\n",
    "|---------|------------|------|\n",
    "| **Pre-training** | General patterns of genomic language | 📚 Learned to \"read\" genomes |\n",
    "| **Fine-tuning** | Task-specific specialized knowledge | 🎯 Learned to \"understand\" translation efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723c2fb",
   "metadata": {},
   "source": [
    "## Training Components in OmniGenBench 🔧\n",
    "\n",
    "To start training, we need to assemble several key components:\n",
    "\n",
    "### 1. 🗂️ **Datasets and DataLoaders**\n",
    "Wrap our training, validation, and test data into PyTorch `DataLoader`s that efficiently provide batched data to the model.\n",
    "\n",
    "### 2. 📊 **Evaluation Metrics**\n",
    "Define how we measure success. For classification tasks, we'll use the **F1 score** which balances precision and recall.\n",
    "\n",
    "### 3. ⚙️ **Optimizer**\n",
    "The algorithm that updates model weights. We'll use the popular **AdamW** optimizer.\n",
    "\n",
    "### 4. 🎯 **Trainer**\n",
    "OmniGenBench provides a powerful `Trainer` class that orchestrates the entire training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48740d",
   "metadata": {},
   "source": [
    "## 🚀 OmniGenBench Trainer Selection Guide\n",
    "\n",
    "OmniGenBench provides multiple trainer backends to meet different needs:\n",
    "\n",
    "| Trainer Type | Base Technology | Main Advantages | Best Use Cases |\n",
    "|-----------|----------|----------|------------|\n",
    "| **`Trainer`** (PyTorch Native) | Pure PyTorch | 🟢 Transparent and understandable<br>🟢 Full control | 🎯 Learning and understanding<br>🎯 Single GPU training |\n",
    "| **`AccelerateTrainer`** | 🤗 Accelerate | 🟡 Seamless scaling<br>🟡 Distributed-friendly | 🎯 Multi-GPU/TPU<br>🎯 Large-scale training |\n",
    "| **`HFTrainer`** | 🤗 Trainer | 🔴 Feature-rich<br>🔴 Complete ecosystem | 🎯 HF users<br>🎯 Standardized workflows |\n",
    "\n",
    "**In this tutorial, we use `AccelerateTrainer`** - it matches the complete tutorial exactly and provides excellent performance.\n",
    "\n",
    "> 💭 **Selection Principles**: For this tutorial, we use `AccelerateTrainer` to match the complete tutorial workflow exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c47afb",
   "metadata": {},
   "source": [
    "### 🛠️ Environment Setup and Data Preparation\n",
    "\n",
    "First, let's set up our environment and rebuild the necessary components from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install omnigenbench -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d52cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniDatasetForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2933d",
   "metadata": {},
   "source": [
    "### 📊 Configure Training Parameters\n",
    "\n",
    "Let's define all training hyperparameters. This centralized configuration matches our complete tutorial exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration - matches complete tutorial exactly\n",
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "label2id = {\"0\": 0, \"1\": 1}  # 0: Low TE, 1: High TE\n",
    "\n",
    "print(\"📋 Training configuration initialized!\")\n",
    "print(f\"  🤖 Model: {config_or_model}\")\n",
    "print(f\"  🏷️ Labels: {label2id}\")\n",
    "print(f\"  🎯 Task: Translation Efficiency Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c29614",
   "metadata": {},
   "source": [
    "### 🏗️ Assemble Training Components\n",
    "\n",
    "Now, let's create all the objects needed for training, exactly as in the complete tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f625be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load tokenizer - matches complete tutorial\n",
    "print(\"🔄 Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config_or_model)\n",
    "print(f\"✅ Tokenizer loaded: {config_or_model}\")\n",
    "\n",
    "# 2. Load datasets - matches complete tutorial exactly\n",
    "print(\"📊 Loading datasets...\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name_or_path=\"translation_efficiency_prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"📊 Datasets loaded: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize model - matches complete tutorial exactly\n",
    "print(\"🤖 Initializing model...\")\n",
    "model = OmniModelForSequenceClassification(\n",
    "    config_or_model,\n",
    "    tokenizer,\n",
    "    num_labels=2,  # Binary classification: Low TE vs High TE\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✅ Model initialized! Parameter count: {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c5998",
   "metadata": {},
   "source": [
    "## 🚀 Start Training with AccelerateTrainer!\n",
    "\n",
    "Now we'll use the same training approach as the complete tutorial. The `AccelerateTrainer` will automatically handle:\n",
    "\n",
    "- ✅ Moving data and model to the correct device (GPU or CPU)\n",
    "- ✅ Iterating through training data for the specified number of epochs\n",
    "- ✅ Calculating loss and updating model weights\n",
    "- ✅ Evaluating the model on the validation set after each epoch\n",
    "- ✅ Logging performance metrics\n",
    "- ✅ Saving the best-performing model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a52989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training - exactly as in complete tutorial\n",
    "print(\"🚀 Setting up training with AccelerateTrainer...\")\n",
    "\n",
    "metric_functions = [ClassificationMetric().f1_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    ")\n",
    "\n",
    "print(\"🎓 Starting training...\")\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_te_finetuned\")  # Matches complete tutorial\n",
    "\n",
    "print('Metrics:', metrics)\n",
    "print(\"\\n🎉 Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162ed15",
   "metadata": {},
   "source": [
    "## 🎯 Understanding Training Results\n",
    "\n",
    "After training completes, you'll see metrics that help you understand your model's performance:\n",
    "\n",
    "### 📊 Key Metrics to Watch:\n",
    "- **F1 Score**: Balances precision and recall (higher is better)\n",
    "- **Accuracy**: Overall classification accuracy\n",
    "- **Loss**: How \"wrong\" the model's predictions are (lower is better)\n",
    "\n",
    "### 🎯 What Good Results Look Like:\n",
    "- **F1 Score > 0.7**: Good performance\n",
    "- **F1 Score > 0.8**: Excellent performance\n",
    "- **Stable validation metrics**: Model is learning generalizable patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training summary\n",
    "print(\"📈 Training Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"✅ Model successfully fine-tuned for translation efficiency prediction\")\n",
    "print(f\"📊 Dataset: Rice mRNA sequences with experimental TE labels\")\n",
    "print(f\"🎯 Task: Binary classification (High TE vs Low TE)\")\n",
    "print(f\"💾 Model saved as: 'ogb_te_finetuned'\")\n",
    "print(f\"🚀 Ready for inference on new sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e47d61",
   "metadata": {},
   "source": [
    "## 🎯 Summary and Next Steps\n",
    "\n",
    "🎉 Congratulations! You have successfully completed the model training tutorial. Let's review what we've accomplished:\n",
    "\n",
    "### ✅ **Skills Mastered**\n",
    "\n",
    "✅ **Understood supervised fine-tuning**: How pre-trained models learn specific tasks  \n",
    "✅ **Mastered the AccelerateTrainer**: Professional training with minimal code  \n",
    "✅ **Learned training best practices**: Proper data loading, metric selection, model saving  \n",
    "✅ **Completed end-to-end training**: From raw data to trained model  \n",
    "✅ **Matched complete tutorial workflow**: Consistent with the main tutorial  \n",
    "\n",
    "**Your model is now \"intelligent\"!** 🧠✨\n",
    "\n",
    "Through fine-tuning, we have transformed a general genomic foundation model into a translation efficiency prediction specialist. This trained model has been saved and is ready for making predictions on new mRNA sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 What's Next...\n",
    "\n",
    "In the final tutorial, we will explore:\n",
    "- 🔮 **Model inference**: Using your trained model to predict new sequences\n",
    "- 📊 **Result interpretation**: Understanding and validating predictions\n",
    "- 🌐 **Deployment options**: From research to production applications\n",
    "- 🚀 **Real-world usage**: Applying your model to biological research\n",
    "\n",
    "**Ready to put your trained model to work?**\n",
    "\n",
    "> 🎊 **Milestone**: You are now a qualified genomic AI trainer!\n",
    "\n",
    "👉 **Final Step**: Open [04_model_inference.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/04_model_inference.ipynb) to complete your learning journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf55d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
