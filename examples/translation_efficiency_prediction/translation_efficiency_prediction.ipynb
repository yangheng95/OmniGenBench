{"cells":[{"metadata":{"id":"37f815534558f161"},"cell_type":"markdown","source":["# Translation Efficiency Prediction with OmniGenBench\n","\n","This notebook demonstrates how to fine-tune a Genomic Foundation Model (GFM) to predict Rice translation efficiency (TE) from mRNA sequences using OmniGenBench. The example focusing on plant RNA translation efficiency.\n","\n","**Background (PlantRNA-FM):**\n","PlantRNA-FM (\"An interpretable RNA foundation model for exploring functional RNA motifs in plants\") introduces an RNA foundation model tailored to plant genomics, highlighting interpretability and motif discovery capabilities. TE prediction is a representative downstream task where models learn sequence determinants associated with efficient translation. In this demo, we use a small rice RNA [`translation efficiency dataset`](https://huggingface.co/datasets/yangheng/translation_efficiency_prediction) to illustrate fine-tuning and evaluation within OmniGenBench.\n","\n","- **Task type**: Binary sequence classification (High-TE vs Low-TE)\n","- **Input**: RNA sequences (string), up to a configurable `max_length`\n","- **Label space**: {0: Low-TE, 1: High-TE}\n","\n","**Estimated runtime:** On a single NVIDIA RTX 4090, a short training run on this toy dataset typically takes ~10–30 minutes depending on epochs/model size.\n","\n","##  Notebook Structure\n","\n","This notebook is organized into concise sections. Most core logic is moved to [`examples/translation_efficiency_prediction/utils.py`](https://github.com/COLA-Laboratory/OmniGenBench/blob/master/examples/translation_efficiency_prediction/utils.py) and imported here:\n","\n","1. **Setup & Installation**: Install optional dependencies.\n","2. **Import Libraries**: Load Python packages and local utilities.\n","3. **Configuration & Data**: Choose a model, set hyperparameters, and point to dataset files.\n","4. **Main Analysis Pipeline**: Run training/evaluation with `utils.run_training`.\n","5. **Results Overview**: Summarize validation/test metrics.\n","6. **Visualization**: Plot validation metrics across epochs.\n","7. **References**: Link to PlantRNA-FM."],"id":"37f815534558f161"},{"metadata":{"id":"9ceb2a8181e7820e"},"cell_type":"markdown","source":["## 1. Setup & Installation\n","\n","This section handles the initial setup, including installing necessary packages. If dependencies are already available, you can skip the installation cell.\n","\n","### Install Dependencies\n","Uncomment and run the following cell to install the required packages."],"id":"9ceb2a8181e7820e"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T15:14:19.037993Z","start_time":"2025-09-11T15:14:19.033248Z"},"id":"2fdadda994b56c9c"},"cell_type":"code","source":["# Optional: install dependencies if not available\n","!pip install torch transformers pandas autocuda multimolecule biopython scipy scikit-learn tqdm dill findfile requests omnigenbench -U\n"],"id":"2fdadda994b56c9c","outputs":[],"execution_count":null},{"metadata":{"id":"368aae70b71a4b9b"},"cell_type":"markdown","source":["## 2. Import Libraries\n","Import all necessary libraries for data processing, model training, and analysis."],"id":"368aae70b71a4b9b"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T15:14:31.275795Z","start_time":"2025-09-11T15:14:19.061709Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"d0014e45e882422d","executionInfo":{"status":"ok","timestamp":1757613340607,"user_tz":-60,"elapsed":18349,"user":{"displayName":"Heng Yang","userId":"05057657490446851168"}},"outputId":"75f57cb8-9def-42be-c387-90558aafdade"},"cell_type":"code","source":["import warnings\n","import importlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import torch, requests\n","from autocuda import auto_cuda\n","\n","# download utils.py, see GitHub for detailed codes at: https://github.com/COLA-Laboratory/OmniGenBench/tree/master/examples/translation_efficiency_prediction\n","url = \"https://huggingface.co/datasets/yangheng/translation_efficiency_prediction/resolve/main/utils.py\"\n","response = requests.get(url)\n","with open(\"utils.py\", \"wb\") as f:\n","    f.write(response.content)\n","\n","\n","# Import utilities from the local utils.py file\n","utils_spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n","utils = importlib.util.module_from_spec(utils_spec)\n","utils_spec.loader.exec_module(utils)\n","\n","warnings.filterwarnings('ignore')\n","print(\"Libraries imported successfully!\")\n"],"id":"d0014e45e882422d","outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","   **  +----------- **           ___                     _\n","  @@                 @@         / _ \\  _ __ ___   _ __  (_)\n"," @@* #============== *@@       | | | || '_ ` _ \\ | '_ \\ | |\n"," @@*                 *@@       | |_| || | | | | || | | || |\n"," *@@  +------------ *@@         \\___/ |_| |_| |_||_| |_||_|\n","  *@*               @@*\n","   *@@  #========= @@*\n","    *@@*         *@@*\n","      *@@  +---@@@*              ____\n","        *@@*   **               / ___|  ___  _ __\n","          **@**                | |  _  / _ \\| '_ \\\n","        *@@* *@@*              | |_| ||  __/| | | |\n","      *@@ ---+  @@*             \\____| \\___||_| |_|\n","    *@@*         *@@*\n","   *@@ =========#  @@*\n","  *@@               @@*\n"," *@@ -------------+  @@*        ____                      _\n"," @@                   @@       | __ )   ___  _ __    ___ | |__\n"," @@ ===============#  @@       |  _ \\  / _ \\| '_ \\  / __|| '_ \\\n","  @@                 @@        | |_) ||  __/| | | || (__ | | | |\n","   ** -----------+  **         |____/  \\___||_| |_| \\___||_| |_|\n","\n","Libraries imported successfully!\n"]}],"execution_count":1},{"metadata":{"id":"59584089ed4d0003"},"cell_type":"markdown","source":["## 3. Configuration & Data\n","\n","Set up the analysis parameters, file paths, and model selection here. You can easily change the `MODEL_NAME` to test different GFMs.\n","\n","### Model Selection\n","Choose the Genomic Foundation Model to fine-tune."],"id":"59584089ed4d0003"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T15:14:31.414310Z","start_time":"2025-09-11T15:14:31.406256Z"},"id":"547e026cafd913b5"},"cell_type":"code","source":["# Using utils for reusable logic\n","from utils import run_finetuning\n","print(\"Core classes and functions imported from utils.\")\n","\n","# --- Available Models for Testing ---\n","AVAILABLE_MODELS = [\n","    'yangheng/OmniGenome-52M',\n","    # 'yangheng/OmniGenome-186M',\n","    # 'yangheng/OmniGenome-v1.5',\n","]\n","MODEL_NAME = AVAILABLE_MODELS[0]  # Model to use for predictions\n","print(f\"Selected model: {MODEL_NAME}\")\n"],"id":"547e026cafd913b5","outputs":[],"execution_count":null},{"metadata":{"id":"8a9859d376ae79da"},"cell_type":"markdown","source":["### Hyperparameter and Dataset Configuration\n","Define the training hyperparameters and paths to the dataset files."],"id":"8a9859d376ae79da"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T15:14:32.154246Z","start_time":"2025-09-11T15:14:31.439878Z"},"id":"e266997994ee0a88"},"cell_type":"code","source":["import findfile\n","\n","# --- Training Hyperparameters ---\n","EPOCHS = 10\n","LEARNING_RATE = 2e-5\n","WEIGHT_DECAY = 1e-5\n","BATCH_SIZE = 4\n","MAX_LENGTH = 512\n","SEED = 42\n","\n","# --- Dataset Configuration ---\n","LOCAL_PATH = \"te_rice_dataset\"  # Local directory for the dataset\n","from utils import download_te_dataset\n","download_te_dataset(LOCAL_PATH)  # Download the VEP dataset if not already available\n","\n","# --- Dataset Configuration ---\n","TRAIN_FILE = findfile.find_cwd_file(\"train.json\")  # training split\n","VALID_FILE = findfile.find_cwd_file(\"valid.json\")  # validation split (optional)\n","TEST_FILE = findfile.find_cwd_file(\"test.json\")  # test split\n","# --- Label Mapping ---\n","# The task is binary classification: 1 for TE, 0 for non-TE.\n","LABEL2ID = {\"0\": 0, \"1\": 1}\n","\n","print(f\"Selected model: {MODEL_NAME}\")\n"],"id":"e266997994ee0a88","outputs":[],"execution_count":null},{"metadata":{"id":"e332d80578aba4ed"},"cell_type":"markdown","source":["## 4. Main Analysis Pipeline\n","\n","This section executes the training/evaluation pipeline using the configuration defined above. The `run_training` function from `examples/translation_efficiency_prediction/utils.py` orchestrates tokenization, dataset creation, training, and evaluation.\n","### Note\n","You can skip this section and load the pre-finetuned model in the following cells."],"id":"e332d80578aba4ed"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T16:15:25.616747Z","start_time":"2025-09-11T15:14:32.162274Z"},"id":"ca6e91fa0cb64ec6"},"cell_type":"code","source":["# Import main pipeline from utils for a concise demo\n","from utils import run_finetuning\n","\n","print(\"Main analysis pipeline imported from utils.\")\n","\n","print(\"=\" * 50)\n","\n","# Run the analysis\n","metrics = run_finetuning(\n","\tmodel_name=MODEL_NAME,\n","\ttrain_file=TRAIN_FILE,\n","\tvalid_file=VALID_FILE,\n","\ttest_file=TEST_FILE,\n","\tlabel2id=LABEL2ID,\n","\tepochs=EPOCHS,\n","\tlearning_rate=LEARNING_RATE,\n","\tweight_decay=WEIGHT_DECAY,\n","\tbatch_size=BATCH_SIZE,\n","\tmax_length=MAX_LENGTH,\n","\tseed=SEED,\n",")\n","\n","print(\"=\" * 50)\n","print(\"Analysis completed!\")\n","\n","# Print final validation and test metrics (if available)\n","if metrics.get('valid'):\n","\tprint(\"\\nValidation Set Performance (last epoch):\")\n","\tfor key, value in metrics['valid'][-1].items():\n","\t\tprint(f\"{key}: {value:.4f}\")\n","\n","if metrics.get('test'):\n","\tprint(\"\\nTest Set Performance (best checkpoint):\")\n","\tfor key, value in metrics['test'][-1].items():\n","\t\tprint(f\"{key}: {value:.4f}\")\n"],"id":"ca6e91fa0cb64ec6","outputs":[],"execution_count":null},{"metadata":{"id":"824e800bc61706aa"},"cell_type":"markdown","source":["## 5. Inference  Example\n","This section demonstrates how to run inference on a single sequence using the fine-tuned model. The `encode_tokens` function ensures the same preprocessing as during training."],"id":"824e800bc61706aa"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T16:15:35.089656Z","start_time":"2025-09-11T16:15:25.686007Z"},"id":"f4eaf486034752c8","outputId":"1f9db9cb-c818-4094-a7b3-6b31a5375dab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757613408872,"user_tz":-60,"elapsed":13273,"user":{"displayName":"Heng Yang","userId":"05057657490446851168"}}},"cell_type":"code","source":["from omnigenbench import ModelHub\n","from autocuda import auto_cuda\n","\n","model = ModelHub.load(\"yangheng/ogb_te_finetuned\").to(auto_cuda())\n","\n","# Example sequence for inference\n","sample_sequence1 = \"AAACCAACAAAATGCAGTAGAAGTACTCTCGAGCTATAGTCGCGACGTGCTGCCCCGCAGGAGTACAGTAGTAGTACAACGTAAGCGGGAGCAACAGACTCCCCCCCTGCAACCCACTGTGCCTGTGCCCTCGACGCGTCTCCGTCGCTTTGGCAAATGTCACGTACATATTACCGTCTCAGGCTCTCAGCCATGCTCCCTACCACCCCTGCAGCGAAGCAAAAGCCACGCACGCGGCGCCTGACATGTAACAGGACTAGACCATCTTGTTCATTTCCCGCACCCCCTCCTCTCCTCTTCCTCCATCTGCCTCTTTAAAACAGTAAAAATAACCGTGCATCCCCTGGGCAAAATCTCTCCCATACATACACTACAGCGGCGAACCTTTCCTTATTCTCGCAACGCCTCGGTAACGGGCAGCGCCTGCTCCGCGCCGCGGTTGCGAGTTCGGGAAGGCGGCCGGAGTCGCGGGGAGGAGAGGGAGGATTCGATCGGCCAGA\"  # High-TE sequence\n","\n","sample_sequence2 = \"TGGAGATGGGCAGATGGCACACAAAACATGAATAGAAAACCCAAAAGGAAGGATGAAAAAAACACACACACACACACACACAAAACACAGAGAGAGAGAGAGAGAGAGAGCGAGAAAAGAAAAGAAAAAACCAATTCTTTTGGTCTCTTCCCTCTCCGTTTGTCGTGTCGAAGCCTTTGCCCCCACCACCTCCTCCTCTCCTCTCCCTTCCTCCCCTCCTCCCCATCTCGCTCTCCTCCCTCCTCTCTCCTCTCCTCGTCTCCTCTTCCTCTCCATTCCATTGGCCATTCCATTCCATTCCACCCCCCATGAAACCCCAAACCCTCGTCGGCCTCGCCGCGCTCGCGTAGCGCACCCGCCCTTCTCCTCTCGCCGGTGGTCCGCCGCCAGCCTCCCCCCACCCGATCCCGCCGCCCCCCCCGCCTTCACCCCGCCCACGCGGACGCATCCGATCCCGCCGCATCGCCGCGCGGGGGGGGGGGGGGGGGGGGGAGGGCACG \"  # Low-TE sequence\n","\n","# Run inference on the sample sequences\n","outputs = model.inference(sample_sequence1)\n","print(f\"Sample sequence 1 prediction: {outputs['predictions']}\")\n","outputs = model.inference(sample_sequence2)\n","print(f\"Sample sequence 2 prediction: {outputs['predictions']}\")\n","\n"],"id":"f4eaf486034752c8","outputs":[{"output_type":"stream","name":"stdout","text":["[2025-09-11 17:56:36.695] [omnigenbench 0.3.12alpha]  Model yangheng/ogb_te_finetuned already exists locally at __OMNIGENOME_DATA__/models/yangheng--ogb_te_finetuned\n","[2025-09-11 17:56:36.700] [omnigenbench 0.3.12alpha]  Cloned model from Hugging Face Hub to: __OMNIGENOME_DATA__/models/yangheng--ogb_te_finetuned\n","[2025-09-11 17:56:36.712] [omnigenbench 0.3.12alpha]  Loaded metadata from: __OMNIGENOME_DATA__/models/yangheng--ogb_te_finetuned/metadata.json\n","[2025-09-11 17:56:49.293] [omnigenbench 0.3.12alpha]  Model Name: OmniModelForSequenceClassification\n","Model Metadata: {'library_name': 'omnigenbench', 'omnigenbench_version': '0.3.12alpha', 'torch_version': '2.8.0+cu126+cu12.6+gita1cb3cc05d46d198467bebbb6e8fba50a325d4e7', 'transformers_version': '4.56.1', 'model_cls': 'OmniModelForSequenceClassification', 'tokenizer_cls': 'EsmTokenizer', 'model_name': 'OmniModelForSequenceClassification'}\n","Base Model Name: __OMNIGENOME_DATA__/models/yangheng--ogb_te_finetuned\n","Model Type: omnigenome\n","Model Architecture: ['OmniGenomeModel']\n","Model Parameters: 185.886801 M\n","Model Config: OmniGenomeConfig {\n","  \"OmniGenomefold_config\": null,\n","  \"architectures\": [\n","    \"OmniGenomeModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"configuration_omnigenome.OmniGenomeConfig\",\n","    \"AutoModel\": \"modeling_omnigenome.OmniGenomeModel\",\n","    \"AutoModelForMaskedLM\": \"modeling_omnigenome.OmniGenomeForMaskedLM\",\n","    \"AutoModelForSeq2SeqLM\": \"modeling_omnigenome.OmniGenomeForSeq2SeqLM\",\n","    \"AutoModelForSequenceClassification\": \"modeling_omnigenome.OmniGenomeForSequenceClassification\",\n","    \"AutoModelForTokenClassification\": \"modeling_omnigenome.OmniGenomeForTokenClassification\"\n","  },\n","  \"classifier_dropout\": null,\n","  \"dtype\": \"float16\",\n","  \"emb_layer_norm_before\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 720,\n","  \"id2label\": {\n","    \"0\": \"0\",\n","    \"1\": \"1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2560,\n","  \"is_folding_model\": false,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"mask_token_id\": 23,\n","  \"max_position_embeddings\": 1026,\n","  \"metadata\": {\n","    \"library_name\": \"omnigenbench\",\n","    \"loss_fn_class\": \"CrossEntropyLoss\",\n","    \"loss_fn_module\": \"torch.nn.modules.loss\",\n","    \"model_cls\": \"OmniModelForSequenceClassification\",\n","    \"model_name\": \"OmniModelForSequenceClassification\",\n","    \"omnigenbench_version\": \"0.3.11alpha2\",\n","    \"tokenizer_cls\": \"EsmTokenizer\",\n","    \"torch_version\": \"2.8.0+cu129+cu12.9+gita1cb3cc05d46d198467bebbb6e8fba50a325d4e7\",\n","    \"transformers_version\": \"4.56.1\"\n","  },\n","  \"model_type\": \"omnigenome\",\n","  \"num_attention_heads\": 30,\n","  \"num_generation\": 50,\n","  \"num_hidden_layers\": 32,\n","  \"num_population\": 100,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"rotary\",\n","  \"token_dropout\": true,\n","  \"transformers_version\": \"4.56.1\",\n","  \"use_cache\": true,\n","  \"vocab_list\": null,\n","  \"vocab_size\": 24\n","}\n","\n","\n","[2025-09-11 17:56:49.296] [omnigenbench 0.3.12alpha]  Loading state dict from: __OMNIGENOME_DATA__/models/yangheng--ogb_te_finetuned/pytorch_model.bin\n","[2025-09-11 17:56:49.692] [omnigenbench 0.3.12alpha]  No device is specified, the model will be loaded to the default device: cuda:0\n","Sample sequence 1 prediction: 1\n","Sample sequence 2 prediction: 0\n"]}],"execution_count":3},{"metadata":{"id":"10df7a1e8c5c29a3"},"cell_type":"markdown","source":["## 6. Visualization\n","\n","In this section, we visualize validation metrics across epochs to assess learning dynamics.\n","\n","### Plot Validation Curves\n","We plot macro F1 across epochs. Additional metrics (e.g., MCC) can be added if enabled in `utils.py` or dataset config.\n"],"id":"10df7a1e8c5c29a3"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T16:15:35.211913Z","start_time":"2025-09-11T16:15:35.199731Z"},"id":"d7da5f9f72240be1"},"cell_type":"code","source":["# Results Overview — Quick summary of metrics\n","if metrics.get('valid'):\n","\tprint('Validation (last epoch):')\n","\tfor k, v in metrics['valid'][-1].items():\n","\t\tprint(f\"{k}: {v:.4f}\")\n","\n","if metrics.get('test'):\n","\tprint('\\nTest (best checkpoint):')\n","\tfor k, v in metrics['test'][-1].items():\n","\t\tprint(f\"{k}: {v:.4f}\")\n"],"id":"d7da5f9f72240be1","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2025-09-11T16:15:35.492896Z","start_time":"2025-09-11T16:15:35.220531Z"},"id":"8c1f58a326b90338"},"cell_type":"code","source":["# Visualization — Plot validation curve\n","valid_key = 'valid' if 'valid' in metrics else ('eval' if 'eval' in metrics else None)\n","if valid_key is None:\n","\tprint(\"No validation metrics found for plotting.\")\n","else:\n","\tvalid_df = pd.DataFrame(metrics[valid_key])\n","\tplt.style.use('seaborn-v0_8-whitegrid')\n","\tfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n","\tif 'f1_score' in valid_df.columns:\n","\t\tsns.lineplot(data=valid_df, x=valid_df.index, y='f1_score', ax=ax, label='Validation F1 (Macro)')\n","\t\tax.set_ylabel('F1 Score')\n","\telif 'matthews_corrcoef' in valid_df.columns:\n","\t\tsns.lineplot(data=valid_df, x=valid_df.index, y='matthews_corrcoef', ax=ax, label='Validation MCC')\n","\t\tax.set_ylabel('MCC')\n","\telse:\n","\t\tfirst_col = [c for c in valid_df.columns if isinstance(valid_df[c].iloc[-1], (int, float))]\n","\t\tif first_col:\n","\t\t\tsns.lineplot(data=valid_df, x=valid_df.index, y=first_col[0], ax=ax, label=first_col[0])\n","\t\t\tax.set_ylabel(first_col[0])\n","\t\telse:\n","\t\t\tprint(\"Validation metrics exist but no numeric columns to plot.\")\n","\tax.set_title('Validation Metric across Epochs')\n","\tax.set_xlabel('Epoch')\n","\tax.legend()\n","\tplt.tight_layout()\n","\tplt.show()"],"id":"8c1f58a326b90338","outputs":[],"execution_count":null},{"metadata":{"id":"f03109d6a613f0a"},"cell_type":"markdown","source":["## References\n","\n","- PlantRNA-FM: \"An interpretable RNA foundation model for exploring functional RNA motifs in plants\" (Nature Machine Intelligence, 2024).\n"],"id":"f03109d6a613f0a"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}