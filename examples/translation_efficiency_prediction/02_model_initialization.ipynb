{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14e2670",
   "metadata": {},
   "source": [
    "# Model Initialization Tutorial: Setting Up PlantRNA-FM for Plant Genomics ğŸ¤–\n",
    "\n",
    "Welcome to the second part of our tutorial series! In the previous tutorial, we prepared our data with the enhanced OmniDataset framework. Now we'll focus on **Model Initialization** - setting up the pre-trained **PlantRNA-FM** (Plant RNA Foundation Model) for our rice translation efficiency prediction task.\n",
    "\n",
    "> ğŸ’¡ **Learning Objectives**: Understand PlantRNA-FM and plant-specialized foundation models, master model initialization patterns, and configure models for plant genomic tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding PlantRNA-FM: A Plant-Specialized Foundation Model ğŸ§¬\n",
    "\n",
    "**PlantRNA-FM** represents a new generation of specialized foundation models for plant biology. Unlike general genomic models, PlantRNA-FM is specifically trained on plant RNA and genomic data, learning the unique patterns of plant gene regulation, codon usage, and RNA structure.\n",
    "\n",
    "### ğŸ”¬ From Language to Plant Life\n",
    "\n",
    "| Concept | Language Models | PlantRNA-FM |\n",
    "|---------|----------------|---------------------------|\n",
    "| **Vocabulary** | Words, punctuation | A, U, C, G (plant RNA) + regulatory elements |\n",
    "| **Grammar** | Syntax, semantics | Plant codon usage, UTR structures, splicing signals |\n",
    "| **Context** | Sentence meaning | Plant genomic context, expression regulation |\n",
    "| **Training Data** | Books, web text | Plant transcriptomes, genomes, expression data |\n",
    "| **Tasks** | Translation, summarization | TE prediction, gene regulation, RNA design |\n",
    "\n",
    "### ğŸ¯ The Power of Plant-Specific Pre-training\n",
    "\n",
    "**PlantRNA-FM** (published in *Nature Machine Intelligence*) has been pre-trained on:\n",
    "- ğŸŒ± **Extensive plant RNA data**: Transcriptomes from major crop species (rice, maize, wheat, Arabidopsis)\n",
    "- ğŸ“Š **Plant-specific features**: Alternative splicing patterns, UTR regulatory elements, polycistronic transcripts\n",
    "- ğŸ”¬ **Expression contexts**: Tissue-specific expression, stress responses, developmental stages\n",
    "- ğŸ’¡ **Efficient architecture**: Only 35M parameters, yet achieves state-of-the-art performance\n",
    "\n",
    "This specialized pre-training allows PlantRNA-FM to understand:\n",
    "- **Plant codon bias**: Species-specific codon usage preferences that affect translation\n",
    "- **UTR regulatory elements**: 5' and 3' UTR structures critical for plant mRNA stability and translation\n",
    "- **Plant-specific splicing**: Alternative splicing patterns unique to plant gene regulation\n",
    "- **GC content effects**: How GC-rich regions affect plant mRNA secondary structure and ribosome binding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06f8c8",
   "metadata": {},
   "source": [
    "## OmniGenBench Model Architecture ğŸ—ï¸\n",
    "\n",
    "The OmniGenBench framework provides specialized model classes for different genomic tasks. For plant genomics, **PlantRNA-FM** serves as the foundation, with task-specific heads for different applications:\n",
    "\n",
    "### ğŸ¯ Classification Models for Plant Genomics\n",
    "\n",
    "| Model Class | Use Case | Plant Applications |\n",
    "|-------------|----------|---------------------|\n",
    "| **`OmniModelForSequenceClassification`** | Binary/Multi-class sequence labeling | Translation efficiency (our task), promoter detection, gene function prediction |\n",
    "| **`OmniModelForMultiLabelSequenceClassification`** | Multiple labels per sequence | Transcription factor binding, regulatory element prediction |\n",
    "| **`OmniModelForTokenClassification`** | Per-nucleotide labeling | Splice site detection, m6A modification sites |\n",
    "\n",
    "### ğŸ“Š Regression Models for Plant Traits\n",
    "\n",
    "| Model Class | Use Case | Plant Applications |\n",
    "|-------------|----------|---------------------|\n",
    "| **`OmniModelForSequenceRegression`** | Continuous sequence values | Expression levels, translation efficiency scores, protein abundance |\n",
    "| **`OmniModelForTokenRegression`** | Per-nucleotide continuous values | RNA accessibility scores, ribosome density profiling |\n",
    "\n",
    "### ğŸ§¬ Specialized Models for Plant RNA\n",
    "\n",
    "| Model Class | Use Case | Plant Applications |\n",
    "|-------------|----------|---------------------|\n",
    "| **`OmniModelForMLM`** | Masked language modeling | Variant effect prediction, sequence completion |\n",
    "| **`OmniModelForSeq2Seq`** | Sequence transformation | RNA secondary structure prediction |\n",
    "| **`OmniModelForRNADesign`** | Sequence optimization | Synthetic gene design for plant transformation |\n",
    "\n",
    "> ğŸ¯ **For our rice translation efficiency task**: We use `OmniModelForSequenceClassification` with **PlantRNA-FM** because we're classifying each plant mRNA sequence as \"High TE\" (1) or \"Low TE\" (0), leveraging plant-specific codon usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44f774",
   "metadata": {},
   "source": [
    "## Environment Setup ğŸ”§\n",
    "\n",
    "Let's start by installing the required packages and importing the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f11d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install omnigenbench -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fce259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniDatasetForSequenceClassification,\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ¯ GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fd095",
   "metadata": {},
   "source": [
    "## Model Initialization: PlantRNA-FM for Plant Translation Efficiency ğŸš€\n",
    "\n",
    "The beauty of the OmniGenBench framework lies in its simplicity. Model initialization with PlantRNA-FM follows a consistent pattern regardless of the specific plant genomics task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afec69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Using PlantRNA-FM for rice translation efficiency\n",
    "model_name_or_path = \"yangheng/PlantRNA-FM\"  # Plant RNA Foundation Model\n",
    "label2id = {\"0\": 0, \"1\": 1}  # 0: Low TE, 1: High TE\n",
    "\n",
    "print(\"âš™ï¸ Configuration:\")\n",
    "print(f\"   ğŸŒ± Base model: {model_name_or_path} (Plant-specialized)\")\n",
    "print(f\"   ğŸ·ï¸ Label mapping: {label2id}\")\n",
    "print(f\"   ğŸ§¬ Organism: Rice (Oryza sativa)\")\n",
    "print(f\"   ğŸ“Š Task: Binary sequence classification (Translation Efficiency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c05d53",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the Tokenizer ğŸ“\n",
    "\n",
    "The tokenizer converts biological sequences into numerical tokens that the model can process. **It's crucial to use the exact same tokenizer that was used during pre-training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b64fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer - must match the pre-trained model\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "print(f\"âœ… Tokenizer loaded: {model_name_or_path}\")\n",
    "\n",
    "# Demonstrate tokenizer functionality\n",
    "sample_sequence = \"AUGCUGCUAUGCUA\"  # Sample mRNA sequence\n",
    "tokens = tokenizer(sample_sequence, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"\\nğŸ§¬ Sample tokenization:\")\n",
    "print(f\"   Input sequence: {sample_sequence}\")\n",
    "print(f\"   Tokenized IDs: {tokens['input_ids'].squeeze().tolist()}\")\n",
    "print(f\"   Sequence length: {len(tokens['input_ids'].squeeze())} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630c380",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Model ğŸ¤–\n",
    "\n",
    "Now we'll initialize our sequence classification model. The model automatically loads the pre-trained weights and adds a classification head for our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced68e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model - exactly as in the complete tutorial\n",
    "model = OmniModelForSequenceClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer,\n",
    "    num_labels=2,  # Binary classification: Low TE vs High TE\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– Model initialized successfully!\")\n",
    "print(f\"   ğŸ“Š Task type: Sequence Classification\")\n",
    "print(f\"   ğŸ·ï¸ Number of labels: 2 (Low TE, High TE)\")\n",
    "print(f\"   ğŸ§¬ Base architecture: OmniGenome Transformer\")\n",
    "\n",
    "# Display model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Model Statistics:\")\n",
    "print(f\"   ğŸ”¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   ğŸ“ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   ğŸ’¾ Model size: ~{total_params * 4 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc870e2d",
   "metadata": {},
   "source": [
    "## Understanding Model Components ğŸ”\n",
    "\n",
    "Let's explore what makes up our initialized model. Understanding these components helps you appreciate the sophistication of modern genomic AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5edb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model architecture\n",
    "print(\"ğŸ—ï¸ Model Architecture Components:\")\n",
    "print(\"\\nğŸ“Š Core Components:\")\n",
    "\n",
    "# Check if model has expected components\n",
    "if hasattr(model, 'model'):\n",
    "    print(\"   âœ… Base Transformer Model (pre-trained OmniGenome)\")\n",
    "if hasattr(model, 'classifier'):\n",
    "    print(\"   âœ… Classification Head (newly initialized)\")\n",
    "if hasattr(model, 'pooler'):\n",
    "    print(\"   âœ… Pooling Layer (sequence â†’ single representation)\")\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\nâš™ï¸ Model Configuration:\")\n",
    "if hasattr(model, 'config'):\n",
    "    config = model.config\n",
    "    print(f\"   ğŸ”¢ Hidden size: {getattr(config, 'hidden_size', 'N/A')}\")\n",
    "    print(f\"   ğŸ·ï¸ Number of labels: {getattr(config, 'num_labels', 'N/A')}\")\n",
    "    print(f\"   ğŸ“ Max sequence length: {getattr(config, 'max_position_embeddings', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for: Translation Efficiency Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97009939",
   "metadata": {},
   "source": [
    "## Testing Model Functionality ğŸ§ª\n",
    "\n",
    "Before training, let's test that our model can process sequences correctly. This helps us verify everything is set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c813870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample sequences\n",
    "print(\"ğŸ§ª Testing Model Functionality\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample mRNA sequences for testing\n",
    "test_sequences = [\n",
    "    \"AUGCUGCUAUGCUAGCUAGC\",  # Short test sequence\n",
    "    \"AUGAAACCAACAAAATGCAGTAGAAGTACTCTCGAGCTATAGTCGCGACGTGCTGCCCCGC\",  # Longer sequence\n",
    "]\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for i, sequence in enumerate(test_sequences, 1):\n",
    "        print(f\"\\nğŸ§¬ Test Sequence {i}:\")\n",
    "        print(f\"   Sequence: {sequence[:50]}{'...' if len(sequence) > 50 else ''}\")\n",
    "        print(f\"   Length: {len(sequence)} nucleotides\")\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        print(f\"   ğŸ“Š Raw logits: {logits.squeeze().tolist()}\")\n",
    "        print(f\"   ğŸ“ˆ Probabilities: [Low TE: {probabilities[0][0]:.3f}, High TE: {probabilities[0][1]:.3f}]\")\n",
    "        print(f\"   ğŸ¯ Predicted class: {predicted_class.item()} ({'High TE' if predicted_class.item() == 1 else 'Low TE'})\")\n",
    "\n",
    "print(f\"\\nâœ… Model functionality test completed!\")\n",
    "print(f\"   ğŸ¯ Model can process sequences and generate predictions\")\n",
    "print(f\"   ğŸ“Š Ready for training on the translation efficiency dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b228ad4",
   "metadata": {},
   "source": [
    "## Understanding Pre-trained vs Fine-tuned Models ğŸ“\n",
    "\n",
    "It's important to understand what we have at this stage and what training will accomplish.\n",
    "\n",
    "### ğŸ¤– Current Model State (Pre-trained + Classification Head)\n",
    "\n",
    "**What the model knows:**\n",
    "- âœ… **General genomic patterns**: Codon usage, sequence motifs, structural constraints\n",
    "- âœ… **Biological relationships**: GC content effects, splice sites, regulatory elements\n",
    "- âœ… **Sequence representations**: How to encode biological meaning from nucleotide sequences\n",
    "\n",
    "**What the model doesn't know yet:**\n",
    "- âŒ **Translation efficiency patterns**: Which sequence features correlate with high/low TE\n",
    "- âŒ **Task-specific relationships**: How ribosome binding, codon optimization affect TE\n",
    "- âŒ **Decision boundaries**: Where to classify sequences as high vs low TE\n",
    "\n",
    "### ğŸ¯ What Fine-tuning Will Accomplish\n",
    "\n",
    "During training, the model will learn:\n",
    "1. **Task-specific patterns**: Which genomic features predict translation efficiency\n",
    "2. **Decision boundaries**: How to separate high TE from low TE sequences\n",
    "3. **Biological relationships**: How sequence context affects ribosome binding and translation\n",
    "\n",
    "The pre-trained knowledge provides a **strong foundation**, and fine-tuning **specializes** this knowledge for our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895291c",
   "metadata": {},
   "source": [
    "## Model Configuration Best Practices ğŸ“‹\n",
    "\n",
    "Let's discuss important configuration choices and their biological implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate key configuration options\n",
    "print(\"âš™ï¸ Model Configuration Best Practices\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nğŸ¯ Key Configuration Choices:\")\n",
    "print(\"\\n1. ğŸ“¦ Base Model Selection:\")\n",
    "print(f\"   Current: {model_name_or_path}\")\n",
    "print(\"   Options: OmniGenome-52M (fast), OmniGenome-186M (more accurate)\")\n",
    "print(\"   Trade-off: Speed vs Accuracy\")\n",
    "\n",
    "print(\"\\n2. ğŸ·ï¸ Label Configuration:\")\n",
    "print(f\"   Labels: {list(label2id.keys())} â†’ {list(label2id.values())}\")\n",
    "print(\"   Meaning: 0 = Low TE, 1 = High TE\")\n",
    "print(\"   Type: Binary classification\")\n",
    "\n",
    "print(\"\\n3. ğŸ“ Sequence Length Considerations:\")\n",
    "max_length = 512  # Common choice for mRNA sequences\n",
    "print(f\"   Max length: {max_length} nucleotides\")\n",
    "print(\"   Rationale: Covers most mRNA 5' UTR + start of CDS\")\n",
    "print(\"   Biological relevance: Key regulatory regions for translation\")\n",
    "\n",
    "print(\"\\n4. ğŸ”§ Model Architecture:\")\n",
    "print(\"   Type: Transformer-based (attention mechanism)\")\n",
    "print(\"   Advantages: Captures long-range dependencies\")\n",
    "print(\"   Biological relevance: Secondary structures, distant motifs\")\n",
    "\n",
    "print(\"\\nâœ… Configuration optimized for translation efficiency prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96dc03",
   "metadata": {},
   "source": [
    "## Connecting to Data Pipeline ğŸ”—\n",
    "\n",
    "Let's briefly connect our initialized model to the data pipeline we created in the previous tutorial, ensuring everything works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to demonstrate model-data compatibility\n",
    "print(\"ğŸ”— Testing Model-Data Pipeline Integration\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Load datasets - exactly as in complete tutorial\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name=\"translation_efficiency_prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Datasets loaded: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")\n",
    "\n",
    "# Test model with actual dataset sample\n",
    "print(\"\\nğŸ§ª Testing with real dataset sample:\")\n",
    "sample = datasets['train'][0]\n",
    "print(f\"   Sample keys: {list(sample.keys())}\")\n",
    "print(f\"   Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"   Label: {sample['labels'].item()} ({'High TE' if sample['labels'].item() == 1 else 'Low TE'})\")\n",
    "\n",
    "# Test forward pass with real data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Add batch dimension\n",
    "    batch_input = {k: v.unsqueeze(0) if v.dim() == 1 else v for k, v in sample.items() if k != 'labels'}\n",
    "    outputs = model(**batch_input)\n",
    "    \n",
    "    predicted_class = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"   ğŸ¯ Model prediction: {predicted_class.item()} ({'High TE' if predicted_class.item() == 1 else 'Low TE'})\")\n",
    "\n",
    "print(\"\\nâœ… Model-data integration successful!\")\n",
    "print(\"   ğŸ¯ Model can process real dataset samples\")\n",
    "print(\"   ğŸ“Š Ready for training pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649365f3",
   "metadata": {},
   "source": [
    "## ğŸ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed the model initialization tutorial. Let's review what we've accomplished:\n",
    "\n",
    "### ğŸ“ **Skills Mastered**\n",
    "\n",
    "âœ… **Understood Genomic Foundation Models**: The principles behind pre-trained genomic AI  \n",
    "âœ… **Mastered Model Architecture**: Different model types for various biological tasks  \n",
    "âœ… **Initialized Models Correctly**: Proper tokenizer and model setup  \n",
    "âœ… **Tested Model Functionality**: Verified the model can process genomic sequences  \n",
    "âœ… **Configured for Biology**: Optimized settings for translation efficiency prediction  \n",
    "âœ… **Integrated with Data**: Connected model to the data pipeline  \n",
    "\n",
    "\n",
    "### ğŸ”¬ **Key Concepts Learned**\n",
    "\n",
    "- **ğŸ§¬ Pre-training vs Fine-tuning**: How models learn general then specific knowledge\n",
    "- **ğŸ¤– Model Architecture**: Transformer-based models for genomic sequences\n",
    "- **ğŸ“Š Task Specialization**: Choosing the right model class for your biological question\n",
    "- **âš™ï¸ Configuration Management**: Best practices for reproducible model setup\n",
    "\n",
    "### ğŸš€ **What's Next...**\n",
    "\n",
    "You now have a properly initialized model ready for training! In the next tutorial, we will:\n",
    "\n",
    "- ğŸ“ **Learn the training process**: How models learn from data\n",
    "- âš™ï¸ **Configure training parameters**: Learning rates, optimizers, evaluation metrics\n",
    "- ğŸ“Š **Monitor training progress**: Loss curves, validation metrics, early stopping\n",
    "- ğŸ’¾ **Save trained models**: Model persistence for later use\n",
    "\n",
    "**Ready to train your genomic AI model?** ğŸ§¬âœ¨\n",
    "\n",
    "ğŸ‘‰ **Next Step**: Open [03_model_training.ipynb](https://github.com/yangheng95/OmniGenBench/blob/master/examples/translation_efficiency_prediction/03_model_training.ipynb) to continue your learning journey!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
