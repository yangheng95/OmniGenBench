{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9d837f",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Genomic Embeddings and Representation Learning with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to generate **high-quality genomic embeddings** from DNA and RNA sequences using **OmniGenBench**. This guide will walk you through the process of extracting meaningful vector representations from genomic sequences for downstream analysis and machine learning applications.\n",
    "\n",
    "### 1. The Computational Challenge: What are Genomic Embeddings?\n",
    "\n",
    "**Genomic embeddings** are dense vector representations that capture the semantic and functional information encoded in DNA and RNA sequences. These embeddings transform discrete nucleotide sequences into continuous vector spaces where similar sequences are positioned closer together.\n",
    "\n",
    "The power of genomic embeddings lies in their ability to:\n",
    "- **Capture Sequence Semantics**: Encode biological meaning and functional relationships\n",
    "- **Enable Similarity Analysis**: Find functionally related sequences through vector similarity\n",
    "- **Support Downstream ML**: Serve as input features for various machine learning tasks\n",
    "- **Compress Information**: Reduce high-dimensional sequence data to manageable representations\n",
    "\n",
    "Applications span across computational biology:\n",
    "- **Drug Discovery**: Finding target sequences and analyzing molecular interactions\n",
    "- **Evolutionary Analysis**: Studying sequence relationships and phylogenetic patterns  \n",
    "- **Functional Annotation**: Predicting sequence function from embedding similarity\n",
    "- **Biomarker Discovery**: Identifying disease-related sequence patterns\n",
    "\n",
    "### 2. The Data: From Sequences to Vectors\n",
    "\n",
    "Unlike traditional one-hot encoding, genomic foundation models learn rich representations that capture:\n",
    "\n",
    "- **Local Patterns**: k-mer frequencies, motifs, and short-range dependencies\n",
    "- **Global Context**: Long-range interactions and structural relationships  \n",
    "- **Functional Similarities**: Sequences with similar biological roles cluster together\n",
    "- **Evolutionary Relationships**: Homologous sequences have similar embeddings\n",
    "\n",
    "**Transformation Process:**\n",
    "\n",
    "| Raw Sequence | Traditional Encoding | Embedding Vector |\n",
    "|-------------|---------------------|------------------|\n",
    "| `ATGCGATCG` | `[1,0,0,0,0,1,0,0,...]` | `[0.23, -0.45, 0.12, ...]` |\n",
    "| `ATGCGTTCG` | `[1,0,0,0,0,1,0,1,...]` | `[0.21, -0.43, 0.15, ...]` |\n",
    "\n",
    "### 3. The Tool: Genomic Foundation Models for Representation Learning\n",
    "\n",
    "#### Pre-trained Understanding\n",
    "**OmniGenome** models are pre-trained on massive genomic datasets, learning to represent sequences in biologically meaningful vector spaces. This pre-training captures:\n",
    "\n",
    "1. **Sequence Patterns**: Common motifs, regulatory elements, and structural features\n",
    "2. **Functional Relationships**: Similar functions lead to similar representations\n",
    "3. **Evolutionary Context**: Related sequences cluster in embedding space\n",
    "4. **Multi-scale Information**: From local k-mers to global sequence properties\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Genomic Embeddings\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for Genomic Embeddings\"\n",
    "        A[\"ðŸ“¥ Step 1: Setup and Configuration<br/>Initialize models and prepare sequences\"] --> B[\"ðŸ”§ Step 2: Model Loading<br/>Load pre-trained genomic foundation models\"]\n",
    "        B --> C[\"ðŸŽ“ Step 3: Embedding Generation<br/>Extract vector representations from sequences\"]\n",
    "        C --> D[\"ðŸ”® Step 4: Analysis and Applications<br/>Analyze embeddings and explore applications\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start generating powerful genomic embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3bc0f",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Setup and Configuration\n",
    "\n",
    "This first step focuses on setting up our environment for genomic embedding generation and analysis.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required packages for genomic embedding generation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1.1: Environment Setup and Verification\n",
    "# =============================================================================\n",
    "# This cell installs required packages and sets up the reproducible environment\n",
    "# using shared utilities. All randomness is controlled via explicit seeds.\n",
    "\n",
    "# Install required packages (uncomment if running for first time)\n",
    "# !pip install omnigenbench torch transformers scikit-learn matplotlib seaborn -U\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add examples directory to path to import shared utilities\n",
    "examples_dir = Path.cwd()\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir.parent))\n",
    "\n",
    "# Import shared utilities for reproducibility\n",
    "try:\n",
    "    from shared_utils import (\n",
    "        setup_notebook_environment,\n",
    "        verify_environment,\n",
    "        set_global_seed,\n",
    "        resolve_data_path,\n",
    "    )\n",
    "    print(\"[SUCCESS] Shared utilities imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"[ERROR] Could not import shared_utils.py\")\n",
    "    print(\"  This file should be in: examples/shared_utils.py\")\n",
    "    print(f\"  Current directory: {Path.cwd()}\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# One-line setup: seed, environment verification, matplotlib config\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INITIALIZING REPRODUCIBLE NOTEBOOK ENVIRONMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "env_info = setup_notebook_environment(\n",
    "    seed=42,  # Will be overridden by RANDOM_SEED in next cell\n",
    "    required_packages=['omnigenbench', 'torch', 'transformers', 'numpy', \n",
    "                       'matplotlib', 'seaborn', 'sklearn'],\n",
    "    check_gpu=True,\n",
    "    suppress_warnings=True,\n",
    "    matplotlib_style='seaborn-v0_8',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n[SUCCESS] Environment setup completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d3b0b",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Next, we import the essential libraries for genomic embedding generation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaed138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import OmniGenBench models\n",
    "# ðŸŽ¯ IMPORTANT: All OmniModel types support embedding extraction via EmbeddingMixin\n",
    "from omnigenbench import (\n",
    "    OmniModelForEmbedding,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForSequenceRegression,\n",
    "    ModelHub,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Verify imports succeeded\n",
    "print(\"âœ… All required libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18138115",
   "metadata": {},
   "source": [
    "### 1.3: Global Configuration\n",
    "\n",
    "Let's define our configuration parameters for embedding generation and analysis.\n",
    "\n",
    "#### Key Parameters\n",
    "- **Model Selection**: Choose the appropriate genomic foundation model for embedding generation\n",
    "- **Analysis Settings**: Configure parameters for dimensionality reduction and clustering\n",
    "- **Visualization Options**: Set up parameters for embedding visualization and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION (Single Source of Truth - SSoT)\n",
    "# =============================================================================\n",
    "# All configuration parameters are defined here for easy modification and \n",
    "# reproducibility. This follows the SSoT principle: change parameters here\n",
    "# and they propagate throughout the notebook automatically.\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Random Seed (CRITICAL for Reproducibility)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This seed controls ALL random operations in the notebook:\n",
    "# - Python's random module\n",
    "# - NumPy's random number generator  \n",
    "# - PyTorch's CPU and CUDA random number generators\n",
    "# - PyTorch's cudnn backend (deterministic mode)\n",
    "\n",
    "RANDOM_SEED = 42  # Single source of truth for reproducibility\n",
    "\n",
    "# Set seeds for all libraries\n",
    "set_global_seed(RANDOM_SEED, verbose=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "embedding_config = {\n",
    "    \"model_name\": \"yangheng/OmniGenome-52M\",\n",
    "    \"aggregation_method\": \"mean\",  # Options: \"head\", \"mean\", \"tail\"\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 16,\n",
    "    \"use_fp16\": True,  # Mixed precision for GPU (faster, less memory)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Analysis Configuration  \n",
    "# -----------------------------------------------------------------------------\n",
    "analysis_config = {\n",
    "    \"n_components_pca\": 50,\n",
    "    \"n_components_tsne\": 2,\n",
    "    \"n_clusters\": 4,  # Number of sequence clusters to discover\n",
    "    \"random_state\": RANDOM_SEED,  # Use same seed for consistency\n",
    "    \"tsne_perplexity\": 3,  # t-SNE perplexity (must be < n_samples/3)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Visualization Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "viz_config = {\n",
    "    \"figsize\": (12, 8),\n",
    "    \"dpi\": 100,\n",
    "    \"cmap\": \"viridis\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration Validation (Fail Fast)\n",
    "# -----------------------------------------------------------------------------\n",
    "from shared_utils import validate_config\n",
    "\n",
    "# Validate embedding config schema\n",
    "embedding_schema = {\n",
    "    \"model_name\": str,\n",
    "    \"aggregation_method\": str,\n",
    "    \"max_length\": int,\n",
    "    \"batch_size\": int,\n",
    "    \"use_fp16\": bool,\n",
    "}\n",
    "validate_config(embedding_config, embedding_schema)\n",
    "\n",
    "# Validate aggregation method\n",
    "valid_agg_methods = [\"head\", \"mean\", \"tail\"]\n",
    "assert embedding_config[\"aggregation_method\"] in valid_agg_methods, \\\n",
    "    f\"aggregation_method must be one of {valid_agg_methods}\"\n",
    "\n",
    "# Validate analysis config schema\n",
    "analysis_schema = {\n",
    "    \"n_components_pca\": int,\n",
    "    \"n_components_tsne\": int,\n",
    "    \"n_clusters\": int,\n",
    "    \"random_state\": int,\n",
    "    \"tsne_perplexity\": (int, float),  # Can be int or float\n",
    "}\n",
    "\n",
    "# Validate ranges\n",
    "assert 1 <= analysis_config[\"n_clusters\"] <= 20, \"n_clusters must be in [1, 20]\"\n",
    "assert analysis_config[\"n_components_tsne\"] in [2, 3], \"t-SNE components must be 2 or 3\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Print Configuration Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFIGURATION SUMMARY (Single Source of Truth)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n[SEED] Random seed: {RANDOM_SEED}\")\n",
    "print(\"  - All random operations are deterministic\")\n",
    "print(\"  - Results are 100% reproducible\")\n",
    "\n",
    "print(f\"\\n[MODEL] Genomic Foundation Model:\")\n",
    "print(f\"  - Model: {embedding_config['model_name']}\")\n",
    "print(f\"  - Aggregation: {embedding_config['aggregation_method']}\")\n",
    "print(f\"  - Max length: {embedding_config['max_length']}\")\n",
    "print(f\"  - Batch size: {embedding_config['batch_size']}\")\n",
    "print(f\"  - Mixed precision (FP16): {embedding_config['use_fp16']}\")\n",
    "\n",
    "print(f\"\\n[ANALYSIS] Dimensionality Reduction & Clustering:\")\n",
    "print(f\"  - PCA components: {analysis_config['n_components_pca']}\")\n",
    "print(f\"  - t-SNE components: {analysis_config['n_components_tsne']}\")\n",
    "print(f\"  - Number of clusters: {analysis_config['n_clusters']}\")\n",
    "print(f\"  - t-SNE perplexity: {analysis_config['tsne_perplexity']}\")\n",
    "print(f\"  - Random state: {analysis_config['random_state']}\")\n",
    "\n",
    "print(f\"\\n[VISUALIZATION] Plot Settings:\")\n",
    "print(f\"  - Figure size: {viz_config['figsize']}\")\n",
    "print(f\"  - DPI: {viz_config['dpi']}\")\n",
    "print(f\"  - Colormap: {viz_config['cmap']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"[SUCCESS] Configuration validated and loaded\")\n",
    "print(\"[INFO] Modify parameters above to experiment with different settings\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c3e9b",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Loading\n",
    "\n",
    "Now let's load the pre-trained genomic foundation model for embedding generation. \n",
    "\n",
    "### ðŸŽ¯ Important: All OmniModel Classes Support Embeddings!\n",
    "\n",
    "**All OmniGenBench models** now inherit from `EmbeddingMixin`, which means:\n",
    "- âœ… `OmniModelForEmbedding` - Dedicated embedding extraction\n",
    "- âœ… `OmniModelForSequenceClassification` - Classification + Embeddings\n",
    "- âœ… `OmniModelForSequenceRegression` - Regression + Embeddings  \n",
    "- âœ… `OmniModelForTokenClassification` - Token classification + Embeddings\n",
    "- âœ… **All other OmniModel variants** - Task-specific + Embeddings\n",
    "\n",
    "You can use **any** of these model types to extract embeddings and attention scores!\n",
    "\n",
    "### Model Features\n",
    "- **Pre-trained Understanding**: Leverages genomic foundation model knowledge\n",
    "- **Flexible Aggregation**: Multiple methods for sequence-to-vector conversion\n",
    "- **Batch Processing**: Efficient handling of multiple sequences\n",
    "- **GPU Acceleration**: Automatic CUDA optimization for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Model Loading and Initialization\n",
    "# =============================================================================\n",
    "# Initialize the genomic foundation model for embedding extraction.\n",
    "# All OmniModel types support embeddings via EmbeddingMixin.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING GENOMIC FOUNDATION MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[INFO] Model: {embedding_config['model_name']}\")\n",
    "print(\"[INFO] Initializing OmniModelForEmbedding...\")\n",
    "\n",
    "try:\n",
    "    # Option 1: Use dedicated embedding model (RECOMMENDED)\n",
    "    embedding_model = OmniModelForEmbedding(\n",
    "        embedding_config[\"model_name\"],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Option 2: Any OmniModel type also works!\n",
    "    # All OmniModel classes inherit from EmbeddingMixin and support the same API:\n",
    "    # \n",
    "    # from omnigenbench import OmniModelForSequenceClassification\n",
    "    # embedding_model = OmniModelForSequenceClassification.from_pretrained(\n",
    "    #     embedding_config[\"model_name\"], \n",
    "    #     num_labels=2, \n",
    "    #     trust_remote_code=True\n",
    "    # )\n",
    "    # \n",
    "    # They all support: .encode(), .batch_encode(), .extract_attention_scores(), etc.\n",
    "    \n",
    "    print(\"[SUCCESS] Model loaded successfully!\")\n",
    "    \n",
    "    # Move to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    # Use mixed precision for GPU inference (faster and memory efficient)\n",
    "    if device.type == \"cuda\" and embedding_config[\"use_fp16\"]:\n",
    "        embedding_model = embedding_model.to(torch.float16)\n",
    "        precision = \"float16 (mixed precision)\"\n",
    "        print(\"[INFO] Mixed precision (FP16) enabled\")\n",
    "        print(\"  - Memory usage: ~50% reduction\")\n",
    "        print(\"  - Inference speed: ~2x faster\")\n",
    "    else:\n",
    "        precision = \"float32\"\n",
    "        if device.type == \"cuda\":\n",
    "            print(\"[INFO] Using float32 precision (disable with use_fp16=True)\")\n",
    "    \n",
    "    # Print model info\n",
    "    print(f\"\\n[MODEL INFO]\")\n",
    "    print(f\"  - Architecture: Genomic foundation model\")\n",
    "    print(f\"  - Parameters: ~52M\")\n",
    "    print(f\"  - Device: {device}\")\n",
    "    print(f\"  - Precision: {precision}\")\n",
    "    print(f\"  - Embedding dimension: 768\")\n",
    "    print(f\"  - Max sequence length: {embedding_config['max_length']}\")\n",
    "    \n",
    "    print(\"\\n[KEY POINTS]\")\n",
    "    print(\"  â€¢ All OmniModel types support embedding extraction via EmbeddingMixin\")\n",
    "    print(\"  â€¢ Mixed precision (FP16) reduces memory by ~50% on GPU\")\n",
    "    print(\"  â€¢ Embeddings are 768-dimensional vectors (model hidden_size)\")\n",
    "    print(\"  â€¢ Three aggregation methods: head (CLS), mean (average), tail (last)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load model: {e}\")\n",
    "    print(\"\\n[TROUBLESHOOTING]\")\n",
    "    print(\"  1. Check internet connection (required for first download)\")\n",
    "    print(\"  2. Verify PyTorch installation: pip install torch -U\")\n",
    "    print(\"  3. Try: pip install omnigenbench -U\")\n",
    "    print(\"  4. Check HuggingFace Hub access\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"[SUCCESS] Model initialization completed\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3324a3e",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Embedding Generation\n",
    "\n",
    "Now let's generate embeddings for various types of genomic sequences. We'll use diverse sequences to demonstrate how the model captures different biological patterns and relationships.\n",
    "\n",
    "### Our Sequence Collection\n",
    "\n",
    "We'll analyze sequences with different characteristics:\n",
    "- **Functional RNAs**: tRNAs, rRNAs, and regulatory sequences\n",
    "- **Coding Sequences**: mRNAs encoding different proteins\n",
    "- **Regulatory Elements**: Promoters, enhancers, and UTRs\n",
    "- **Structural Variants**: Sequences with different folding properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80325ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Embedding Generation from Diverse Genomic Sequences\n",
    "# =============================================================================\n",
    "# Generate embeddings for various types of RNA sequences to demonstrate how\n",
    "# the model captures different biological patterns and relationships.\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define Diverse Sequence Collection\n",
    "# -----------------------------------------------------------------------------\n",
    "# We include sequences with different characteristics to test the model's\n",
    "# ability to capture various biological features:\n",
    "# - Functional RNAs (tRNA, rRNA, miRNA)\n",
    "# - mRNA sequences (coding regions)\n",
    "# - Regulatory elements (promoters, enhancers, UTRs)\n",
    "# - Structural variants (hairpins, repeats, random)\n",
    "\n",
    "genomic_sequences = {\n",
    "    \"Functional RNAs\": {\n",
    "        \"tRNA-Ala\": \"GGGGGUAUAGCUCAGUGGUAGAGCGCGUGCCUUUGCAAGCACAAGAGUCUCGGGAGUCGUUGGUUCGAAUCACCGUACCCCCA\",\n",
    "        \"rRNA-18S\": \"CGGCUACCACAUCCAAGGAAGGCAGCAGGCGCGCAAAUUACCCACUCCCGACCCGGGGAGGGUAGUGGCGGUUCGCCAGGA\",\n",
    "        \"miRNA-21\": \"UAGCUUAUCAGACUGAUGUUGACUGUUGAAUCUCAUGGCAACACCAGUCGAUGGGCUGU\",\n",
    "    },\n",
    "    \"mRNA Sequences\": {\n",
    "        \"Insulin mRNA\": \"AUGCCGCGCAACGAGGCCUACACUGUGCGAACUGCUGCCUGCUGCUGCCCGCUGCUGCUGCUGGGCUCCGCCCGCCGAG\",\n",
    "        \"Hemoglobin mRNA\": \"AUGGUGGACGACGUGCUCGGCAAGAACGUCAACCACGUGAAGCUGGUGGUGGACGACGACGGCUGCGUGGGCAACUGC\",\n",
    "        \"p53 mRNA\": \"AUGGAGGAGCCGCAGUCAGAUCCUAGCGUCCGGGACGACACGCCAACCUGCUCUCCUGCCGUCCCCGCCAAGACCAGC\",\n",
    "    },\n",
    "    \"Regulatory Elements\": {\n",
    "        \"TATA promoter\": \"CGGCGCGCCAUAUAAAGCAUCGAGCGCGCACGUGCGCUGCGCGCGCGCUACGCGCGCAUGUGCGCGCACGUACGCGCG\",\n",
    "        \"Enhancer seq\": \"GCGCGCGCACGUGCGCACGUGCGCGCACGUGCGCGCGCACGUGCGCGCACGUGCGCGCACGUGCGCGCACGUGCGCGC\",\n",
    "        \"5'UTR\": \"GCGCGCCACCAAUGCGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCC\",\n",
    "    },\n",
    "    \"Structural Variants\": {\n",
    "        \"Hairpin RNA\": \"CGGAAACCCUUUGGGAAACCCGGGAAACCCUUUGGGAAACCCGGGAAACCCUUUGGGAAACCCG\",\n",
    "        \"Repeat seq\": \"CACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACA\",\n",
    "        \"Random seq\": \"AUGCGAUCUCGAGCUACGUCGAUGCUAGCUCGAUGGCAUCCGAUUCGAGCUACGUCGAUGCUAG\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flatten sequences for batch processing\n",
    "all_sequences = []\n",
    "sequence_labels = []\n",
    "sequence_categories = []\n",
    "\n",
    "for category, sequences in genomic_sequences.items():\n",
    "    for label, sequence in sequences.items():\n",
    "        all_sequences.append(sequence)\n",
    "        sequence_labels.append(label)\n",
    "        sequence_categories.append(category)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GENOMIC SEQUENCE COLLECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[INFO] Prepared {len(all_sequences)} genomic sequences:\")\n",
    "for category, sequences in genomic_sequences.items():\n",
    "    print(f\"  - {category}: {len(sequences)} sequences\")\n",
    "print(f\"\\n[INFO] Sequence length range: {min(len(s) for s in all_sequences)} - {max(len(s) for s in all_sequences)} nt\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate Embeddings in Batches\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EMBEDDING GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[INFO] Processing {len(all_sequences)} sequences...\")\n",
    "print(f\"[INFO] Batch size: {embedding_config['batch_size']}\")\n",
    "print(f\"[INFO] Aggregation method: {embedding_config['aggregation_method']}\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "# Process sequences in batches for efficiency\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = embedding_config[\"batch_size\"]\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_sequences), batch_size), \n",
    "              desc=\"Generating embeddings\", \n",
    "              unit=\"batch\"):\n",
    "    batch_sequences = all_sequences[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.batch_encode(\n",
    "        batch_sequences, \n",
    "        agg=embedding_config[\"aggregation_method\"]\n",
    "    )\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "genomic_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Validate Output\n",
    "# -----------------------------------------------------------------------------\n",
    "from shared_utils import assert_shape\n",
    "\n",
    "print(\"\\n[VALIDATION]\")\n",
    "assert_shape(genomic_embeddings, (len(all_sequences), 768), \"genomic_embeddings\")\n",
    "print(f\"[SUCCESS] Embedding matrix shape: {genomic_embeddings.shape}\")\n",
    "print(f\"  - Number of sequences: {genomic_embeddings.shape[0]}\")\n",
    "print(f\"  - Embedding dimension: {genomic_embeddings.shape[1]}\")\n",
    "\n",
    "# Check value ranges (sanity check)\n",
    "emb_min = genomic_embeddings.min().item()\n",
    "emb_max = genomic_embeddings.max().item()\n",
    "emb_mean = genomic_embeddings.mean().item()\n",
    "emb_std = genomic_embeddings.std().item()\n",
    "\n",
    "print(f\"\\n[STATISTICS]\")\n",
    "print(f\"  - Value range: [{emb_min:.4f}, {emb_max:.4f}]\")\n",
    "print(f\"  - Mean: {emb_mean:.4f}\")\n",
    "print(f\"  - Std: {emb_std:.4f}\")\n",
    "\n",
    "# Sanity check: embeddings should be in reasonable range\n",
    "assert -10 < emb_min < 10, f\"Embedding min value out of range: {emb_min}\"\n",
    "assert -10 < emb_max < 10, f\"Embedding max value out of range: {emb_max}\"\n",
    "\n",
    "print(f\"\\n[SUCCESS] Embedding generation completed!\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27b1bc",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Comprehensive Embedding Analysis\n",
    "\n",
    "Now let's analyze our genomic embeddings to understand the relationships between sequences and explore various applications. This demonstrates the power of genomic foundation models in capturing biological meaning.\n",
    "\n",
    "### Analysis Pipeline\n",
    "\n",
    "Our comprehensive analysis includes four key components:\n",
    "1. **Similarity Analysis**: Calculate pairwise cosine similarities between sequences\n",
    "2. **Dimensionality Reduction**: Visualize embeddings in 2D space using PCA and t-SNE\n",
    "3. **Clustering**: Discover sequence groups with similar genomic properties\n",
    "4. **Biological Interpretation**: Connect computational results to biological insights\n",
    "\n",
    "Let's begin with similarity analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3031cb7",
   "metadata": {},
   "source": [
    "### 4.1: Pairwise Similarity Analysis\n",
    "\n",
    "Let's compute the cosine similarity between all sequence pairs to understand which sequences the model considers functionally related.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccda5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise cosine similarity matrix\n",
    "# Convert embeddings to numpy for sklearn compatibility\n",
    "embeddings_np = genomic_embeddings.cpu().numpy()\n",
    "\n",
    "# Compute similarity matrix (shape: n_sequences x n_sequences)\n",
    "similarity_matrix = cosine_similarity(embeddings_np)\n",
    "\n",
    "print(f\"ðŸ“Š Similarity Matrix Analysis:\")\n",
    "print(f\"  Shape: {similarity_matrix.shape}\")\n",
    "print(f\"  Value range: [{similarity_matrix.min():.4f}, {similarity_matrix.max():.4f}]\")\n",
    "print(f\"  Mean similarity: {similarity_matrix.mean():.4f}\")\n",
    "print(f\"  Diagonal values (self-similarity): {np.diag(similarity_matrix).mean():.4f}\")\n",
    "\n",
    "# Visualize similarity matrix as heatmap\n",
    "plt.figure(figsize=viz_config[\"figsize\"])\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    xticklabels=sequence_labels,\n",
    "    yticklabels=sequence_labels,\n",
    "    cmap=viz_config[\"cmap\"],\n",
    "    annot=False,  # Set to True to show values (cluttered for many sequences)\n",
    "    fmt=\".2f\",\n",
    "    cbar_kws={'label': 'Cosine Similarity'},\n",
    "    square=True\n",
    ")\n",
    "plt.title(\"Pairwise Sequence Similarity Matrix\\n(Darker = More Similar)\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Sequences\", fontsize=12)\n",
    "plt.ylabel(\"Sequences\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most similar sequence pairs (excluding self-similarity)\n",
    "print(f\"\\nðŸ”¬ Top 5 Most Similar Sequence Pairs:\")\n",
    "# Set diagonal to -1 to exclude self-similarity\n",
    "sim_no_diag = similarity_matrix.copy()\n",
    "np.fill_diagonal(sim_no_diag, -1)\n",
    "\n",
    "# Get top 5 pairs\n",
    "n_seqs = len(sequence_labels)\n",
    "top_pairs = []\n",
    "for i in range(n_seqs):\n",
    "    for j in range(i+1, n_seqs):\n",
    "        top_pairs.append((i, j, sim_no_diag[i, j]))\n",
    "top_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for rank, (i, j, sim) in enumerate(top_pairs[:5], 1):\n",
    "    print(f\"  {rank}. {sequence_labels[i]} â†” {sequence_labels[j]}\")\n",
    "    print(f\"     Similarity: {sim:.4f} | Categories: {sequence_categories[i]} vs {sequence_categories[j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7feaac9",
   "metadata": {},
   "source": [
    "### 4.2: Dimensionality Reduction and Visualization\n",
    "\n",
    "High-dimensional embeddings (768-D) are difficult to visualize. Let's use PCA and t-SNE to project them into 2D space while preserving the most important relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for initial dimensionality reduction (768D â†’ 50D)\n",
    "print(\"ðŸ”„ Applying PCA dimensionality reduction...\")\n",
    "pca = PCA(\n",
    "    n_components=analysis_config[\"n_components_pca\"],\n",
    "    random_state=analysis_config[\"random_state\"]\n",
    ")\n",
    "embeddings_pca = pca.fit_transform(embeddings_np)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "print(f\"  PCA reduced dimensions: 768 â†’ {analysis_config['n_components_pca']}\")\n",
    "print(f\"  Variance explained by first 10 components: {cumulative_var[9]:.2%}\")\n",
    "print(f\"  Variance explained by all {analysis_config['n_components_pca']} components: {cumulative_var[-1]:.2%}\")\n",
    "\n",
    "# Apply t-SNE for 2D visualization (50D â†’ 2D)\n",
    "print(f\"\\nðŸ”„ Applying t-SNE for 2D visualization...\")\n",
    "tsne = TSNE(\n",
    "    n_components=analysis_config[\"n_components_tsne\"],\n",
    "    random_state=analysis_config[\"random_state\"],\n",
    "    perplexity=min(analysis_config[\"tsne_perplexity\"], (len(embeddings_pca) - 1) / 3),\n",
    "    n_iter=1000,\n",
    "    verbose=0\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
    "print(f\"  t-SNE reduced dimensions: {analysis_config['n_components_pca']} â†’ 2\")\n",
    "\n",
    "# Visualize embeddings colored by category\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Colored by sequence category\n",
    "categories_unique = list(set(sequence_categories))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(categories_unique)))\n",
    "category_to_color = {cat: colors[i] for i, cat in enumerate(categories_unique)}\n",
    "\n",
    "for category in categories_unique:\n",
    "    mask = [cat == category for cat in sequence_categories]\n",
    "    ax1.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[category_to_color[category]],\n",
    "        label=category,\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        edgecolors='black',\n",
    "        linewidth=1\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "ax1.set_title('Genomic Embeddings (Colored by Category)', fontsize=14, pad=15)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Annotated with sequence labels\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    cat = sequence_categories[i]\n",
    "    ax2.scatter(x, y, c=[category_to_color[cat]], s=100, alpha=0.7, edgecolors='black', linewidth=1)\n",
    "    ax2.annotate(\n",
    "        sequence_labels[i],\n",
    "        (x, y),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "ax2.set_title('Genomic Embeddings (Annotated)', fontsize=14, pad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¨ Visualization Interpretation:\")\n",
    "print(f\"  â€¢ Sequences close together have similar genomic properties\")\n",
    "print(f\"  â€¢ Clusters indicate functional or structural similarity\")\n",
    "print(f\"  â€¢ Distance reflects dissimilarity in the learned embedding space\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46494b69",
   "metadata": {},
   "source": [
    "### 4.3: Unsupervised Clustering Analysis\n",
    "\n",
    "Let's use K-means clustering to automatically discover groups of sequences with similar genomic properties. This demonstrates how embeddings can be used for unsupervised sequence classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering on the embeddings\n",
    "print(f\"ðŸ” Performing K-means clustering (k={analysis_config['n_clusters']})...\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=analysis_config[\"n_clusters\"],\n",
    "    random_state=analysis_config[\"random_state\"],\n",
    "    n_init=10\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings_np)\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(f\"\\nðŸ“Š Cluster Composition Analysis:\")\n",
    "for cluster_id in range(analysis_config[\"n_clusters\"]):\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_sequences = [sequence_labels[i] for i, mask in enumerate(cluster_mask) if mask]\n",
    "    cluster_cats = [sequence_categories[i] for i, mask in enumerate(cluster_mask) if mask]\n",
    "    \n",
    "    print(f\"\\n  Cluster {cluster_id} ({sum(cluster_mask)} sequences):\")\n",
    "    for seq_label in cluster_sequences:\n",
    "        seq_idx = sequence_labels.index(seq_label)\n",
    "        print(f\"    â€¢ {seq_label} ({sequence_categories[seq_idx]})\")\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=viz_config[\"figsize\"])\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='tab10',\n",
    "    s=200,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add cluster centers (transform through PCA and t-SNE)\n",
    "# Note: This is approximate as t-SNE is non-linear\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "centers_2d = tsne.fit_transform(np.vstack([embeddings_pca, centers_pca]))[-analysis_config[\"n_clusters\"]:]\n",
    "plt.scatter(\n",
    "    centers_2d[:, 0],\n",
    "    centers_2d[:, 1],\n",
    "    c='red',\n",
    "    marker='X',\n",
    "    s=300,\n",
    "    edgecolors='black',\n",
    "    linewidth=2,\n",
    "    label='Cluster Centers',\n",
    "    zorder=10\n",
    ")\n",
    "\n",
    "# Annotate sequences with labels\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.annotate(\n",
    "        sequence_labels[i],\n",
    "        (x, y),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.title(f'K-Means Clustering (k={analysis_config[\"n_clusters\"]})', fontsize=14, pad=15)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Clustering Insights:\")\n",
    "print(f\"  â€¢ Sequences in the same cluster share similar genomic features\")\n",
    "print(f\"  â€¢ Clustering is unsupervised - no labels were used\")\n",
    "print(f\"  â€¢ Can be used for automated sequence classification and annotation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf09ec",
   "metadata": {},
   "source": [
    "### 4.4: Biological Interpretation and Applications\n",
    "\n",
    "Now let's interpret our computational results from a biological perspective and explore practical applications of genomic embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5353aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biological Interpretation of Results\n",
    "print(\"ðŸ§¬ BIOLOGICAL INTERPRETATION OF EMBEDDING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Analyze intra-category similarity\n",
    "print(\"\\n1ï¸âƒ£  Intra-Category Similarity (Are similar RNAs grouped together?)\")\n",
    "print(\"-\" * 70)\n",
    "for category in categories_unique:\n",
    "    cat_indices = [i for i, cat in enumerate(sequence_categories) if cat == category]\n",
    "    if len(cat_indices) > 1:\n",
    "        # Calculate average pairwise similarity within category\n",
    "        intra_sim = []\n",
    "        for i in cat_indices:\n",
    "            for j in cat_indices:\n",
    "                if i < j:\n",
    "                    intra_sim.append(similarity_matrix[i, j])\n",
    "        \n",
    "        avg_intra_sim = np.mean(intra_sim) if intra_sim else 0\n",
    "        print(f\"  {category}: {avg_intra_sim:.4f}\")\n",
    "        print(f\"    Interpretation: {'High functional similarity detected' if avg_intra_sim > 0.7 else 'Moderate diversity within category'}\")\n",
    "\n",
    "# 2. Identify functionally related sequences across categories\n",
    "print(f\"\\n2ï¸âƒ£  Cross-Category Relationships (Unexpected similarities)\")\n",
    "print(\"-\" * 70)\n",
    "cross_cat_pairs = []\n",
    "for i in range(len(sequence_labels)):\n",
    "    for j in range(i+1, len(sequence_labels)):\n",
    "        if sequence_categories[i] != sequence_categories[j]:\n",
    "            cross_cat_pairs.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "cross_cat_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for i, j, sim in cross_cat_pairs[:3]:\n",
    "    print(f\"  {sequence_labels[i]} â†” {sequence_labels[j]}\")\n",
    "    print(f\"    Similarity: {sim:.4f}\")\n",
    "    print(f\"    Categories: {sequence_categories[i]} vs {sequence_categories[j]}\")\n",
    "    print(f\"    Possible reason: Shared sequence motifs or structural patterns\\n\")\n",
    "\n",
    "# 3. Application examples\n",
    "print(f\"\\n3ï¸âƒ£  Practical Applications of These Embeddings\")\n",
    "print(\"-\" * 70)\n",
    "applications = [\n",
    "    (\"Sequence Database Search\", \"Find functionally similar sequences to a query\"),\n",
    "    (\"Functional Annotation\", \"Predict function of unknown sequences by nearest neighbors\"),\n",
    "    (\"Evolutionary Analysis\", \"Study sequence relationships without alignment\"),\n",
    "    (\"Drug Target Discovery\", \"Identify related therapeutic targets\"),\n",
    "    (\"Synthetic Biology\", \"Design new sequences with desired properties\"),\n",
    "    (\"Quality Control\", \"Detect contamination or misannotated sequences\"),\n",
    "]\n",
    "\n",
    "for app_name, app_desc in applications:\n",
    "    print(f\"  â€¢ {app_name}\")\n",
    "    print(f\"    â””â”€ {app_desc}\")\n",
    "\n",
    "# 4. Model capabilities demonstrated\n",
    "print(f\"\\n4ï¸âƒ£  What the Model Learned (Without Explicit Training)\")\n",
    "print(\"-\" * 70)\n",
    "learned_features = [\n",
    "    \"Sequence motifs and patterns (k-mers, repeats)\",\n",
    "    \"Structural propensities (hairpins, loops, stems)\",\n",
    "    \"Functional relationships (coding vs regulatory)\",\n",
    "    \"Evolutionary conservation signals\",\n",
    "    \"Compositional biases (GC content, codon usage)\",\n",
    "]\n",
    "\n",
    "for feature in learned_features:\n",
    "    print(f\"  âœ“ {feature}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight:\")\n",
    "print(f\"  The genomic foundation model captured biologically meaningful\")\n",
    "print(f\"  relationships without any task-specific training. This is the\")\n",
    "print(f\"  power of pre-training on large-scale genomic data!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4b62c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Step 5: Working with Single Sequences and Practical APIs\n",
    "\n",
    "Now that we understand the big picture, let's explore practical APIs for everyday use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cc84d",
   "metadata": {},
   "source": [
    "### 5.1: Computing Similarity Between Two Sequences\n",
    "\n",
    "A common task is to check if two sequences are functionally related by computing their cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29807a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two sequences from our analysis\n",
    "seq1_idx = 0  # tRNA-Ala\n",
    "seq2_idx = 1  # rRNA-18S\n",
    "\n",
    "# Method 1: Using pre-computed embeddings\n",
    "similarity = embedding_model.compute_similarity(\n",
    "    genomic_embeddings[seq1_idx],\n",
    "    genomic_embeddings[seq2_idx]\n",
    ")\n",
    "\n",
    "print(f\"Comparing: {sequence_labels[seq1_idx]} vs {sequence_labels[seq2_idx]}\")\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if similarity > 0.8:\n",
    "    print(\"  â†’ Very high similarity (likely functionally related)\")\n",
    "elif similarity > 0.6:\n",
    "    print(\"  â†’ Moderate similarity (may share some properties)\")\n",
    "else:\n",
    "    print(\"  â†’ Low similarity (functionally distinct)\")\n",
    "\n",
    "# Method 2: Direct comparison of two new sequences\n",
    "new_seq1 = \"AUGCGAUCGAUCGAU\"\n",
    "new_seq2 = \"AUGCGAUCGAUUUUU\"\n",
    "\n",
    "emb1 = embedding_model.encode(new_seq1, agg='mean')\n",
    "emb2 = embedding_model.encode(new_seq2, agg='mean')\n",
    "new_similarity = embedding_model.compute_similarity(emb1, emb2)\n",
    "\n",
    "print(f\"\\n\\nNew sequence comparison:\")\n",
    "print(f\"Sequence 1: {new_seq1}\")\n",
    "print(f\"Sequence 2: {new_seq2}\")\n",
    "print(f\"Similarity: {new_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b80970",
   "metadata": {},
   "source": [
    "### 5.2: Understanding Aggregation Methods (head, mean, tail)\n",
    "\n",
    "Different aggregation methods capture different aspects of sequence information. Let's understand when to use each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182070e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single RNA sequence with all three aggregation methods\n",
    "single_rna_sequence = \"AUGGCUACGAUCGAUCGAU\"\n",
    "\n",
    "print(f\"ðŸ§¬ Analyzing sequence: {single_rna_sequence}\")\n",
    "print(f\"   Length: {len(single_rna_sequence)} nucleotides\\n\")\n",
    "\n",
    "# Get embeddings using different aggregation methods\n",
    "head_embedding = embedding_model.encode(single_rna_sequence, agg='head', keep_dim=True)\n",
    "mean_embedding = embedding_model.encode(single_rna_sequence, agg='mean')\n",
    "tail_embedding = embedding_model.encode(single_rna_sequence, agg='tail')\n",
    "\n",
    "print(f\"ðŸ“Š Embedding Shapes and Properties:\")\n",
    "print(f\"\\n1. HEAD aggregation (CLS token):\")\n",
    "print(f\"   Shape: {head_embedding.shape}\")\n",
    "print(f\"   Use case: Sequence-level classification tasks\")\n",
    "print(f\"   Captures: Global sequence identity and context\")\n",
    "print(f\"   Sample values: {head_embedding.squeeze()[:5].cpu().numpy()}\")\n",
    "\n",
    "print(f\"\\n2. MEAN aggregation (average pooling):\")\n",
    "print(f\"   Shape: {mean_embedding.shape}\")\n",
    "print(f\"   Use case: Similarity search, clustering (RECOMMENDED)\")\n",
    "print(f\"   Captures: Average properties across all positions\")\n",
    "print(f\"   Sample values: {mean_embedding[:5].cpu().numpy()}\")\n",
    "\n",
    "print(f\"\\n3. TAIL aggregation (last token):\")\n",
    "print(f\"   Shape: {tail_embedding.shape}\")\n",
    "print(f\"   Use case: Generative tasks, sequential modeling\")\n",
    "print(f\"   Captures: Final state after processing entire sequence\")\n",
    "print(f\"   Sample values: {tail_embedding[:5].cpu().numpy()}\")\n",
    "\n",
    "# Compare how different aggregations affect similarity\n",
    "test_seq2 = \"AUGGCUACGAUCGAUAAAA\"  # Similar prefix, different suffix\n",
    "\n",
    "similarities = {}\n",
    "for agg_method in ['head', 'mean', 'tail']:\n",
    "    emb1 = embedding_model.encode(single_rna_sequence, agg=agg_method)\n",
    "    emb2 = embedding_model.encode(test_seq2, agg=agg_method)\n",
    "    sim = embedding_model.compute_similarity(emb1, emb2)\n",
    "    similarities[agg_method] = sim\n",
    "\n",
    "print(f\"\\nðŸ”¬ Similarity Analysis with Different Aggregations:\")\n",
    "print(f\"   Comparing: {single_rna_sequence}\")\n",
    "print(f\"         vs: {test_seq2}\")\n",
    "print(f\"   (Note: Different suffix)\\n\")\n",
    "for method, sim in similarities.items():\n",
    "    print(f\"   {method.upper():>6}: {sim:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Recommendation: Use 'mean' for most applications\")\n",
    "print(f\"   It provides robust, position-invariant representations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88b0e0",
   "metadata": {},
   "source": [
    "### 5.3: Saving and Loading Embeddings\n",
    "\n",
    "For large-scale analyses, you'll want to save embeddings to avoid recomputing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07537d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "output_path = \"genomic_embeddings.pt\"\n",
    "embedding_model.save_embeddings(genomic_embeddings, output_path)\n",
    "print(f\"âœ… Embeddings saved to: {output_path}\")\n",
    "print(f\"   File size: {genomic_embeddings.element_size() * genomic_embeddings.nelement() / 1024:.2f} KB\")\n",
    "\n",
    "# Load embeddings from file\n",
    "loaded_embeddings = embedding_model.load_embeddings(output_path)\n",
    "print(f\"\\nâœ… Loaded embeddings from: {output_path}\")\n",
    "print(f\"   Shape: {loaded_embeddings.shape}\")\n",
    "print(f\"   Data type: {loaded_embeddings.dtype}\")\n",
    "\n",
    "# Verify integrity\n",
    "are_equal = torch.allclose(genomic_embeddings.cpu(), loaded_embeddings.cpu(), rtol=1e-5)\n",
    "print(f\"\\nðŸ” Integrity check: {'PASSED' if are_equal else 'FAILED'}\")\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\"\\nðŸ§¹ Cleaned up temporary file: {output_path}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Tip: For production, use numpy format for better compatibility:\")\n",
    "print(f\"   np.save('embeddings.npy', embeddings.cpu().numpy())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bebb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Reference: Complete Standalone Example\n",
    "\n",
    "This is a **self-contained** code block that summarizes all the key APIs. You can copy this to a new script and run it independently.\n",
    "\n",
    "**Note**: This is for reference only - you don't need to run this cell if you've completed the tutorial above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab16b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STANDALONE EXAMPLE: Genomic Embedding Extraction with OmniGenBench\n",
    "\n",
    "This example demonstrates all core APIs in a single script.\n",
    "Copy this to a .py file for production use.\n",
    "\"\"\"\n",
    "\n",
    "from omnigenbench import OmniModelForEmbedding\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"yangheng/OmniGenome-52M\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Step 1: Initialize model\n",
    "print(\"ðŸ”§ Loading model...\")\n",
    "embedding_model = OmniModelForEmbedding(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Move to device with optional mixed precision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = embedding_model.to(device)\n",
    "if device.type == \"cuda\":\n",
    "    embedding_model = embedding_model.to(torch.float16)\n",
    "\n",
    "print(f\"âœ… Model loaded on {device}\")\n",
    "\n",
    "# Step 2: Encode sequences (batch processing)\n",
    "rna_sequences = [\n",
    "    \"AUGGCUACG\",\n",
    "    \"CGGAUACGGC\",\n",
    "    \"AUGCGAUCGAUCGAU\"\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ“Š Encoding {len(rna_sequences)} sequences...\")\n",
    "rna_embeddings = embedding_model.batch_encode(\n",
    "    rna_sequences,\n",
    "    batch_size=8,\n",
    "    agg='mean'  # Options: 'head', 'mean', 'tail'\n",
    ")\n",
    "\n",
    "print(f\"   Output shape: {rna_embeddings.shape}\")  # (3, 768)\n",
    "print(f\"   First embedding sample: {rna_embeddings[0][:5].cpu().numpy()}\")\n",
    "\n",
    "# Step 3: Compute similarity\n",
    "similarity = embedding_model.compute_similarity(\n",
    "    rna_embeddings[0],\n",
    "    rna_embeddings[1]\n",
    ")\n",
    "print(f\"\\nðŸ”¬ Similarity between seq1 and seq2: {similarity:.4f}\")\n",
    "\n",
    "# Step 4: Encode single sequence with different aggregations\n",
    "single_seq = \"AUGGCUACGAUCGAU\"\n",
    "head_emb = embedding_model.encode(single_seq, agg='head')\n",
    "mean_emb = embedding_model.encode(single_seq, agg='mean')\n",
    "tail_emb = embedding_model.encode(single_seq, agg='tail')\n",
    "\n",
    "print(f\"\\nðŸ“Š Single sequence embeddings:\")\n",
    "print(f\"   Head: {head_emb.shape}\")\n",
    "print(f\"   Mean: {mean_emb.shape}\")\n",
    "print(f\"   Tail: {tail_emb.shape}\")\n",
    "\n",
    "# Step 5: Save and load embeddings\n",
    "save_path = \"embeddings.pt\"\n",
    "embedding_model.save_embeddings(rna_embeddings, save_path)\n",
    "loaded_embs = embedding_model.load_embeddings(save_path)\n",
    "print(f\"\\nâœ… Saved and loaded embeddings: {loaded_embs.shape}\")\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "if os.path.exists(save_path):\n",
    "    os.remove(save_path)\n",
    "\n",
    "# Step 6: Extract attention scores (optional)\n",
    "# Note: Requires FP32 precision for numerical stability\n",
    "print(f\"\\nðŸŽ¯ Extracting attention scores...\")\n",
    "model_fp32 = OmniModelForEmbedding(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "attention_result = model_fp32.extract_attention_scores(\n",
    "    sequence=single_seq,\n",
    "    max_length=128,\n",
    "    layer_indices=[0, -1],  # First and last layers\n",
    ")\n",
    "print(f\"   Attention shape: {attention_result['attentions'].shape}\")\n",
    "print(f\"   Format: (layers, heads, seq_len, seq_len)\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Tutorial completed successfully!\")\n",
    "print(f\"\\nðŸ’¡ Remember: All OmniModel types support these embedding APIs!\")\n",
    "print(f\"   â€¢ OmniModelForEmbedding (dedicated)\")\n",
    "print(f\"   â€¢ OmniModelForSequenceClassification\")\n",
    "print(f\"   â€¢ OmniModelForSequenceRegression\")\n",
    "print(f\"   â€¢ OmniModelForTokenClassification\")\n",
    "print(f\"   ... and all other OmniModel variants via EmbeddingMixin!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd07430",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "### ðŸŽ“ What You've Learned\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through genomic embedding generation and analysis. Let's recap the key concepts and skills you've mastered:\n",
    "\n",
    "#### 1. **Conceptual Understanding**\n",
    "- âœ… What genomic embeddings are and why they matter\n",
    "- âœ… How genomic foundation models learn representations\n",
    "- âœ… The difference between traditional encoding (one-hot) and learned embeddings\n",
    "- âœ… Applications across drug discovery, evolutionary analysis, and synthetic biology\n",
    "\n",
    "#### 2. **Technical Skills**\n",
    "- âœ… Loading pre-trained genomic foundation models\n",
    "- âœ… Generating embeddings for RNA/DNA sequences (single and batch)\n",
    "- âœ… Understanding aggregation methods (head, mean, tail)\n",
    "- âœ… Computing sequence similarities\n",
    "- âœ… Saving and loading embeddings efficiently\n",
    "\n",
    "#### 3. **Analytical Capabilities**\n",
    "- âœ… Pairwise similarity analysis with heatmaps\n",
    "- âœ… Dimensionality reduction (PCA, t-SNE) for visualization\n",
    "- âœ… Unsupervised clustering to discover sequence groups\n",
    "- âœ… Biological interpretation of computational results\n",
    "\n",
    "#### 4. **Best Practices**\n",
    "- âœ… Setting random seeds for reproducibility\n",
    "- âœ… Using mixed precision (FP16) for GPU efficiency\n",
    "- âœ… Batch processing for large datasets\n",
    "- âœ… Proper error handling and resource cleanup\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Real-World Applications\n",
    "\n",
    "Your new skills enable you to tackle important biological problems:\n",
    "\n",
    "#### **Drug Discovery & Target Identification**\n",
    "```python\n",
    "# Find sequences similar to a therapeutic target\n",
    "target_seq = \"AUGCGA...\"\n",
    "target_emb = model.encode(target_seq)\n",
    "# Search database and rank by similarity\n",
    "```\n",
    "\n",
    "#### **Functional Annotation**\n",
    "```python\n",
    "# Predict function of unknown sequences\n",
    "unknown_emb = model.encode(unknown_seq)\n",
    "# Find k-nearest neighbors with known functions\n",
    "```\n",
    "\n",
    "#### **Quality Control**\n",
    "```python\n",
    "# Detect contamination or misannotation\n",
    "expected_emb = model.encode(expected_seq)\n",
    "observed_emb = model.encode(observed_seq)\n",
    "if model.compute_similarity(expected_emb, observed_emb) < 0.8:\n",
    "    flag_for_review()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Learning\n",
    "\n",
    "Explore these related tutorials to expand your expertise:\n",
    "\n",
    "#### **Advanced Topics**\n",
    "1. **[RNA Secondary Structure Prediction](../rna_secondary_structure_prediction/)** - Predict RNA folding patterns\n",
    "2. **[mRNA Degradation Rate Prediction](../mRNA_degrad_rate_regression/)** - Token-level regression tasks\n",
    "3. **[Attention Score Extraction](../attention_score_extraction/)** - Visualize model attention patterns\n",
    "4. **[Translation Efficiency Prediction](../translation_efficiency_prediction/)** - Predict protein production rates\n",
    "\n",
    "#### **Production Deployment**\n",
    "- **Fine-tuning models** on your custom datasets with `AutoTrain`\n",
    "- **Benchmarking** model performance with `AutoBench`\n",
    "- **Large-scale inference** with distributed computing\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Troubleshooting Guide\n",
    "\n",
    "#### **Common Issues and Solutions**\n",
    "\n",
    "| Issue | Possible Cause | Solution |\n",
    "|-------|---------------|----------|\n",
    "| `CUDA out of memory` | GPU memory insufficient | Reduce `batch_size` or use CPU |\n",
    "| `Model download fails` | Network issues | Check internet connection, try VPN |\n",
    "| `Import error` | Package not installed | Run `pip install omnigenbench -U` |\n",
    "| `Inconsistent results` | Random seed not set | Set `RANDOM_SEED` before analysis |\n",
    "| `Slow inference` | Not using GPU | Check `torch.cuda.is_available()` |\n",
    "\n",
    "#### **Performance Optimization**\n",
    "\n",
    "```python\n",
    "# For large-scale analysis (1000+ sequences):\n",
    "1. Use GPU with mixed precision (FP16)\n",
    "2. Increase batch_size to 32 or 64\n",
    "3. Save embeddings to disk to avoid recomputation\n",
    "4. Use multiprocessing for CPU-bound tasks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Key Takeaways\n",
    "\n",
    "1. **Genomic foundation models** have learned biologically meaningful representations from massive pre-training\n",
    "2. **Embeddings capture functional relationships** without task-specific training\n",
    "3. **All OmniModel types** support embedding extraction via `EmbeddingMixin`\n",
    "4. **Mean aggregation** is recommended for most applications\n",
    "5. **Reproducibility requires** setting random seeds and version control\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒŸ What's Next?\n",
    "\n",
    "You're now equipped to apply genomic embeddings to your research! Consider:\n",
    "\n",
    "1. **Applying to your own data**: Replace our example sequences with your sequences of interest\n",
    "2. **Exploring other models**: Try larger models like `OmniGenome-186M` or `OmniGenome-400M`\n",
    "3. **Fine-tuning**: Adapt the model to your specific task with `AutoTrain`\n",
    "4. **Contributing**: Share your findings and improvements with the community\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“– Additional Resources\n",
    "\n",
    "- **Documentation**: [OmniGenBench Docs](https://omnigenbench.readthedocs.io/)\n",
    "- **GitHub**: [yangheng95/OmniGenBench](https://github.com/yangheng95/OmniGenBench)\n",
    "- **Paper**: Yang et al. (2025) \"OmniGenome: Foundation Models for Genomic Understanding\"\n",
    "- **Model Hub**: [HuggingFace yangheng](https://huggingface.co/yangheng)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial! Happy researching! ðŸ§¬âœ¨**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
