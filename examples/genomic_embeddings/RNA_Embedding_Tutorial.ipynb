{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960e3c137296ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd9d837f",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Genomic Embeddings and Representation Learning with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to generate **high-quality genomic embeddings** from DNA and RNA sequences using **OmniGenBench**. This guide will walk you through the process of extracting meaningful vector representations from genomic sequences for downstream analysis and machine learning applications.\n",
    "\n",
    "### 1. The Computational Challenge: What are Genomic Embeddings?\n",
    "\n",
    "**Genomic embeddings** are dense vector representations that capture the semantic and functional information encoded in DNA and RNA sequences. These embeddings transform discrete nucleotide sequences into continuous vector spaces where similar sequences are positioned closer together.\n",
    "\n",
    "The power of genomic embeddings lies in their ability to:\n",
    "- **Capture Sequence Semantics**: Encode biological meaning and functional relationships\n",
    "- **Enable Similarity Analysis**: Find functionally related sequences through vector similarity\n",
    "- **Support Downstream ML**: Serve as input features for various machine learning tasks\n",
    "- **Compress Information**: Reduce high-dimensional sequence data to manageable representations\n",
    "\n",
    "Applications span across computational biology:\n",
    "- **Drug Discovery**: Finding target sequences and analyzing molecular interactions\n",
    "- **Evolutionary Analysis**: Studying sequence relationships and phylogenetic patterns  \n",
    "- **Functional Annotation**: Predicting sequence function from embedding similarity\n",
    "- **Biomarker Discovery**: Identifying disease-related sequence patterns\n",
    "\n",
    "### 2. The Data: From Sequences to Vectors\n",
    "\n",
    "Unlike traditional one-hot encoding, genomic foundation models learn rich representations that capture:\n",
    "\n",
    "- **Local Patterns**: k-mer frequencies, motifs, and short-range dependencies\n",
    "- **Global Context**: Long-range interactions and structural relationships  \n",
    "- **Functional Similarities**: Sequences with similar biological roles cluster together\n",
    "- **Evolutionary Relationships**: Homologous sequences have similar embeddings\n",
    "\n",
    "**Transformation Process:**\n",
    "\n",
    "| Raw Sequence | Traditional Encoding | Embedding Vector |\n",
    "|-------------|---------------------|------------------|\n",
    "| `ATGCGATCG` | `[1,0,0,0,0,1,0,0,...]` | `[0.23, -0.45, 0.12, ...]` |\n",
    "| `ATGCGTTCG` | `[1,0,0,0,0,1,0,1,...]` | `[0.21, -0.43, 0.15, ...]` |\n",
    "\n",
    "### 3. The Tool: Genomic Foundation Models for Representation Learning\n",
    "\n",
    "#### Pre-trained Understanding\n",
    "**OmniGenome** models are pre-trained on massive genomic datasets, learning to represent sequences in biologically meaningful vector spaces. This pre-training captures:\n",
    "\n",
    "1. **Sequence Patterns**: Common motifs, regulatory elements, and structural features\n",
    "2. **Functional Relationships**: Similar functions lead to similar representations\n",
    "3. **Evolutionary Context**: Related sequences cluster in embedding space\n",
    "4. **Multi-scale Information**: From local k-mers to global sequence properties\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Genomic Embeddings\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for Genomic Embeddings\"\n",
    "        A[\"ðŸ“¥ Step 1: Setup and Configuration<br/>Initialize models and prepare sequences\"] --> B[\"ðŸ”§ Step 2: Model Loading<br/>Load pre-trained genomic foundation models\"]\n",
    "        B --> C[\"ðŸŽ“ Step 3: Embedding Generation<br/>Extract vector representations from sequences\"]\n",
    "        C --> D[\"ðŸ”® Step 4: Analysis and Applications<br/>Analyze embeddings and explore applications\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start generating powerful genomic embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3bc0f",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Setup and Configuration\n",
    "\n",
    "This first step focuses on setting up our environment for genomic embedding generation and analysis.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required packages for genomic embedding generation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omnigenbench torch transformers scikit-learn matplotlib seaborn -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d3b0b",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries\n",
    "\n",
    "Next, we import the essential libraries for genomic embedding generation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaed138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import various model types - ALL support embedding and attention extraction!\n",
    "from omnigenbench import (\n",
    "    OmniModelForEmbedding,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForSequenceRegression,\n",
    "    ModelHub,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18138115",
   "metadata": {},
   "source": [
    "### 1.3: Global Configuration\n",
    "\n",
    "Let's define our configuration parameters for embedding generation and analysis.\n",
    "\n",
    "#### Key Parameters\n",
    "- **Model Selection**: Choose the appropriate genomic foundation model for embedding generation\n",
    "- **Analysis Settings**: Configure parameters for dimensionality reduction and clustering\n",
    "- **Visualization Options**: Set up parameters for embedding visualization and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for embedding generation and analysis\n",
    "embedding_config = {\n",
    "    \"model_name\": \"yangheng/OmniGenome-52M\",\n",
    "    \"aggregation_method\": \"mean\",  # Options: mean, max, cls\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "\n",
    "# Analysis configuration\n",
    "analysis_config = {\n",
    "    \"n_components_pca\": 50,\n",
    "    \"n_components_tsne\": 2,\n",
    "    \"n_clusters\": 5,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ Genomic Embedding Configuration:\")\n",
    "print(f\"  Model: {embedding_config['model_name']}\")\n",
    "print(f\"  Aggregation method: {embedding_config['aggregation_method']}\")\n",
    "print(f\"  Max sequence length: {embedding_config['max_length']}\")\n",
    "print(f\"\\nðŸ“Š Analysis Configuration:\")\n",
    "print(f\"  PCA components: {analysis_config['n_components_pca']}\")\n",
    "print(f\"  t-SNE components: {analysis_config['n_components_tsne']}\")\n",
    "print(f\"  Number of clusters: {analysis_config['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c3e9b",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Loading\n",
    "\n",
    "Now let's load the pre-trained genomic foundation model for embedding generation. \n",
    "\n",
    "### ðŸŽ¯ Important: All OmniModel Classes Support Embeddings!\n",
    "\n",
    "**All OmniGenBench models** now inherit from `EmbeddingMixin`, which means:\n",
    "- âœ… `OmniModelForEmbedding` - Dedicated embedding extraction\n",
    "- âœ… `OmniModelForSequenceClassification` - Classification + Embeddings\n",
    "- âœ… `OmniModelForSequenceRegression` - Regression + Embeddings  \n",
    "- âœ… `OmniModelForTokenClassification` - Token classification + Embeddings\n",
    "- âœ… **All other OmniModel variants** - Task-specific + Embeddings\n",
    "\n",
    "You can use **any** of these model types to extract embeddings and attention scores!\n",
    "\n",
    "### Model Features\n",
    "- **Pre-trained Understanding**: Leverages genomic foundation model knowledge\n",
    "- **Flexible Aggregation**: Multiple methods for sequence-to-vector conversion\n",
    "- **Batch Processing**: Efficient handling of multiple sequences\n",
    "- **GPU Acceleration**: Automatic CUDA optimization for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "print(\"ðŸ”§ Loading genomic foundation model for embeddings...\")\n",
    "\n",
    "# Option 1: Use dedicated embedding model\n",
    "embedding_model = OmniModelForEmbedding(\n",
    "    embedding_config[\"model_name\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Option 2: You can also use any other OmniModel type!\n",
    "# All of these models support the same embedding extraction methods:\n",
    "# classification_model = OmniModelForSequenceClassification.from_pretrained(\"path/to/model\")\n",
    "# regression_model = OmniModelForSequenceRegression.from_pretrained(\"path/to/model\")\n",
    "# They all have: .encode(), .batch_encode(), .extract_attention_scores(), etc.\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = embedding_model.to(device)\n",
    "\n",
    "# Use mixed precision for efficiency\n",
    "if device.type == \"cuda\":\n",
    "    embedding_model = embedding_model.to(torch.float16)\n",
    "    \n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"ðŸ“Š Model configuration:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Precision: {'float16' if device.type == 'cuda' else 'float32'}\")\n",
    "print(f\"  Model parameters: ~52M\")\n",
    "print(f\"\\nðŸ’¡ Note: All OmniModel types (Classification, Regression, etc.) support embedding extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3324a3e",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Embedding Generation\n",
    "\n",
    "Now let's generate embeddings for various types of genomic sequences. We'll use diverse sequences to demonstrate how the model captures different biological patterns and relationships.\n",
    "\n",
    "### Our Sequence Collection\n",
    "\n",
    "We'll analyze sequences with different characteristics:\n",
    "- **Functional RNAs**: tRNAs, rRNAs, and regulatory sequences\n",
    "- **Coding Sequences**: mRNAs encoding different proteins\n",
    "- **Regulatory Elements**: Promoters, enhancers, and UTRs\n",
    "- **Structural Variants**: Sequences with different folding properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80325ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse collection of genomic sequences for embedding analysis\n",
    "genomic_sequences = {\n",
    "    \"Functional RNAs\": {\n",
    "        \"tRNA-Ala\": \"GGGGGUAUAGCUCAGUGGUAGAGCGCGUGCCUUUGCAAGCACAAGAGUCUCGGGAGUCGUUGGUUCGAAUCACCGUACCCCCA\",\n",
    "        \"rRNA-18S\": \"CGGCUACCACAUCCAAGGAAGGCAGCAGGCGCGCAAAUUACCCACUCCCGACCCGGGGAGGGUAGUGGCGGUUCGCCAGGA\",\n",
    "        \"miRNA-21\": \"UAGCUUAUCAGACUGAUGUUGACUGUUGAAUCUCAUGGCAACACCAGUCGAUGGGCUGU\",\n",
    "    },\n",
    "    \"mRNA Sequences\": {\n",
    "        \"Insulin mRNA\": \"AUGCCGCGCAACGAGGCCUACACUGUGCGAACUGCUGCCUGCUGCUGCCCGCUGCUGCUGCUGGGCUCCGCCCGCCGAG\",\n",
    "        \"Hemoglobin mRNA\": \"AUGGUGGACGACGUGCUCGGCAAGAACGUCAACCACGUGAAGCUGGUGGUGGACGACGACGGCUGCGUGGGCAACUGC\",\n",
    "        \"p53 mRNA\": \"AUGGAGGAGCCGCAGUCAGAUCCUAGCGUCCGGGACGACACGCCAACCUGCUCUCCUGCCGUCCCCGCCAAGACCAGC\",\n",
    "    },\n",
    "    \"Regulatory Elements\": {\n",
    "        \"TATA promoter\": \"CGGCGCGCCAUAUAAAGCAUCGAGCGCGCACGUGCGCUGCGCGCGCGCUACGCGCGCAUGUGCGCGCACGUACGCGCG\",\n",
    "        \"Enhancer seq\": \"GCGCGCGCACGUGCGCACGUGCGCGCACGUGCGCGCGCACGUGCGCGCACGUGCGCGCACGUGCGCGCACGUGCGCGC\",\n",
    "        \"5'UTR\": \"GCGCGCCACCAAUGCGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCCACCAUGUGCGCGCC\",\n",
    "    },\n",
    "    \"Structural Variants\": {\n",
    "        \"Hairpin RNA\": \"CGGAAACCCUUUGGGAAACCCGGGAAACCCUUUGGGAAACCCGGGAAACCCUUUGGGAAACCCG\",\n",
    "        \"Repeat seq\": \"CACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACA\",\n",
    "        \"Random seq\": \"AUGCGAUCUCGAGCUACGUCGAUGCUAGCUCGAUGGCAUCCGAUUCGAGCUACGUCGAUGCUAG\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flatten sequences for batch processing\n",
    "all_sequences = []\n",
    "sequence_labels = []\n",
    "sequence_categories = []\n",
    "\n",
    "for category, sequences in genomic_sequences.items():\n",
    "    for label, sequence in sequences.items():\n",
    "        all_sequences.append(sequence)\n",
    "        sequence_labels.append(label)\n",
    "        sequence_categories.append(category)\n",
    "\n",
    "print(f\"ðŸ§¬ Prepared {len(all_sequences)} genomic sequences for embedding:\")\n",
    "for category, sequences in genomic_sequences.items():\n",
    "    print(f\"  {category}: {len(sequences)} sequences\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "print(f\"\\nðŸŽ“ Generating embeddings...\")\n",
    "print(f\"âš¡ Using batch processing with aggregation method: {embedding_config['aggregation_method']}\")\n",
    "\n",
    "# Process sequences in batches\n",
    "batch_size = embedding_config[\"batch_size\"]\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(all_sequences), batch_size):\n",
    "    batch_sequences = all_sequences[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.batch_encode(\n",
    "        batch_sequences, \n",
    "        agg=embedding_config[\"aggregation_method\"]\n",
    "    )\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "genomic_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "print(f\"âœ… Embedding generation completed!\")\n",
    "print(f\"ðŸ“Š Embedding matrix shape: {genomic_embeddings.shape}\")\n",
    "print(f\"  Number of sequences: {genomic_embeddings.shape[0]}\")\n",
    "print(f\"  Embedding dimension: {genomic_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27b1bc",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Analysis and Applications\n",
    "\n",
    "Now let's analyze our genomic embeddings to understand the relationships between sequences and explore various applications. This demonstrates the power of genomic foundation models in capturing biological meaning.\n",
    "\n",
    "### Analysis Pipeline\n",
    "\n",
    "Our comprehensive analysis includes:\n",
    "1. **Similarity Analysis**: Calculate pairwise similarities between sequences\n",
    "2. **Dimensionality Reduction**: Visualize embeddings in 2D space using PCA and t-SNE\n",
    "3. **Clustering**: Discover sequence groups with similar properties\n",
    "4. **Functional Analysis**: Interpret biological relationships captured by embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3031cb7",
   "metadata": {},
   "source": [
    "## Step 5: Computing Similarity Between RNA Sequences\n",
    "Let's compute the similarity between two RNA sequence embeddings using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccda5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity between the first two RNA sequence embeddings\n",
    "similarity = embedding_model.compute_similarity(loaded_embeddings[0], loaded_embeddings[1])\n",
    "\n",
    "# Display the similarity score\n",
    "print(f\"Similarity between the first two RNA sequences: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7feaac9",
   "metadata": {},
   "source": [
    "## Step 6: Encoding a Single RNA Sequence\n",
    "You can also encode a single RNA sequence into its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single RNA sequence\n",
    "single_rna_sequence = \"AUGGCUACG\"\n",
    "\n",
    "# Get the embedding for the single RNA sequence\n",
    "\n",
    "head_rna_embedding = embedding_model.encode(rna_sequences[0], agg='head', keep_dim=True)  # Encode a single RNA sequence\n",
    "mean_rna_embedding = embedding_model.encode(rna_sequences[0], agg='mean')  # Encode a single RNA sequence\n",
    "tail_rna_embedding = embedding_model.encode(rna_sequences[0], agg='tail')  # Encode a single RNA sequence\n",
    "\n",
    "# Display the embedding for the single RNA sequence\n",
    "print(\"Single RNA Sequence Embedding:\")\n",
    "print(head_rna_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bebb0",
   "metadata": {},
   "source": [
    "## Full Example\n",
    "Here's a complete example that walks through all the steps we covered in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab16b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import (\n",
    "    OmniModelForEmbedding,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForSequenceRegression\n",
    ")\n",
    "\n",
    "# Step 1: Initialize the model\n",
    "# ðŸŽ¯ ALL OmniModel types support embedding extraction!\n",
    "model_name = \"yangheng/OmniGenome-52M\"\n",
    "\n",
    "# Option A: Use dedicated embedding model\n",
    "embedding_model = OmniModelForEmbedding(model_name, trust_remote_code=True).to(torch.device(\"cuda\")).to(torch.float16)\n",
    "\n",
    "# Option B: Use classification model (also supports embeddings!)\n",
    "# embedding_model = OmniModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Option C: Use regression model (also supports embeddings!)\n",
    "# embedding_model = OmniModelForSequenceRegression.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Step 2: Encode RNA sequences (works with ANY OmniModel type!)\n",
    "rna_sequences = [\"AUGGCUACG\", \"CGGAUACGGC\"]\n",
    "rna_embeddings = embedding_model.batch_encode(rna_sequences)\n",
    "print(\"RNA Embeddings:\", rna_embeddings)\n",
    "\n",
    "# Step 3: Save embeddings to a file\n",
    "embedding_model.save_embeddings(rna_embeddings, \"rna_embeddings.pt\")\n",
    "\n",
    "# Step 4: Load embeddings from the file\n",
    "loaded_embeddings = embedding_model.load_embeddings(\"rna_embeddings.pt\")\n",
    "\n",
    "# Step 5: Compute similarity between the first two RNA sequence embeddings\n",
    "similarity = embedding_model.compute_similarity(loaded_embeddings[0], loaded_embeddings[1])\n",
    "print(f\"Similarity between RNA sequences: {similarity:.4f}\")\n",
    "\n",
    "# Step 6: Encode a single RNA sequence\n",
    "single_rna_sequence = \"AUGGCUACG\"\n",
    "single_rna_embedding = embedding_model.encode(single_rna_sequence)\n",
    "print(\"Single RNA Sequence Embedding:\", single_rna_embedding)\n",
    "\n",
    "# Step 7: Extract attention scores (also available on ALL OmniModel types!)\n",
    "attention_result = embedding_model.extract_attention_scores(\n",
    "    sequence=rna_sequences[0],\n",
    "    max_length=128,\n",
    "    layer_indices=[0, -1],  # First and last layer\n",
    ")\n",
    "print(f\"\\nAttention scores shape: {attention_result['attentions'].shape}\")\n",
    "print(f\"ðŸ’¡ Embedding and attention extraction work with ALL OmniModel types!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
