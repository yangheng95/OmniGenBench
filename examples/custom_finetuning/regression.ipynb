{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Fine‑Tuning Composition for Regression Tasks\n",
    "\n",
    "This notebook introduces the custom fine-tuning of OmniGenome models for regression tasks.\n",
    "\n",
    "## Task Formulation\n",
    "\n",
    "Regression tasks in OmniGenome can be:\n",
    "- **Sequence Regression**: Predict a continuous value for the entire sequence\n",
    "- **Token Regression**: Predict continuous values for each token (e.g., nucleotide) in the sequence\n",
    "\n",
    "### Simple Model APIs\n",
    "```\n",
    "from omnigenbench import (\n",
    "    OmniModelForSequenceRegression,\n",
    "    OmniModelForTokenRegression,\n",
    "    OmniModelForMatrixRegression,\n",
    "    OmniModelForStructuralImputation,\n",
    ")\n",
    "```\n",
    "\n",
    "### Simple Dataset APIs\n",
    "```\n",
    "from omnigenbench import (\n",
    "    OmniDatasetForSequenceRegression,\n",
    "    OmniDatasetForTokenRegression,\n",
    ")\n",
    "```\n",
    "\n",
    "Otherwise, you can define your own model and dataset classes by inheriting from the base classes provided in OmniGenome. We will demonstrate how to do this in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q omnigenbench torch datasets scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we will introduce the custom finetuning of OmniGenome models for regression tasks.\n",
    "model_name = \"yangheng/OmniGenome-52M\" # 52M parameters\n",
    "\n",
    "# 1. Load the model and tokenizer according to model_name for later use\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)  # trust_remote_code=True is used to load the model from the remote repository, which is necessary for OmniGenome models\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# OmniGenome-52M tokenizer can be initialized by AutoTokenizer, for other models, you can use can define a wrapper to initialize the tokenizer to be compatible with transformers tokenizer APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Model with a Regression Head for Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a sequence regression task (predicts a single value for the entire sequence)\n",
    "from omnigenbench import OmniModelForSequenceRegression\n",
    "model = OmniModelForSequenceRegression(\n",
    "    config_or_model=model_name,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    num_labels=1,  # Single regression value\n",
    ")\n",
    "\n",
    "# For a token regression task (predicts values for each token in the sequence)\n",
    "from omnigenbench import OmniModelForTokenRegression\n",
    "model = OmniModelForTokenRegression(\n",
    "    config_or_model=model_name,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    num_labels=3,  # Multiple regression targets (e.g., reactivity, deg_Mg_pH10, deg_Mg_50C)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Define a Custom Model for Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import OmniModel, OmniPooling\n",
    "import torch\n",
    "\n",
    "class OmniModelForSequenceRegression(OmniModel):\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = self.__class__.__name__\n",
    "        self.pooler = OmniPooling(self.config)\n",
    "        self.regressor = torch.nn.Linear(\n",
    "            self.config.hidden_size, self.config.num_labels\n",
    "        )\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "        last_hidden_state = self.pooler(inputs, last_hidden_state)\n",
    "        predictions = self.regressor(last_hidden_state)\n",
    "        outputs = {\n",
    "            \"logits\": predictions,  # For regression, logits are the predictions\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        predictions = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        outputs = {\n",
    "            \"predictions\": predictions,\n",
    "            \"logits\": predictions,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        predictions = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        if not isinstance(sequence_or_inputs, list):\n",
    "            outputs = {\n",
    "                \"predictions\": predictions[0],\n",
    "                \"logits\": predictions[0],\n",
    "                \"last_hidden_state\": last_hidden_state[0],\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"predictions\": predictions,\n",
    "                \"logits\": predictions,\n",
    "                \"last_hidden_state\": last_hidden_state,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        loss = self.loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1, self.config.num_labels))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class OmniModelForTokenRegression(OmniModel):\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = self.__class__.__name__\n",
    "        self.regressor = torch.nn.Linear(\n",
    "            self.config.hidden_size, self.config.num_labels\n",
    "        )\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "        predictions = self.regressor(last_hidden_state)\n",
    "        outputs = {\n",
    "            \"logits\": predictions,  # For regression, logits are the predictions\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        predictions = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        outputs = {\n",
    "            \"predictions\": predictions,\n",
    "            \"logits\": predictions,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        inputs = raw_outputs[\"inputs\"]\n",
    "        predictions = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        # Filter out padding tokens for token-level predictions\n",
    "        filtered_predictions = []\n",
    "        for i in range(predictions.shape[0]):\n",
    "            i_pred = predictions[i][inputs[\"input_ids\"][i].ne(self.config.pad_token_id)][1:-1]\n",
    "            filtered_predictions.append(i_pred.detach().cpu())\n",
    "\n",
    "        if not isinstance(sequence_or_inputs, list):\n",
    "            outputs = {\n",
    "                \"predictions\": filtered_predictions[0],\n",
    "                \"logits\": predictions[0],\n",
    "                \"last_hidden_state\": last_hidden_state[0],\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"predictions\": filtered_predictions,\n",
    "                \"logits\": predictions,\n",
    "                \"last_hidden_state\": last_hidden_state,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        padding_value = (\n",
    "            self.config.ignore_y if hasattr(self.config, \"ignore_y\") else -100\n",
    "        )\n",
    "        logits = logits.view(-1, self.config.num_labels)\n",
    "        labels = labels.view(-1, self.config.num_labels)\n",
    "        mask = torch.where(labels != padding_value)[0]\n",
    "\n",
    "        filtered_logits = logits[mask]\n",
    "        filtered_targets = labels[mask]\n",
    "\n",
    "        loss = self.loss_fn(filtered_logits, filtered_targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Dataset for Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a sequence regression task\n",
    "from omnigenbench import OmniDatasetForSequenceRegression\n",
    "# For a token regression task\n",
    "from omnigenbench import OmniDatasetForTokenRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Define a Custom Dataset for Downstream Task\n",
    "\n",
    "### To define a custom dataset for a token regression task, you can inherit from `OmniDatasetForTokenRegression` and implement the `prepare_input` method to process the input data.\n",
    "Make sure your dataset is in a format compatible with the tokenizer API, returning tokenized inputs and labels.\n",
    "```\n",
    "class Dataset(OmniDatasetForTokenRegression):\n",
    "    def __init__(self, data_source, tokenizer, max_length, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "        instance[\"sequence\"] = f'{instance[\"sequence\"]}'\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            instance[\"sequence\"],\n",
    "            padding=kwargs.get(\"padding\", \"do_not_pad\"),\n",
    "            truncation=kwargs.get(\"truncation\", True),\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = [instance[target_col] for target_col in target_cols]\n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                np.array(labels),\n",
    "                np.array(\n",
    "                    [\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).T\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "        return tokenized_inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset according to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"toy_datasets/RNA-mRNA/\"  # Path to your dataset files\n",
    "# RNA-mRNA is an mRNA degradation regression dataset (token regression), containing train.json, test.json, and valid.json files\n",
    "train_file = dataset_path + \"train.json\"\n",
    "test_file = dataset_path + \"test.json\"\n",
    "valid_file = dataset_path + \"valid.json\"\n",
    "\n",
    "# For token regression (predicting values for each token)\n",
    "train_set = OmniDatasetForTokenRegression(\n",
    "    data_source=train_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=128,  # Set the maximum sequence length\n",
    ")\n",
    "test_set = OmniDatasetForTokenRegression(\n",
    "    data_source=test_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=128,  # Set the maximum sequence length\n",
    ")\n",
    "valid_set = OmniDatasetForTokenRegression(\n",
    "    data_source=valid_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=128,  # Set the maximum sequence length\n",
    ")\n",
    "\n",
    "# For sequence regression (predicting a single value for the entire sequence)\n",
    "# train_set = OmniDatasetForSequenceRegression(\n",
    "#     data_source=train_file,\n",
    "#     tokenizer=base_tokenizer,\n",
    "#     max_length=128,\n",
    "# )\n",
    "# test_set = OmniDatasetForSequenceRegression(\n",
    "#     data_source=test_file,\n",
    "#     tokenizer=base_tokenizer,\n",
    "#     max_length=128,\n",
    "# )\n",
    "# valid_set = OmniDatasetForSequenceRegression(\n",
    "#     data_source=valid_file,\n",
    "#     tokenizer=base_tokenizer,\n",
    "#     max_length=128,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import RegressionMetric  # contains all metrics from sklearn.metrics and some custom metrics for regression tasks\n",
    "from omnigenbench import Trainer\n",
    "import torch\n",
    "\n",
    "# necessary hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "batch_size = 8\n",
    "max_length = 128\n",
    "seeds = [45]  # Each seed will be used for one run\n",
    "\n",
    "# Regression metrics\n",
    "compute_metrics = [\n",
    "    RegressionMetric(ignore_y=-100).mean_squared_error,  # MSE\n",
    "    RegressionMetric(ignore_y=-100).mean_absolute_error,  # MAE\n",
    "    RegressionMetric(ignore_y=-100).r2_score,  # R² score\n",
    "    RegressionMetric(ignore_y=-100).mcrmse,  # Mean Columnwise Root Mean Squared Error (custom metric)\n",
    "]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "for seed in seeds:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        seeds=seed,\n",
    "    )\n",
    "\n",
    "    metrics = trainer.train()\n",
    "    test_metrics = metrics[\"test\"][-1]\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After training, we evaluate the model on the validation set with regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        outputs = model(**batch)\n",
    "        predictions = outputs['logits']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Filter out padding tokens\n",
    "        mask = labels != -100\n",
    "        filtered_preds = predictions[mask]\n",
    "        filtered_labels = labels[mask]\n",
    "        \n",
    "        all_predictions.append(filtered_preds.cpu().numpy())\n",
    "        all_labels.append(filtered_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(all_labels, all_predictions)\n",
    "mae = mean_absolute_error(all_labels, all_predictions)\n",
    "r2 = r2_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_labels.flatten(), all_predictions.flatten(), alpha=0.5)\n",
    "plt.plot([all_labels.min(), all_labels.max()], [all_labels.min(), all_labels.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = all_predictions.flatten() - all_labels.flatten()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_predictions.flatten(), residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"OmniGenome-52M-Regression\"\n",
    "model.save(path_to_save, overwrite=True)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = model.load(path_to_save)\n",
    "results = model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "print(\"Predictions:\", results[\"predictions\"])\n",
    "print(\"Logits:\", results[\"logits\"])\n",
    "\n",
    "# We can load the model checkpoint using the ModelHub\n",
    "from omnigenbench import ModelHub\n",
    "\n",
    "# Example: Load a pre-trained regression model\n",
    "# regression_model = ModelHub.load(\"OmniGenome-186M-Regression\")\n",
    "# results = regression_model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "# print(\"Predictions:\", results[\"predictions\"])\n",
    "# print(\"Logits:\", results[\"logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction Explanation\n",
    "\n",
    "For regression tasks, the model outputs continuous values:\n",
    "\n",
    "- **Sequence Regression**: Returns a single continuous value for the entire sequence\n",
    "- **Token Regression**: Returns continuous values for each token in the sequence\n",
    "\n",
    "The predictions can be interpreted based on your specific task:\n",
    "- mRNA degradation rates\n",
    "- Protein expression levels\n",
    "- Binding affinities\n",
    "- Structural properties\n",
    "\n",
    "### Example Use Cases:\n",
    "1. **mRNA Degradation Prediction**: Predict degradation rates for each nucleotide\n",
    "2. **Protein Expression Prediction**: Predict expression levels from promoter sequences\n",
    "3. **Binding Affinity Prediction**: Predict binding strengths between molecules\n",
    "4. **Structural Property Prediction**: Predict structural characteristics of biomolecules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
