{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe79169e",
   "metadata": {},
   "source": [
    "# ðŸ¤– VEP Model Setup Tutorial\n",
    "\n",
    "This tutorial demonstrates model initialization and configuration for Variant Effect Prediction using genomic foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee1ac2",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification\n",
    ")\n",
    "from autocuda import auto_cuda\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸŽ¯ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "class VEPModelConfig:\n",
    "    MODEL_NAME = \"yangheng/OmniGenome-52M\"\n",
    "    NUM_LABELS = 2  # Binary: functional vs neutral\n",
    "    DEVICE = auto_cuda()\n",
    "    \n",
    "    # Model parameters\n",
    "    OUTPUT_HIDDEN_STATES = True  # Need for embedding extraction\n",
    "    OUTPUT_ATTENTIONS = False    # Optional for attention analysis\n",
    "\n",
    "config = VEPModelConfig()\n",
    "print(f\"âš™ï¸ Model configuration: {config.MODEL_NAME}\")\n",
    "print(f\"ðŸ“± Target device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6bbf8",
   "metadata": {},
   "source": [
    "## 2. Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"ðŸ”„ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
    "print(\"âœ… Tokenizer loaded!\")\n",
    "\n",
    "# Load genomic foundation model\n",
    "print(\"ðŸ”„ Loading genomic foundation model...\")\n",
    "model = OmniModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=config.NUM_LABELS,\n",
    "    trust_remote_code=True,\n",
    "    output_hidden_states=config.OUTPUT_HIDDEN_STATES,\n",
    "    output_attentions=config.OUTPUT_ATTENTIONS\n",
    ")\n",
    "\n",
    "# Move to device and set evaluation mode\n",
    "model.to(config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"  ðŸ§  Model: {config.MODEL_NAME}\")\n",
    "print(f\"  ðŸ“Š Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  ðŸ“± Device: {config.DEVICE}\")\n",
    "print(f\"  ðŸŽ¯ Labels: {config.NUM_LABELS} (Binary classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f49e5",
   "metadata": {},
   "source": [
    "## 3. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample sequences\n",
    "test_sequences = [\n",
    "    \"ATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",  # Reference\n",
    "    \"ATCGATCGATCGGTCGATCGATCGATCGATCGATCGATCG\",  # Variant (A->G)\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing model with sample sequences...\")\n",
    "\n",
    "for i, sequence in enumerate(test_sequences):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        sequence,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract information\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "    \n",
    "    print(f\"\\n  Sequence {i+1}: {sequence[:30]}...\")\n",
    "    print(f\"    ðŸŽ¯ Logits: {logits.cpu().numpy().flatten()}\")\n",
    "    print(f\"    ðŸ“Š Probabilities: {probabilities.cpu().numpy().flatten()}\")\n",
    "    print(f\"    ðŸ§  Hidden state shape: {hidden_states.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Model testing complete!\")\n",
    "print(\"ðŸŽ‰ Model is ready for variant effect prediction!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
