{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8a215f",
   "metadata": {},
   "source": [
    "# üß¨ VEP Tutorial 1/4: From Genetic Variants to Predictive Models\n",
    "\n",
    "Welcome to the first tutorial in our four-part Variant Effect Prediction (VEP) series. This guide focuses on understanding genetic variants and preparing data for computational analysis.\n",
    "\n",
    "> üìö **Prerequisites**: It is recommended to study the **[Fundamental Concepts Tutorial](../../00_fundamental_concepts.ipynb)** to understand machine learning task classification and foundation model principles, particularly how models like PlantRNA-FM work.\n",
    "\n",
    "Before diving into code, let's understand what genetic variants are and why predicting their effects is crucial for genomic medicine.\n",
    "\n",
    "## 1. Understanding Genetic Variants: The Language of Diversity üß¨\n",
    "\n",
    "### 1.1 What Are Genetic Variants?\n",
    "\n",
    "**Genetic variants** are differences in DNA sequences between individuals. They range from:\n",
    "- **Single Nucleotide Variants (SNVs)**: A single base change (e.g., A‚ÜíG)\n",
    "- **Insertions/Deletions (InDels)**: Added or removed DNA segments\n",
    "- **Structural Variants**: Large-scale chromosomal rearrangements\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Reference Genome<br/>...ATCG...] --> B[SNV<br/>...AGCG...]\n",
    "    A --> C[Insertion<br/>...ATCCG...]\n",
    "    A --> D[Deletion<br/>...A_CG...]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#ffe1e1\n",
    "    style C fill:#e1ffe1\n",
    "    style D fill:#fff5e1\n",
    "```\n",
    "\n",
    "### 1.2 Why Variant Effect Prediction Matters\n",
    "\n",
    "Understanding variant effects is critical for:\n",
    "- üè• **Clinical Diagnostics**: Identifying disease-causing mutations\n",
    "- üíä **Personalized Medicine**: Tailoring treatments based on genetic profiles\n",
    "- üß™ **Drug Discovery**: Understanding protein-drug interactions\n",
    "- üåæ **Agriculture**: Breeding crops with improved traits\n",
    "- üî¨ **Evolutionary Biology**: Understanding species adaptation\n",
    "\n",
    "However, experimentally validating millions of variants is **impractical and expensive**. This is where computational prediction becomes essential.\n",
    "\n",
    "### 1.3 The VEP Challenge: Classification vs. Interpretation\n",
    "\n",
    "Unlike other genomic tasks, VEP is unique because:\n",
    "1. **Zero-shot learning**: We often don't have labeled training data for specific variants\n",
    "2. **Embedding-based approach**: We compare sequence embeddings rather than training classifiers\n",
    "3. **Reference vs. alternative**: We analyze the *difference* between wild-type and mutant sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64f1d43",
   "metadata": {},
   "source": [
    "## 2. Framing VEP as a Machine Learning Task üéØ\n",
    "\n",
    "### 2.1 VEP in the ML Task Taxonomy\n",
    "\n",
    "| Aspect | Description |\n",
    "| :--- | :--- |\n",
    "| **Primary Task** | Sequence Comparison & Scoring |\n",
    "| **Input Format** | Paired sequences (reference + alternative) |\n",
    "| **Output Format** | Effect score (continuous) or classification (binary/multi-class) |\n",
    "| **Learning Paradigm** | Zero-shot or few-shot learning |\n",
    "| **Model Usage** | Feature extraction (embeddings) |\n",
    "\n",
    "### 2.2 Two Approaches to VEP\n",
    "\n",
    "**Approach 1: Supervised Classification (Traditional)**\n",
    "```\n",
    "Sequence ‚Üí Model ‚Üí Pathogenic/Benign Label\n",
    "```\n",
    "*Requires*: Large labeled dataset of known variant effects\n",
    "\n",
    "**Approach 2: Embedding-Based Scoring (This Tutorial)**\n",
    "```\n",
    "Reference Sequence ‚Üí Model ‚Üí Embedding‚ÇÅ\n",
    "Alternative Sequence ‚Üí Model ‚Üí Embedding‚ÇÇ\n",
    "Compare(Embedding‚ÇÅ, Embedding‚ÇÇ) ‚Üí Effect Score\n",
    "```\n",
    "*Requires*: Only reference genome and variant coordinates\n",
    "\n",
    "### 2.3 Why Embedding-Based VEP?\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- No labeled training data required\n",
    "- Works for novel variants\n",
    "- Captures semantic differences\n",
    "- Transferable across organisms\n",
    "\n",
    "‚ö†Ô∏è **Considerations:**\n",
    "- Requires high-quality pre-trained models\n",
    "- May need calibration for clinical use\n",
    "- Interpretation requires biological context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a1f14",
   "metadata": {},
   "source": [
    "## 3. The OmniGenBench VEP Workflow üîÑ\n",
    "\n",
    "Our four-part tutorial follows the standard OmniGenBench methodology:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üìä Data<br/>Preparation] --> B[ü§ñ Model<br/>Setup]\n",
    "    B --> C[üß¨ Embedding &<br/>Scoring]\n",
    "    C --> D[üìà Visualization &<br/>Export]\n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#ffe1f5\n",
    "    style C fill:#f5ffe1\n",
    "    style D fill:#ffe1e1\n",
    "```\n",
    "\n",
    "| Tutorial | Focus | Key Concepts |\n",
    "| :--- | :--- | :--- |\n",
    "| **Part 1** (This) | Data Preparation | Variant formats, data loading, quality control |\n",
    "| **Part 2** | Model Setup | PlantRNA-FM (35M, *Nature MI*) initialization, embedding extraction |\n",
    "| **Part 3** | Scoring Methods | Cosine similarity, effect score calculation |\n",
    "| **Part 4** | Analysis | Visualization, interpretation, export |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d5df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Step-by-Step Guide: Preparing Variant Data\n",
    "\n",
    "Now let's move from theory to practice. This section guides you through loading and processing variant data.\n",
    "\n",
    "### 4.1: Environment Setup\n",
    "\n",
    "First, install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9573905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OmniGenBench and dependencies\n",
    "%pip install omnigenbench -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53221ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForSequenceClassification\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üß¨ Ready to load variant data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcecf8a",
   "metadata": {},
   "source": [
    "## üìã Understanding VEP Data Formats\n",
    "\n",
    "Before loading data, let's understand how variant data is structured in OmniGenBench.\n",
    "\n",
    "### 4.2: VEP Data Templates\n",
    "\n",
    "VEP data follows the standard OmniGenBench directory structure:\n",
    "\n",
    "```\n",
    "variant_effect_prediction/\n",
    "‚îú‚îÄ‚îÄ train.jsonl              # Training variants (if available)\n",
    "‚îú‚îÄ‚îÄ valid.jsonl              # Validation variants (optional)\n",
    "‚îú‚îÄ‚îÄ test.jsonl               # Test variants (required)\n",
    "‚îú‚îÄ‚îÄ config.py                # Dataset metadata\n",
    "‚îî‚îÄ‚îÄ README.md                # Documentation\n",
    "```\n",
    "\n",
    "### 4.3: Variant Data Format\n",
    "\n",
    "**Option 1: JSONL Format (Recommended)**\n",
    "```json\n",
    "{\"sequence\": \"ATCG...GCTA\", \"ref_allele\": \"A\", \"alt_allele\": \"G\", \"label\": 1}\n",
    "{\"sequence\": \"GCTA...ATCG\", \"ref_allele\": \"C\", \"alt_allele\": \"T\", \"label\": 0}\n",
    "```\n",
    "\n",
    "**Option 2: CSV Format**\n",
    "```csv\n",
    "chr,pos,ref,alt,sequence,label\n",
    "chr1,12345,A,G,ATCG...GCTA,1\n",
    "chr2,67890,C,T,GCTA...ATCG,0\n",
    "```\n",
    "\n",
    "### 4.4: Label Conventions\n",
    "\n",
    "For VEP, labels typically represent:\n",
    "- **Binary**: 0 = Benign, 1 = Pathogenic\n",
    "- **Multi-class**: 0 = Benign, 1 = Likely benign, 2 = VUS, 3 = Likely pathogenic, 4 = Pathogenic\n",
    "- **Regression**: Continuous effect scores\n",
    "\n",
    "### 4.5: Sequence Context\n",
    "\n",
    "Variants are embedded in genomic context:\n",
    "```\n",
    "[Upstream Context] [Reference Allele] [Downstream Context]\n",
    "                          ‚Üì\n",
    "[Upstream Context] [Alternative Allele] [Downstream Context]\n",
    "```\n",
    "\n",
    "**Context window size** (e.g., 200bp upstream/downstream) is crucial for capturing regulatory effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233b02f",
   "metadata": {},
   "source": [
    "## 5. Configuration\n",
    "\n",
    "Define analysis parameters with clear documentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "@dataclass\n",
    "class VEPConfig:\n",
    "    \"\"\"Configuration for Variant Effect Prediction pipeline\"\"\"\n",
    "    # Dataset settings\n",
    "    dataset_name: str = \"yangheng/variant_effect_prediction\"\n",
    "    cache_dir: str = \"vep_data\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"yangheng/OmniGenome-52M\"\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Analysis settings\n",
    "    context_length: int = 200  # Base pairs upstream/downstream of variant\n",
    "    batch_size: int = 16\n",
    "    \n",
    "config = VEPConfig()\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"   Dataset: {config.dataset_name}\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   Context window: ¬±{config.context_length}bp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154dc28",
   "metadata": {},
   "source": [
    "## 6. Data Loading\n",
    "\n",
    "Load variant dataset with automatic caching and preprocessing:\n",
    "\n",
    "**What happens during loading:**\n",
    "1. üîΩ Download dataset from Hugging Face Hub (if not cached)\n",
    "2. üî§ Initialize tokenizer for sequence encoding\n",
    "3. üìä Create dataset objects for train/valid/test splits\n",
    "4. ‚úÖ Validate data format and integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"   Special tokens: {tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Load datasets with progress indication\n",
    "print(\"\\nüì• Loading variant datasets...\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name=config.dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_length,\n",
    "    cache_dir=config.cache_dir\n",
    ")\n",
    "print(\"‚úÖ Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee102b",
   "metadata": {},
   "source": [
    "## 7. Data Exploration & Quality Control\n",
    "\n",
    "Understanding your data is crucial before analysis. Let's examine:\n",
    "- Dataset sizes and splits\n",
    "- Sequence characteristics\n",
    "- Label distributions\n",
    "- Data quality indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e28c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset sizes\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"{split.upper():10s}: {len(dataset):6,d} variants\")\n",
    "\n",
    "# Examine sample structure\n",
    "print(\"\\nüîç Sample Data Structure:\")\n",
    "print(\"=\" * 50)\n",
    "sample = datasets['test'][0]\n",
    "print(f\"Available keys: {list(sample.keys())}\")\n",
    "print(f\"\\nInput shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "\n",
    "# Show first few tokens (if available)\n",
    "if hasattr(tokenizer, 'convert_ids_to_tokens'):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'][:20])\n",
    "    print(f\"\\nFirst 20 tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a8037",
   "metadata": {},
   "source": [
    "### 7.1: Label Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìà Label Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for split, dataset in datasets.items():\n",
    "    if 'labels' in dataset[0]:\n",
    "        labels = [dataset[i]['labels'].item() if hasattr(dataset[i]['labels'], 'item') \n",
    "                 else dataset[i]['labels'] for i in range(len(dataset))]\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            percentage = count / len(labels) * 100\n",
    "            print(f\"  Label {label}: {count:6,d} ({percentage:5.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33da52",
   "metadata": {},
   "source": [
    "### 7.2: Sequence Length Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequence lengths\n",
    "print(\"\\nüìè Sequence Length Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "test_sample = datasets['test']\n",
    "lengths = []\n",
    "for i in range(min(1000, len(test_sample))):  # Sample first 1000\n",
    "    length = (sample['attention_mask'] == 1).sum().item()\n",
    "    lengths.append(length)\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "print(f\"Mean length: {lengths.mean():.1f} tokens\")\n",
    "print(f\"Std dev: {lengths.std():.1f} tokens\")\n",
    "print(f\"Min length: {lengths.min()} tokens\")\n",
    "print(f\"Max length: {lengths.max()} tokens\")\n",
    "print(f\"Median length: {np.median(lengths):.1f} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ff42b",
   "metadata": {},
   "source": [
    "## 5. Create DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346df787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "test_loader = datasets['test'].get_dataloader(\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Verify batch structure\n",
    "batch = next(iter(test_loader))\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
