{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e63615864e8b154f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ðŸ§¬ Variant Effect Prediction with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial on predicting the functional effects of genetic variants using **OmniGenBench** and **PlantRNA-FM** (Plant RNA Foundation Model, 35M parameters, published in *Nature Machine Intelligence*). This guide demonstrates a modern approach to variant effect prediction using specialized plant RNA foundation models.\n",
    "\n",
    "> ðŸ“š **Prerequisite**: It is highly recommended to first study the **[Fundamental Concepts Tutorial](https://github.com/yangheng95/OmniGenBench/blob/master/examples/00_fundamental_concepts.ipynb)**, which covers language model concepts, machine learning task classification, and foundation model principlesâ€”particularly how PlantRNA-FM works.\n",
    "\n",
    "### 1. The Biological Challenge: Understanding Genetic Variants\n",
    "\n",
    "**Genetic variants** are differences in DNA sequences between individuals, ranging from single nucleotide polymorphisms (SNPs) to larger structural variations. While most variants are benign, some can significantly impact:\n",
    "\n",
    "- **Gene expression** - affecting protein production levels\n",
    "- **Protein function** - altering structure or activity\n",
    "- **Regulatory networks** - disrupting transcription factor binding\n",
    "- **Disease susceptibility** - increasing risk for genetic disorders\n",
    "\n",
    "Experimentally validating millions of variants is impractical, making computational prediction essential for genomic medicine.\n",
    "\n",
    "### 2. The Data: Variant Effect Prediction Dataset\n",
    "\n",
    "Our dataset contains:\n",
    "- **Genomic coordinates** - Chromosomal positions of variants\n",
    "- **Reference and alternative alleles** - DNA sequence changes\n",
    "- **Functional annotations** - Known biological impacts\n",
    "\n",
    "**Dataset Structure:**\n",
    "\n",
    "| chr | pos | ref | alt | functional_impact |\n",
    "|-----|-----|-----|-----|------------------|\n",
    "| chr1 | 12345 | A | G | Benign |\n",
    "| chr2 | 67890 | C | T | Pathogenic |\n",
    "\n",
    "### 3. Quick Start: Variant Effect Prediction Workflow\n",
    "\n",
    "This tutorial demonstrates the practical application of **[Fundamental Concepts](https://github.com/yangheng95/OmniGenBench/blob/master/examples/00_fundamental_concepts.ipynb)** using a streamlined 4-step workflow:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52811113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"4-step workflow.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bdc8e",
   "metadata": {},
   "source": [
    "**Variant Effect Prediction** uses embedding comparison to assess functional impacts without training. We leverage `OmniModelForSequenceClassification` with **PlantRNA-FM** to extract plant-optimized embeddings and compare reference vs. alternative sequences.\n",
    "\n",
    "### 4. Tutorial Structure\n",
    "\n",
    "1. **[Data Preparation](01_vep_data_preparation.ipynb)**: Load variant datasets and reference genome\n",
    "2. **[Model Setup](02_vep_model_setup.ipynb)**: Initialize PlantRNA-FM for plant genomic analysis\n",
    "3. **[Embedding Extraction](03_embedding_and_scoring.ipynb)**: Compare reference and alternative sequences using PlantRNA-FM\n",
    "4. **[Visualization](04_visualization_and_export.ipynb)**: Analyze and export results\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8399a",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Environment Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install omnigenbench -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniDatasetForSequenceClassification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118b478",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Define analysis parameters with sensible defaults:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "dataset_name = \"yangheng/variant_effect_prediction\"\n",
    "model_name = \"yangheng/PlantRNA-FM\"  # Using PlantRNA-FM for plant variant analysis\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "context_length = 200\n",
    "max_variants = 100  # Use subset for quick testing\n",
    "cache_dir = \"vep_data\"\n",
    "output_dir = \"vep_results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da6f6d",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 2: Data Loading\n",
    "\n",
    "Load the variant dataset using OmniGenBench's enhanced data loading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214caf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load dataset with automatic caching\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name_or_path=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    cache_dir=cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2daad5",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 3: Model Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95feb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OmniModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4a06e",
   "metadata": {},
   "source": [
    "## ðŸ§¬ Step 4: Variant Effect Scoring\n",
    "\n",
    "Extract embeddings and calculate effect scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = datasets['test']\n",
    "dataloader = test_dataset.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing variants\"):\n",
    "        outputs = model(**batch)\n",
    "        embeddings = outputs.hidden_states[-1][:, 0, :]  # CLS token\n",
    "        results.append(embeddings)\n",
    "\n",
    "all_embeddings = torch.cat(results, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947545d",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Results and Visualization\n",
    "\n",
    "For detailed analysis and visualization, see the complete tutorial notebooks:\n",
    "- **[01_vep_data_preparation.ipynb](01_vep_data_preparation.ipynb)**\n",
    "- **[02_vep_model_setup.ipynb](02_vep_model_setup.ipynb)**\n",
    "- **[03_embedding_and_scoring.ipynb](03_embedding_and_scoring.ipynb)**\n",
    "- **[04_visualization_and_export.ipynb](04_visualization_and_export.ipynb)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32780e",
   "metadata": {},
   "source": [
    "# Appendix: Aligning with run_vep_mlm pipeline\\n\\nThis appendix shows how to reproduce the script-style variant effect scoring inside the notebook to stay aligned with `dev/benchpaper/vep/run_vep_mlm.py`:\\n\\n- Input expects paired sequences (`ref_seq`, `alt_seq`) and a `mutation_position` index within the context window.\\n- Tokenization uses `add_special_tokens=False` to keep positions aligned.\\n- Distances reported:\\n  - `cls_dist`: distance at token index 0 (first-token embedding, matches script semantics)\\n  - `mut_dist`: distance at the provided `mutation_position`\\n- If a `label` column exists with at least two classes, ROC-AUC will be computed on `mut_dist`.\\n\\nNote: For human reference genome (hg38) fetching and BED-to-sequence assembly, see the script for a CLI-first workflow. Here we focus on the scoring core.\n",
    "import torch, numpy as np, pandas as pd\\nfrom scipy.spatial import distance\\nfrom sklearn.metrics import roc_auc_score\\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\\n\\n@torch.inference_mode()\\ndef vep_compute_distances(\\n    model, tokenizer,\\n    ref_seqs, alt_seqs, mut_positions,\\n    max_length: int, batch_size: int = 16, device: str = None\\n):\\n    \\\"\\\"\\\"\\n    Compute first-token (index 0) and mutation-position embedding distances for ref/alt pairs.\\n\\n    Returns: DataFrame with columns: cls_dist, mut_dist\\n    \\\"\\\"\\\"\\n    if device is None:\\n        device = model.device\\n    results = []\\n\\n    def encode_batch(seqs):\\n        toks = tokenizer(\\n            [' '.join(s) for s in seqs],\\n            return_tensors='pt',\\n            padding='max_length', max_length=max_length, truncation=True,\\n            add_special_tokens=False\\n        )\\n        toks = {k: v.to(device) for k, v in toks.items()}\\n        out = model(**toks, output_hidden_states=True)\\n        return out.hidden_states[-1].detach().cpu().numpy()  # (B, L, D)\\n\\n    for i in range(0, len(ref_seqs), batch_size):\\n        r = encode_batch(ref_seqs[i:i+batch_size])\\n        a = encode_batch(alt_seqs[i:i+batch_size])\\n        for j, pos in enumerate(mut_positions[i:i+batch_size]):\\n            pos = int(pos) if 0 <= int(pos) < r.shape[1] else 0\\n            # L2-normalize then cosine distance (matches script style)\\n            def norm(x):\\n                n = np.linalg.norm(x)\\n                return x / (n + 1e-8)\\n            cls_dist = distance.cosine(norm(a[j, 0, :]), norm(r[j, 0, :]))\\n            mut_dist = distance.cosine(norm(a[j, pos, :]), norm(r[j, pos, :]))\\n            results.append((cls_dist, mut_dist))\\n\\n    return pd.DataFrame(results, columns=['cls_dist', 'mut_dist'])\\n\\n# Example (pseudocode):\\n# model_name = 'yangheng/OmniGenome-52M'\\n# tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\\n# mdl = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval().to('cuda')\\n# df must contain: ref_seq, alt_seq, mutation_position, and optional label\\n# scores = vep_compute_distances(mdl, tok, df.ref_seq.tolist(), df.alt_seq.tolist(), df.mutation_position.tolist(), max_length=400)\\n# if 'label' in df and df['label'].nunique() > 1:\\n#     print('AUC:', roc_auc_score(df['label'], scores['mut_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d79dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "@torch.inference_mode()\n",
    "def vep_compute_distances(\n",
    "    model, tokenizer,\n",
    "    ref_seqs, alt_seqs, mut_positions,\n",
    "    max_length: int, batch_size: int = 16, device: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute first-token (index 0) and mutation-position embedding distances for ref/alt pairs.\n",
    "\n",
    "    Returns: DataFrame with columns: cls_dist, mut_dist\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = model.device\n",
    "    results = []\n",
    "\n",
    "    def encode_batch(seqs):\n",
    "        toks = tokenizer(\n",
    "            [' '.join(s) for s in seqs],\n",
    "            return_tensors='pt',\n",
    "            padding='max_length', max_length=max_length, truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        toks = {k: v.to(device) for k, v in toks.items()}\n",
    "        out = model(**toks, output_hidden_states=True)\n",
    "        return out.hidden_states[-1].detach().cpu().numpy()  # (B, L, D)\n",
    "\n",
    "    for i in range(0, len(ref_seqs), batch_size):\n",
    "        r = encode_batch(ref_seqs[i:i+batch_size])\n",
    "        a = encode_batch(alt_seqs[i:i+batch_size])\n",
    "        for j, pos in enumerate(mut_positions[i:i+batch_size]):\n",
    "            pos = int(pos) if 0 <= int(pos) < r.shape[1] else 0\n",
    "            def norm(x):\n",
    "                n = np.linalg.norm(x)\n",
    "                return x / (n + 1e-8)\n",
    "            cls_dist = distance.cosine(norm(a[j, 0, :]), norm(r[j, 0, :]))\n",
    "            mut_dist = distance.cosine(norm(a[j, pos, :]), norm(r[j, pos, :]))\n",
    "            results.append((cls_dist, mut_dist))\n",
    "\n",
    "    return pd.DataFrame(results, columns=['cls_dist', 'mut_dist'])\n",
    "\n",
    "# Example (pseudocode):\n",
    "# model_name = 'yangheng/OmniGenome-52M'\n",
    "# tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# mdl = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval().to('cuda')\n",
    "# # df must contain: ref_seq, alt_seq, mutation_position, and optional label\n",
    "# scores = vep_compute_distances(mdl, tok, df.ref_seq.tolist(), df.alt_seq.tolist(), df.mutation_position.tolist(), max_length=400)\n",
    "# if 'label' in df and df['label'].nunique() > 1:\n",
    "#     print('AUC:', roc_auc_score(df['label'], scores['mut_dist']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
