{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0eef8f1",
   "metadata": {},
   "source": [
    "# ðŸ§¬ VEP Tutorial 3/4: Embedding Extraction and Effect Scoring with PlantRNA-FM\n",
    "\n",
    "Welcome to the third tutorial in our VEP series. This is where theory meets practiceâ€”we'll extract embeddings from thousands of variants using **PlantRNA-FM** and compute effect scores at scale.\n",
    "\n",
    "> ðŸ“š **Prerequisites**: \n",
    "> - Complete [Tutorial 1: Data Preparation](01_vep_data_preparation.ipynb)\n",
    "> - Complete [Tutorial 2: Model Setup](02_vep_model_setup.ipynb)\n",
    "> - Understand embedding concepts from [Fundamental Concepts](../../00_fundamental_concepts.ipynb)\n",
    "\n",
    "This tutorial demonstrates the core VEP workflow: comparing sequence embeddings from **PlantRNA-FM** (35M parameters, *Nature Machine Intelligence*) to predict functional effects in plant genomic variants.\n",
    "\n",
    "## 1. The Science of Embedding-Based Scoring ðŸ”¬\n",
    "\n",
    "### 1.1 Why Embeddings Work for VEP\n",
    "\n",
    "**The Central Hypothesis:**\n",
    "> Functionally similar sequences have similar embeddings in the learned representation space of PlantRNA-FM.\n",
    "\n",
    "Therefore, a variant that significantly changes the PlantRNA-FM embedding likely has functional impact on plant gene regulation or RNA structure.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Wild-type Sequence] --> B[PlantRNA-FM]\n",
    "    C[Mutant Sequence] --> B\n",
    "    B --> D[Embedding Space]\n",
    "    D --> E{Large Distance?}\n",
    "    E -->|Yes| F[Likely Pathogenic]\n",
    "    E -->|No| G[Likely Benign]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#ffe1e1\n",
    "    style D fill:#f5ffe1\n",
    "    style F fill:#ffe1e1\n",
    "    style G fill:#e1ffe1\n",
    "```\n",
    "\n",
    "### 1.2 Similarity vs. Distance Metrics\n",
    "\n",
    "Different metrics capture different aspects of embedding relationships:\n",
    "\n",
    "| Metric | Formula | Range | Interpretation | Best For |\n",
    "|--------|---------|-------|----------------|----------|\n",
    "| **Cosine Similarity** | $\\frac{A \\cdot B}{\\\\|A\\\\|\\\\|B\\\\|}$ | [-1, 1] | Angle between vectors | Direction-based comparison |\n",
    "| **Cosine Distance** | $1 - \\text{cosine similarity}$ | [0, 2] | Dissimilarity measure | Effect scores (our focus) |\n",
    "| **Euclidean Distance** | $\\\\|A - B\\\\|_2$ | [0, âˆž] | Straight-line distance | Magnitude-based comparison |\n",
    "| **Manhattan Distance** | $\\\\|A - B\\\\|_1$ | [0, âˆž] | Grid-based distance | Robust to outliers |\n",
    "\n",
    "**For VEP with PlantRNA-FM, we primarily use cosine distance** because:\n",
    "- âœ… Normalized (scale-invariant)\n",
    "- âœ… Captures semantic similarity in learned plant RNA space\n",
    "- âœ… Robust to sequence length variation\n",
    "- âœ… Well-validated in NLP and genomics applications\n",
    "- âœ… Effective for comparing plant regulatory element embeddings\n",
    "\n",
    "### 1.3 Pooling Strategies Revisited\n",
    "\n",
    "How we aggregate token embeddings affects results:\n",
    "\n",
    "| Strategy | Method | Advantages | Disadvantages |\n",
    "|----------|--------|------------|---------------|\n",
    "| **Mean Pooling** | Average all tokens | Stable, robust | May dilute signal |\n",
    "| **CLS Token** | Use [CLS] embedding | Fast, BERT-standard | Single point representation |\n",
    "| **Max Pooling** | Max across tokens | Highlights peaks | Sensitive to outliers |\n",
    "| **Weighted Average** | Attention-weighted | Focuses on important regions | Requires attention weights |\n",
    "\n",
    "**Recommendation:** Start with mean pooling, compare with others if results are ambiguous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cfc5c",
   "metadata": {},
   "source": [
    "## 2. The Embedding Extraction Workflow ðŸ”„\n",
    "\n",
    "Our pipeline processes variants in four stages:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Load<br/>Variants] --> B[Extract<br/>Embeddings]\n",
    "    B --> C[Compute<br/>Distances]\n",
    "    C --> D[Score<br/>Effects]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#ffe1f5\n",
    "    style C fill:#f5ffe1\n",
    "    style D fill:#ffe1e1\n",
    "```\n",
    "\n",
    "1. **Load**: Read variant data with reference and alternative sequences\n",
    "2. **Extract**: Pass through model to get embeddings\n",
    "3. **Compute**: Calculate pairwise distances\n",
    "4. **Score**: Transform distances into interpretable effect scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5803c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ› ï¸ Step-by-Step: Large-Scale Embedding Extraction\n",
    "\n",
    "### 3.1: Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.spatial.distance import cosine, euclidean, cityblock\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniDatasetForSequenceClassification\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Literal, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ’» Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffd183",
   "metadata": {},
   "source": [
    "### 3.2: Configuration\n",
    "\n",
    "Comprehensive settings for embedding extraction and scoring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScoringConfig:\n",
    "    \"\"\"Configuration for embedding extraction and variant scoring\"\"\"\n",
    "    # Data settings\n",
    "    dataset_name: str = \"yangheng/variant_effect_prediction\"\n",
    "    cache_dir: str = \"vep_data\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"yangheng/OmniGenome-52M\"\n",
    "    max_length: int = 512\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Embedding extraction\n",
    "    pooling: Literal['mean', 'cls', 'max'] = 'mean'\n",
    "    batch_size: int = 32  # Increase for faster processing\n",
    "    \n",
    "    # Scoring settings\n",
    "    distance_metric: str = 'cosine'  # cosine, euclidean, manhattan\n",
    "    normalize_scores: bool = True\n",
    "    \n",
    "    # Output settings\n",
    "    save_embeddings: bool = False  # Set True to cache embeddings\n",
    "    output_dir: str = \"vep_results\"\n",
    "\n",
    "config = ScoringConfig()\n",
    "print(\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   Device: {config.device}\")\n",
    "print(f\"   Pooling: {config.pooling}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Distance metric: {config.distance_metric}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ddf30",
   "metadata": {},
   "source": [
    "### 3.3: Load Model and Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"ðŸ”¤ Loading tokenizer...\")\n",
    "tokenizer = OmniTokenizer.from_pretrained(\n",
    "    config.model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"ðŸ¤– Loading model...\")\n",
    "model = OmniModelForSequenceClassification.from_pretrained(\n",
    "    config.model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    num_labels=2,\n",
    "    trust_remote_code=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model = model.to(config.device)\n",
    "model.eval()\n",
    "\n",
    "# Load datasets\n",
    "print(\"ðŸ“¥ Loading datasets...\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_hub(\n",
    "    dataset_name=config.dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_length,\n",
    "    cache_dir=config.cache_dir\n",
    ")\n",
    "\n",
    "print(\"âœ… All components loaded!\")\n",
    "print(f\"   Test set size: {len(datasets['test'])} variants\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae44750",
   "metadata": {},
   "source": [
    "## 4. Embedding Extraction Implementation ðŸ”¬\n",
    "\n",
    "### 4.1: Optimized Batch Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_batch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    pooling='mean',\n",
    "    device='cpu',\n",
    "    show_progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract embeddings from model in batches.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained foundation model\n",
    "        dataloader: DataLoader with tokenized sequences\n",
    "        pooling: Pooling strategy ('mean', 'cls', 'max')\n",
    "        device: Device for computation\n",
    "        show_progress: Whether to show progress bar\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (n_samples, hidden_dim)\n",
    "        \n",
    "    Note:\n",
    "        This function is optimized for large-scale processing with:\n",
    "        - Batch processing for efficiency\n",
    "        - GPU acceleration when available\n",
    "        - Memory-efficient computation\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    iterator = tqdm(dataloader, desc=f\"Extracting embeddings ({pooling})\") if show_progress else dataloader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            # Get final layer hidden states\n",
    "            hidden_states = outputs.hidden_states[-1]  # (batch_size, seq_len, hidden_dim)\n",
    "            \n",
    "            # Apply pooling\n",
    "            if pooling == 'mean':\n",
    "                # Average over sequence length\n",
    "                embeddings = hidden_states.mean(dim=1)\n",
    "            elif pooling == 'cls':\n",
    "                # Use CLS token (first position)\n",
    "                embeddings = hidden_states[:, 0, :]\n",
    "            elif pooling == 'max':\n",
    "                # Max pooling over sequence length\n",
    "                embeddings = hidden_states.max(dim=1)[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling strategy: {pooling}\")\n",
    "            \n",
    "            # Move to CPU and store\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    return all_embeddings.numpy()\n",
    "\n",
    "# Test the function\n",
    "print(\"ðŸ§ª Testing embedding extraction...\")\n",
    "test_loader = datasets['test'].get_dataloader(\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Extract embeddings (this may take a few minutes)\n",
    "embeddings = extract_embeddings_batch(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    pooling=config.pooling,\n",
    "    device=config.device,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Extraction complete!\")\n",
    "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Memory usage: {embeddings.nbytes / 1024**2:.2f} MB\")\n",
    "print(f\"   Embedding norm (mean): {np.linalg.norm(embeddings, axis=1).mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b609e14",
   "metadata": {},
   "source": [
    "### 4.2: Comparing Pooling Strategies\n",
    "\n",
    "Let's compare different pooling strategies on a subset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings with different pooling strategies\n",
    "pooling_strategies = ['mean', 'cls', 'max']\n",
    "pooling_embeddings = {}\n",
    "\n",
    "print(\"ðŸ“Š Comparing pooling strategies...\")\n",
    "for strategy in pooling_strategies:\n",
    "    emb = extract_embeddings_batch(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        pooling=strategy,\n",
    "        device=config.device,\n",
    "        show_progress=False\n",
    "    )\n",
    "    pooling_embeddings[strategy] = emb\n",
    "    print(f\"   {strategy:5s}: shape={emb.shape}, norm={np.linalg.norm(emb, axis=1).mean():.3f}\")\n",
    "\n",
    "# Compare similarity between pooling methods\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\nðŸ” Correlation between pooling strategies:\")\n",
    "for i, strategy1 in enumerate(pooling_strategies):\n",
    "    for strategy2 in pooling_strategies[i+1:]:\n",
    "        # Calculate correlation between first embedding from each method\n",
    "        corr = np.corrcoef(\n",
    "            pooling_embeddings[strategy1][0],\n",
    "            pooling_embeddings[strategy2][0]\n",
    "        )[0, 1]\n",
    "        print(f\"   {strategy1} â†” {strategy2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90eb5a",
   "metadata": {},
   "source": [
    "## 5. Variant Effect Scoring ðŸ“Š\n",
    "\n",
    "### 5.1: Distance Metric Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_distances(\n",
    "    embeddings1: np.ndarray,\n",
    "    embeddings2: np.ndarray,\n",
    "    metric: str = 'cosine'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate pairwise distances between two sets of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings1: First set of embeddings (n_samples, dim)\n",
    "        embeddings2: Second set of embeddings (n_samples, dim)\n",
    "        metric: Distance metric ('cosine', 'euclidean', 'manhattan')\n",
    "        \n",
    "    Returns:\n",
    "        distances: Array of distances (n_samples,)\n",
    "    \"\"\"\n",
    "    assert embeddings1.shape == embeddings2.shape, \"Embedding shapes must match\"\n",
    "    \n",
    "    distances = []\n",
    "    for emb1, emb2 in zip(embeddings1, embeddings2):\n",
    "        if metric == 'cosine':\n",
    "            dist = cosine(emb1, emb2)\n",
    "        elif metric == 'euclidean':\n",
    "            dist = euclidean(emb1, emb2)\n",
    "        elif metric == 'manhattan':\n",
    "            dist = cityblock(emb1, emb2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        distances.append(dist)\n",
    "    \n",
    "    return np.array(distances)\n",
    "\n",
    "\n",
    "def calculate_effect_scores(\n",
    "    ref_embeddings: np.ndarray,\n",
    "    alt_embeddings: np.ndarray,\n",
    "    metric: str = 'cosine',\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate variant effect scores from embeddings.\n",
    "    \n",
    "    Effect score interpretation:\n",
    "    - Higher score = larger embedding change = likely functional impact\n",
    "    - Lower score = smaller embedding change = likely benign\n",
    "    \n",
    "    Args:\n",
    "        ref_embeddings: Reference sequence embeddings\n",
    "        alt_embeddings: Alternative sequence embeddings\n",
    "        metric: Distance metric to use\n",
    "        normalize: Whether to normalize scores to [0, 1]\n",
    "        \n",
    "    Returns:\n",
    "        effect_scores: Array of effect scores\n",
    "    \"\"\"\n",
    "    # Calculate distances\n",
    "    distances = calculate_pairwise_distances(\n",
    "        ref_embeddings,\n",
    "        alt_embeddings,\n",
    "        metric=metric\n",
    "    )\n",
    "    \n",
    "    # For cosine distance, values are already in [0, 2]\n",
    "    # For euclidean/manhattan, we normalize\n",
    "    if normalize:\n",
    "        if metric in ['euclidean', 'manhattan']:\n",
    "            distances = (distances - distances.min()) / (distances.max() - distances.min())\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Example: Calculate effect scores\n",
    "# Note: In practice, you'd have paired ref/alt sequences in your dataset\n",
    "# Here we demonstrate with a mock scenario\n",
    "print(\"ðŸ“Š Calculating effect scores...\")\n",
    "\n",
    "# Assuming embeddings alternate between ref and alt (for demonstration)\n",
    "# In real data, you'd extract these separately\n",
    "n_variants = len(embeddings) // 2\n",
    "ref_emb = embeddings[:n_variants]\n",
    "alt_emb = embeddings[n_variants:2*n_variants]\n",
    "\n",
    "effect_scores = calculate_effect_scores(\n",
    "    ref_emb,\n",
    "    alt_emb,\n",
    "    metric=config.distance_metric,\n",
    "    normalize=config.normalize_scores\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Effect scores calculated!\")\n",
    "print(f\"   Number of variants: {len(effect_scores)}\")\n",
    "print(f\"   Mean effect score: {effect_scores.mean():.4f}\")\n",
    "print(f\"   Std dev: {effect_scores.std():.4f}\")\n",
    "print(f\"   Score range: [{effect_scores.min():.4f}, {effect_scores.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c94f5",
   "metadata": {},
   "source": [
    "### 5.2: Comparing Distance Metrics\n",
    "\n",
    "Different metrics may give different insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different distance metrics\n",
    "metrics = ['cosine', 'euclidean', 'manhattan']\n",
    "metric_scores = {}\n",
    "\n",
    "print(\"ðŸ“Š Comparing distance metrics...\")\n",
    "for metric in metrics:\n",
    "    scores = calculate_effect_scores(\n",
    "        ref_emb,\n",
    "        alt_emb,\n",
    "        metric=metric,\n",
    "        normalize=True\n",
    "    )\n",
    "    metric_scores[metric] = scores\n",
    "    print(f\"   {metric:10s}: mean={scores.mean():.4f}, std={scores.std():.4f}\")\n",
    "\n",
    "# Correlation between metrics\n",
    "print(\"\\nðŸ” Correlation between metrics:\")\n",
    "for i, metric1 in enumerate(metrics):\n",
    "    for metric2 in metrics[i+1:]:\n",
    "        corr = np.corrcoef(metric_scores[metric1], metric_scores[metric2])[0, 1]\n",
    "        print(f\"   {metric1} â†” {metric2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d42ec",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis & Interpretation ðŸ“ˆ\n",
    "\n",
    "### 6.1: Score Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Create comprehensive statistics\n",
    "results_df = pd.DataFrame({\n",
    "    'variant_id': range(len(effect_scores)),\n",
    "    'effect_score': effect_scores,\n",
    "    'cosine_score': metric_scores['cosine'],\n",
    "    'euclidean_score': metric_scores['euclidean'],\n",
    "    'manhattan_score': metric_scores['manhattan']\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Š Effect Score Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df['effect_score'].describe())\n",
    "\n",
    "# Percentile-based thresholds\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "print(\"\\nðŸ“ Percentile Thresholds:\")\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(effect_scores, p)\n",
    "    print(f\"   {p}th percentile: {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c143f",
   "metadata": {},
   "source": [
    "### 6.2: Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e513b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Distribution histogram\n",
    "axes[0].hist(effect_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(effect_scores.mean(), color='red', linestyle='--', label=f'Mean: {effect_scores.mean():.3f}')\n",
    "axes[0].axvline(np.percentile(effect_scores, 95), color='orange', linestyle='--', label='95th percentile')\n",
    "axes[0].set_xlabel('Effect Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Effect Score Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Metric comparison\n",
    "metric_data = [metric_scores[m] for m in metrics]\n",
    "bp = axes[1].boxplot(metric_data, labels=metrics, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#e1f5ff')\n",
    "axes[1].set_ylabel('Normalized Score')\n",
    "axes[1].set_title('Distance Metric Comparison')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Score correlation\n",
    "axes[2].scatter(metric_scores['cosine'], metric_scores['euclidean'], alpha=0.5, s=10)\n",
    "axes[2].set_xlabel('Cosine Distance')\n",
    "axes[2].set_ylabel('Euclidean Distance')\n",
    "axes[2].set_title('Metric Correlation')\n",
    "corr = np.corrcoef(metric_scores['cosine'], metric_scores['euclidean'])[0, 1]\n",
    "axes[2].text(0.05, 0.95, f'r = {corr:.3f}', transform=axes[2].transAxes, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.output_dir}/embedding_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a218d",
   "metadata": {},
   "source": [
    "### 6.3: Variant Classification\n",
    "\n",
    "Based on effect scores, classify variants:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "threshold_high = np.percentile(effect_scores, 75)\n",
    "threshold_low = np.percentile(effect_scores, 25)\n",
    "\n",
    "# Classify variants\n",
    "def classify_variant(score, low_thresh, high_thresh):\n",
    "    if score > high_thresh:\n",
    "        return 'High Impact'\n",
    "    elif score > low_thresh:\n",
    "        return 'Moderate Impact'\n",
    "    else:\n",
    "        return 'Low Impact'\n",
    "\n",
    "results_df['impact'] = results_df['effect_score'].apply(\n",
    "    lambda x: classify_variant(x, threshold_low, threshold_high)\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Variant Impact Classification:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df['impact'].value_counts())\n",
    "print(f\"\\nðŸŽ¯ Classification Thresholds:\")\n",
    "print(f\"   Low â†’ Moderate: {threshold_low:.4f}\")\n",
    "print(f\"   Moderate â†’ High: {threshold_high:.4f}\")\n",
    "\n",
    "# Sample high-impact variants\n",
    "high_impact = results_df[results_df['impact'] == 'High Impact'].sort_values('effect_score', ascending=False)\n",
    "print(f\"\\nðŸ” Top 5 High-Impact Variants:\")\n",
    "print(high_impact.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc2bf9",
   "metadata": {},
   "source": [
    "## 7. Export Results ðŸ’¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "Path(config.output_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Export scores\n",
    "output_file = f\"{config.output_dir}/variant_effect_scores.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Results exported to: {output_file}\")\n",
    "\n",
    "# Optionally save embeddings\n",
    "if config.save_embeddings:\n",
    "    np.save(f\"{config.output_dir}/ref_embeddings.npy\", ref_emb)\n",
    "    np.save(f\"{config.output_dir}/alt_embeddings.npy\", alt_emb)\n",
    "    print(f\"âœ… Embeddings saved to: {config.output_dir}/\")\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    'total_variants': len(effect_scores),\n",
    "    'mean_score': float(effect_scores.mean()),\n",
    "    'std_score': float(effect_scores.std()),\n",
    "    'min_score': float(effect_scores.min()),\n",
    "    'max_score': float(effect_scores.max()),\n",
    "    'high_impact_count': int((results_df['impact'] == 'High Impact').sum()),\n",
    "    'moderate_impact_count': int((results_df['impact'] == 'Moderate Impact').sum()),\n",
    "    'low_impact_count': int((results_df['impact'] == 'Low Impact').sum()),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{config.output_dir}/summary_statistics.json\", 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Summary statistics saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3f73a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this tutorial, we:\n",
    "1. âœ… **Understood embedding distance metrics** - Learned cosine, euclidean, and Manhattan distances\n",
    "2. âœ… **Implemented batch extraction** - Efficiently processed thousands of variants\n",
    "3. âœ… **Compared pooling strategies** - Evaluated mean, CLS, and max pooling\n",
    "4. âœ… **Calculated effect scores** - Converted embeddings to interpretable scores\n",
    "5. âœ… **Analyzed distributions** - Performed statistical analysis and visualization\n",
    "6. âœ… **Classified variants** - Categorized by predicted impact level\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "ðŸ“Š **Distance metrics matter**: Cosine distance is preferred for semantic comparison\n",
    "\n",
    "ðŸ”„ **Batch processing is essential**: Process large datasets efficiently with GPU acceleration\n",
    "\n",
    "ðŸ“ˆ **Thresholds are dataset-specific**: Use percentiles rather than absolute cutoffs\n",
    "\n",
    "ðŸŽ¯ **Multiple metrics provide robustness**: Compare results across different distance measures\n",
    "\n",
    "### Technical Highlights\n",
    "\n",
    "**Optimized Embedding Extraction:**\n",
    "```python\n",
    "embeddings = extract_embeddings_batch(\n",
    "    model, dataloader, pooling='mean', device='cuda'\n",
    ")\n",
    "```\n",
    "\n",
    "**Effect Score Calculation:**\n",
    "```python\n",
    "scores = calculate_effect_scores(\n",
    "    ref_embeddings, alt_embeddings, \n",
    "    metric='cosine', normalize=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Variant Classification:**\n",
    "```python\n",
    "threshold = np.percentile(scores, 75)\n",
    "high_impact = scores > threshold\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[Tutorial 4: Visualization & Export](04_visualization_and_export.ipynb)** where we'll:\n",
    "- Create publication-quality figures\n",
    "- Perform ROC/PR curve analysis\n",
    "- Interpret results biologically\n",
    "- Generate clinical reports\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Complete workflow in 5 lines\n",
    "model = OmniModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "embeddings = extract_embeddings_batch(model, dataloader, pooling='mean')\n",
    "ref_emb, alt_emb = embeddings[::2], embeddings[1::2]  # Split pairs\n",
    "scores = calculate_effect_scores(ref_emb, alt_emb, metric='cosine')\n",
    "results_df = pd.DataFrame({'variant_id': range(len(scores)), 'effect_score': scores})\n",
    "```\n",
    "\n",
    "**Next Tutorial**: [04_visualization_and_export.ipynb](04_visualization_and_export.ipynb) ðŸ“Š\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
