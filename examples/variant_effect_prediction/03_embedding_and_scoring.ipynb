{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda2e0b",
   "metadata": {},
   "source": [
    "# ğŸ§¬ VEP Embedding and Scoring Tutorial\n",
    "\n",
    "This tutorial focuses on the core embedding extraction and variant effect scoring methodology for Variant Effect Prediction using genomic foundation models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll explore:\n",
    "- Sequence embedding extraction from genomic foundation models\n",
    "- Variant effect scoring methodologies\n",
    "- Distance metrics for comparing reference vs alternative sequences\n",
    "- Interpretation of effect scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d62afc",
   "metadata": {},
   "source": [
    "## 1. Setup and Prerequisites\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omnigenbench import (\n",
    "    OmniTokenizer,\n",
    "    OmniModelForSequenceClassification\n",
    ")\n",
    "from autocuda import auto_cuda\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ¯ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319c443",
   "metadata": {},
   "source": [
    "## 2. Configuration and Model Setup\n",
    "\n",
    "Define our analysis configuration and load the genomic foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEP Embedding Configuration\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    MODEL_NAME = \"yangheng/OmniGenome-52M\"\n",
    "    MAX_LENGTH = 512\n",
    "    DEVICE = auto_cuda()\n",
    "    \n",
    "    # Embedding extraction settings\n",
    "    POOLING_STRATEGY = \"mean\"  # Options: mean, max, cls, last\n",
    "    LAYER_INDEX = -1  # Which transformer layer to use for embeddings\n",
    "    \n",
    "    # Scoring parameters\n",
    "    DISTANCE_METRICS = [\"cosine\", \"euclidean\", \"manhattan\"]\n",
    "    EFFECT_THRESHOLD = 0.1  # Threshold for functional significance\n",
    "\n",
    "config = EmbeddingConfig()\n",
    "\n",
    "print(\"âš™ï¸ Embedding Analysis Configuration:\")\n",
    "print(f\"  ğŸ§¬ Model: {config.MODEL_NAME}\")\n",
    "print(f\"  ğŸ“ Max length: {config.MAX_LENGTH}\")\n",
    "print(f\"  ğŸ“± Device: {config.DEVICE}\")\n",
    "print(f\"  ğŸ¯ Pooling: {config.POOLING_STRATEGY}\")\n",
    "print(f\"  ğŸ“Š Distance metrics: {config.DISTANCE_METRICS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(\"ğŸ”„ Loading tokenizer and model...\")\n",
    "\n",
    "tokenizer = OmniTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "model = OmniModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    trust_remote_code=True,\n",
    "    output_hidden_states=True  # Essential for embedding extraction\n",
    ")\n",
    "\n",
    "model.to(config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model setup complete!\")\n",
    "print(f\"  ğŸ§  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  ğŸ“± Device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894d26f",
   "metadata": {},
   "source": [
    "## 3. Embedding Extraction Functions\n",
    "\n",
    "Define comprehensive functions for extracting sequence embeddings from the genomic foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_embedding(\n",
    "    sequence: str, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    device: str,\n",
    "    pooling_strategy: str = \"mean\",\n",
    "    layer_index: int = -1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract sequence embedding using specified pooling strategy.\n",
    "    \n",
    "    Args:\n",
    "        sequence: DNA sequence string\n",
    "        model: Pre-trained genomic model\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        device: Computing device\n",
    "        pooling_strategy: Pooling method ('mean', 'max', 'cls', 'last')\n",
    "        layer_index: Which transformer layer to use\n",
    "    \n",
    "    Returns:\n",
    "        Sequence embedding as numpy array\n",
    "    \"\"\"\n",
    "    # Tokenize sequence\n",
    "    inputs = tokenizer(\n",
    "        sequence,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[layer_index]  # Shape: [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Apply pooling strategy\n",
    "        if pooling_strategy == \"mean\":\n",
    "            # Mean pooling over sequence length\n",
    "            embedding = hidden_states.mean(dim=1)\n",
    "        elif pooling_strategy == \"max\":\n",
    "            # Max pooling over sequence length\n",
    "            embedding = hidden_states.max(dim=1)[0]\n",
    "        elif pooling_strategy == \"cls\":\n",
    "            # Use CLS token (first token)\n",
    "            embedding = hidden_states[:, 0, :]\n",
    "        elif pooling_strategy == \"last\":\n",
    "            # Use last token\n",
    "            embedding = hidden_states[:, -1, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "    \n",
    "    return embedding.cpu().numpy().squeeze()\n",
    "\n",
    "def batch_extract_embeddings(\n",
    "    sequences: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: str,\n",
    "    batch_size: int = 8,\n",
    "    pooling_strategy: str = \"mean\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract embeddings for multiple sequences in batches for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of DNA sequences\n",
    "        model: Pre-trained genomic model\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        device: Computing device\n",
    "        batch_size: Batch size for processing\n",
    "        pooling_strategy: Pooling method\n",
    "    \n",
    "    Returns:\n",
    "        Array of embeddings [num_sequences, embedding_dim]\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch_sequences = sequences[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_sequences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=config.MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "            \n",
    "            # Apply pooling\n",
    "            if pooling_strategy == \"mean\":\n",
    "                batch_embeddings = hidden_states.mean(dim=1)\n",
    "            elif pooling_strategy == \"max\":\n",
    "                batch_embeddings = hidden_states.max(dim=1)[0]\n",
    "            elif pooling_strategy == \"cls\":\n",
    "                batch_embeddings = hidden_states[:, 0, :]\n",
    "            else:\n",
    "                batch_embeddings = hidden_states.mean(dim=1)  # Default to mean\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"âœ… Embedding extraction functions defined!\")\n",
    "print(\"  ğŸ§¬ extract_sequence_embedding() - Single sequence embedding\")\n",
    "print(\"  ğŸ“¦ batch_extract_embeddings() - Batch processing for efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943d85b",
   "metadata": {},
   "source": [
    "## 4. Variant Effect Scoring Methods\n",
    "\n",
    "Implement multiple distance metrics for calculating variant effect scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28543492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variant_effects(\n",
    "    ref_embedding: np.ndarray, \n",
    "    alt_embedding: np.ndarray,\n",
    "    metrics: List[str] = [\"cosine\", \"euclidean\", \"manhattan\"]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate variant effect scores using multiple distance metrics.\n",
    "    \n",
    "    Args:\n",
    "        ref_embedding: Reference sequence embedding\n",
    "        alt_embedding: Alternative sequence embedding\n",
    "        metrics: List of distance metrics to compute\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metric names and their scores\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric == \"cosine\":\n",
    "            # Cosine distance (0 = identical, 1 = orthogonal)\n",
    "            score = cosine(ref_embedding, alt_embedding)\n",
    "        elif metric == \"euclidean\":\n",
    "            # Euclidean distance\n",
    "            score = euclidean(ref_embedding, alt_embedding)\n",
    "        elif metric == \"manhattan\":\n",
    "            # Manhattan (L1) distance\n",
    "            score = np.sum(np.abs(ref_embedding - alt_embedding))\n",
    "        elif metric == \"cosine_similarity\":\n",
    "            # Cosine similarity (1 = identical, -1 = opposite)\n",
    "            # Convert to distance: 1 - similarity\n",
    "            similarity = cosine_similarity([ref_embedding], [alt_embedding])[0, 0]\n",
    "            score = 1 - similarity\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        scores[metric] = score\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def interpret_effect_score(score: float, metric: str = \"cosine\", threshold: float = 0.1) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Interpret variant effect score in biological context.\n",
    "    \n",
    "    Args:\n",
    "        score: Effect score\n",
    "        metric: Distance metric used\n",
    "        threshold: Threshold for functional significance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with interpretation results\n",
    "    \"\"\"\n",
    "    # Define interpretation categories\n",
    "    if metric == \"cosine\":\n",
    "        if score < 0.01:\n",
    "            impact = \"Minimal\"\n",
    "            description = \"Likely neutral variant\"\n",
    "            functional = False\n",
    "        elif score < threshold:\n",
    "            impact = \"Low\"\n",
    "            description = \"Possibly neutral or mildly functional\"\n",
    "            functional = False\n",
    "        elif score < 0.3:\n",
    "            impact = \"Moderate\"\n",
    "            description = \"Likely functional impact\"\n",
    "            functional = True\n",
    "        else:\n",
    "            impact = \"High\"\n",
    "            description = \"Strong functional impact expected\"\n",
    "            functional = True\n",
    "    else:\n",
    "        # Generic interpretation for other metrics\n",
    "        functional = score > threshold\n",
    "        impact = \"High\" if score > threshold * 3 else (\"Moderate\" if functional else \"Low\")\n",
    "        description = f\"Effect score: {score:.4f}\"\n",
    "    \n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"impact\": impact,\n",
    "        \"functional\": functional,\n",
    "        \"description\": description,\n",
    "        \"metric\": metric\n",
    "    }\n",
    "\n",
    "print(\"âœ… Variant effect scoring functions defined!\")\n",
    "print(\"  ğŸ“Š calculate_variant_effects() - Multiple distance metrics\")\n",
    "print(\"  ğŸ”¬ interpret_effect_score() - Biological interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6af18b",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Variant Analysis\n",
    "\n",
    "Let's demonstrate the embedding and scoring pipeline with example variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb33cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example variant sequences\n",
    "def create_variant_sequences(\n",
    "    ref_allele: str, \n",
    "    alt_allele: str, \n",
    "    context_length: int = 100,\n",
    "    position: int = None\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Create reference and alternative sequences for variant analysis.\n",
    "    \n",
    "    Args:\n",
    "        ref_allele: Reference allele\n",
    "        alt_allele: Alternative allele\n",
    "        context_length: Length of context sequence on each side\n",
    "        position: Position within context (if None, places in center)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (reference_sequence, alternative_sequence)\n",
    "    \"\"\"\n",
    "    if position is None:\n",
    "        position = context_length\n",
    "    \n",
    "    # Create synthetic context (in practice, this would come from reference genome)\n",
    "    context_before = \"ATCG\" * (position // 4) + \"ATCG\"[:position % 4]\n",
    "    context_after = \"GCTA\" * (context_length // 4) + \"GCTA\"[:context_length % 4]\n",
    "    \n",
    "    ref_sequence = context_before + ref_allele + context_after\n",
    "    alt_sequence = context_before + alt_allele + context_after\n",
    "    \n",
    "    return ref_sequence, alt_sequence\n",
    "\n",
    "# Example variants to analyze\n",
    "example_variants = [\n",
    "    {\"id\": \"SNV_1\", \"ref\": \"A\", \"alt\": \"G\", \"type\": \"transition\"},\n",
    "    {\"id\": \"SNV_2\", \"ref\": \"C\", \"alt\": \"T\", \"type\": \"transition\"},\n",
    "    {\"id\": \"SNV_3\", \"ref\": \"A\", \"alt\": \"T\", \"type\": \"transversion\"},\n",
    "    {\"id\": \"SNV_4\", \"ref\": \"G\", \"alt\": \"C\", \"type\": \"transversion\"},\n",
    "    {\"id\": \"INDEL_1\", \"ref\": \"AT\", \"alt\": \"A\", \"type\": \"deletion\"},\n",
    "    {\"id\": \"INDEL_2\", \"ref\": \"G\", \"alt\": \"GT\", \"type\": \"insertion\"},\n",
    "]\n",
    "\n",
    "print(\"ğŸ§¬ Analyzing example variants...\")\n",
    "results = []\n",
    "\n",
    "for variant in tqdm(example_variants, desc=\"Processing variants\"):\n",
    "    # Create sequences\n",
    "    ref_seq, alt_seq = create_variant_sequences(\n",
    "        variant[\"ref\"], \n",
    "        variant[\"alt\"], \n",
    "        context_length=150\n",
    "    )\n",
    "    \n",
    "    # Extract embeddings\n",
    "    ref_embedding = extract_sequence_embedding(\n",
    "        ref_seq, model, tokenizer, config.DEVICE, config.POOLING_STRATEGY\n",
    "    )\n",
    "    alt_embedding = extract_sequence_embedding(\n",
    "        alt_seq, model, tokenizer, config.DEVICE, config.POOLING_STRATEGY\n",
    "    )\n",
    "    \n",
    "    # Calculate effect scores\n",
    "    effect_scores = calculate_variant_effects(\n",
    "        ref_embedding, alt_embedding, config.DISTANCE_METRICS\n",
    "    )\n",
    "    \n",
    "    # Interpret primary score (cosine)\n",
    "    interpretation = interpret_effect_score(\n",
    "        effect_scores[\"cosine\"], \"cosine\", config.EFFECT_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"variant_id\": variant[\"id\"],\n",
    "        \"ref_allele\": variant[\"ref\"],\n",
    "        \"alt_allele\": variant[\"alt\"],\n",
    "        \"variant_type\": variant[\"type\"],\n",
    "        \"ref_sequence_length\": len(ref_seq),\n",
    "        \"alt_sequence_length\": len(alt_seq),\n",
    "        **{f\"{metric}_score\": score for metric, score in effect_scores.items()},\n",
    "        \"predicted_functional\": interpretation[\"functional\"],\n",
    "        \"impact_level\": interpretation[\"impact\"],\n",
    "        \"description\": interpretation[\"description\"]\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nğŸ“Š Analysis Results:\")\n",
    "print(results_df[['variant_id', 'ref_allele', 'alt_allele', 'variant_type', \n",
    "                 'cosine_score', 'predicted_functional', 'impact_level']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ¯ Summary:\")\n",
    "print(f\"  ğŸ“ˆ Total variants analyzed: {len(results_df)}\")\n",
    "print(f\"  âš¡ Predicted functional: {results_df['predicted_functional'].sum()}\")\n",
    "print(f\"  ğŸ“Š Mean cosine score: {results_df['cosine_score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d0b65",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis: Pooling Strategy Comparison\n",
    "\n",
    "Compare different embedding pooling strategies to understand their impact on variant effect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pooling strategies\n",
    "pooling_strategies = [\"mean\", \"max\", \"cls\"]\n",
    "comparison_results = []\n",
    "\n",
    "print(\"ğŸ”¬ Comparing pooling strategies...\")\n",
    "\n",
    "# Use first variant as example\n",
    "test_variant = example_variants[0]\n",
    "ref_seq, alt_seq = create_variant_sequences(test_variant[\"ref\"], test_variant[\"alt\"])\n",
    "\n",
    "for strategy in pooling_strategies:\n",
    "    print(f\"  ğŸ“Š Testing {strategy} pooling...\")\n",
    "    \n",
    "    # Extract embeddings with different pooling\n",
    "    ref_embedding = extract_sequence_embedding(\n",
    "        ref_seq, model, tokenizer, config.DEVICE, strategy\n",
    "    )\n",
    "    alt_embedding = extract_sequence_embedding(\n",
    "        alt_seq, model, tokenizer, config.DEVICE, strategy\n",
    "    )\n",
    "    \n",
    "    # Calculate cosine distance\n",
    "    cosine_score = cosine(ref_embedding, alt_embedding)\n",
    "    \n",
    "    # Interpret effect\n",
    "    interpretation = interpret_effect_score(cosine_score, \"cosine\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"pooling_strategy\": strategy,\n",
    "        \"cosine_score\": cosine_score,\n",
    "        \"predicted_functional\": interpretation[\"functional\"],\n",
    "        \"impact_level\": interpretation[\"impact\"],\n",
    "        \"embedding_shape\": ref_embedding.shape\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(f\"\\nğŸ“Š Pooling Strategy Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize pooling comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['pooling_strategy'], comparison_df['cosine_score'], \n",
    "        color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.xlabel('Pooling Strategy')\n",
    "plt.ylabel('Cosine Distance Score')\n",
    "plt.title('ğŸ§¬ Variant Effect Scores by Pooling Strategy')\n",
    "plt.axhline(y=config.EFFECT_THRESHOLD, color='red', linestyle='--', \n",
    "            label=f'Functional threshold ({config.EFFECT_THRESHOLD})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Pooling strategy comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e75ac1",
   "metadata": {},
   "source": [
    "## 7. Distance Metric Analysis\n",
    "\n",
    "Analyze how different distance metrics perform for variant effect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distance metric correlations\n",
    "print(\"ğŸ“Š Analyzing distance metric relationships...\")\n",
    "\n",
    "# Extract scores for all metrics\n",
    "cosine_scores = results_df['cosine_score'].values\n",
    "euclidean_scores = results_df['euclidean_score'].values\n",
    "manhattan_scores = results_df['manhattan_score'].values\n",
    "\n",
    "# Create correlation matrix\n",
    "metric_data = pd.DataFrame({\n",
    "    'Cosine': cosine_scores,\n",
    "    'Euclidean': euclidean_scores,\n",
    "    'Manhattan': manhattan_scores\n",
    "})\n",
    "\n",
    "correlation_matrix = metric_data.corr()\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f')\n",
    "plt.title('ğŸ”— Distance Metric Correlations')\n",
    "\n",
    "# Score distributions\n",
    "plt.subplot(1, 2, 2)\n",
    "for metric in ['Cosine', 'Euclidean', 'Manhattan']:\n",
    "    plt.hist(metric_data[metric], alpha=0.6, label=metric, bins=15)\n",
    "plt.xlabel('Effect Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('ğŸ“Š Score Distributions by Metric')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ”— Metric Correlations:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Find most discriminative metric\n",
    "functional_variants = results_df[results_df['predicted_functional']]\n",
    "neutral_variants = results_df[~results_df['predicted_functional']]\n",
    "\n",
    "print(f\"\\nğŸ¯ Discriminative Power Analysis:\")\n",
    "for metric in ['cosine_score', 'euclidean_score', 'manhattan_score']:\n",
    "    if len(functional_variants) > 0 and len(neutral_variants) > 0:\n",
    "        func_mean = functional_variants[metric].mean()\n",
    "        neut_mean = neutral_variants[metric].mean()\n",
    "        separation = abs(func_mean - neut_mean)\n",
    "        print(f\"  {metric.split('_')[0].title()}: Separation = {separation:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric.split('_')[0].title()}: Insufficient data for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b843b",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on our analysis, here are the key insights for variant effect prediction using genomic foundation models:\n",
    "\n",
    "#### ğŸ§¬ **Embedding Extraction**\n",
    "- **Mean pooling** generally provides robust sequence representations\n",
    "- **Layer selection** matters - last layer captures task-specific features\n",
    "- **Sequence length** should include sufficient context around variants\n",
    "\n",
    "#### ğŸ“Š **Distance Metrics**\n",
    "- **Cosine distance** is effective for comparing sequence semantics\n",
    "- **Euclidean distance** captures magnitude differences\n",
    "- **Multiple metrics** provide complementary information\n",
    "\n",
    "#### ğŸ¯ **Effect Scoring**\n",
    "- Threshold-based classification works well for binary functional prediction\n",
    "- Score interpretation should consider biological context\n",
    "- Validation with experimental data is crucial\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **ğŸ”§ Technical Recommendations**\n",
    "   - Use mean pooling for stable embeddings\n",
    "   - Include adequate sequence context (â‰¥200bp)\n",
    "   - Batch process for efficiency with large datasets\n",
    "   - Validate thresholds on known functional variants\n",
    "\n",
    "2. **ğŸ§¬ Biological Considerations**\n",
    "   - Consider variant type (SNV vs INDEL) in interpretation\n",
    "   - Account for genomic context (coding vs non-coding)\n",
    "   - Integrate with other genomic features when possible\n",
    "   - Validate predictions with functional assays\n",
    "\n",
    "3. **ğŸ“ˆ Performance Optimization**\n",
    "   - Use appropriate batch sizes for GPU memory\n",
    "   - Cache embeddings for repeated analyses\n",
    "   - Consider model size vs accuracy tradeoffs\n",
    "   - Profile different pooling strategies for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"ğŸ‰ VEP Embedding and Scoring Analysis Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“Š Analysis Summary:\")\n",
    "print(f\"  ğŸ§¬ Variants analyzed: {len(results_df)}\")\n",
    "print(f\"  âš¡ Functional predictions: {results_df['predicted_functional'].sum()}\")\n",
    "print(f\"  ğŸ“ˆ Average effect scores:\")\n",
    "for metric in ['cosine_score', 'euclidean_score', 'manhattan_score']:\n",
    "    mean_score = results_df[metric].mean()\n",
    "    std_score = results_df[metric].std()\n",
    "    print(f\"    {metric.split('_')[0].title()}: {mean_score:.4f} Â± {std_score:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Technical Configuration:\")\n",
    "print(f\"  ğŸ§  Model: {config.MODEL_NAME}\")\n",
    "print(f\"  ğŸ“Š Pooling: {config.POOLING_STRATEGY}\")\n",
    "print(f\"  ğŸ“ Max length: {config.MAX_LENGTH}\")\n",
    "print(f\"  ğŸ¯ Threshold: {config.EFFECT_THRESHOLD}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Next Steps:\")\n",
    "print(f\"  ğŸ”¬ Validate predictions with experimental data\")\n",
    "print(f\"  ğŸ“Š Scale analysis to larger variant datasets\")\n",
    "print(f\"  ğŸ§¬ Integrate with other genomic features\")\n",
    "print(f\"  ğŸ¥ Apply to clinical variant interpretation\")\n",
    "\n",
    "print(f\"\\nâœ… Tutorial complete! Ready for variant effect prediction.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
