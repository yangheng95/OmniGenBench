{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5420f6ebf1395838",
   "metadata": {},
   "source": [
    "# üß¨ Automated Genomic Foundation Model Benchmarking with LoRA Fine-Tuning\n",
    "\n",
    "**Tutorial Version:** 1.0 | **Last Updated:** November 2025 | **Estimated Time:** 20-40 minutes\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this tutorial, you will learn how to:\n",
    "\n",
    "1. **Configure and run automated benchmarking** for genomic foundation models (GFMs) using OmniGenBench\n",
    "2. **Apply parameter-efficient fine-tuning** with LoRA (Low-Rank Adaptation) to reduce computational costs\n",
    "3. **Evaluate model performance** across standardized benchmark suites (RGB, GUE, GB, PGB, BEACON)\n",
    "4. **Interpret evaluation results** and compare multiple models systematically\n",
    "\n",
    "## üéØ Prerequisites\n",
    "\n",
    "**Required Knowledge:**\n",
    "- Basic Python programming\n",
    "- Familiarity with deep learning concepts (fine-tuning, evaluation metrics)\n",
    "- Understanding of genomic sequences (DNA/RNA)\n",
    "\n",
    "**Required Environment:**\n",
    "- Python 3.8+\n",
    "- CUDA-capable GPU (recommended: 8GB+ VRAM)\n",
    "- ~10GB free disk space for models and datasets\n",
    "- Internet connection for downloading models and benchmarks\n",
    "\n",
    "**Installed Packages** (will be installed in Section 1):\n",
    "- `omnigenbench` (core framework)\n",
    "- `peft` (Parameter-Efficient Fine-Tuning)\n",
    "- `bitsandbytes` (quantization support)\n",
    "- `transformers`, `torch`, `accelerate` (deep learning infrastructure)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Background: Why Benchmark with LoRA?\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "**Genomic Foundation Models (GFMs)** are pre-trained on massive genomic corpora and need task-specific adaptation. Traditional fine-tuning:\n",
    "- **Requires 100% of model parameters** to be trainable (millions to billions)\n",
    "- **Demands high memory** (16GB+ VRAM for 186M parameter models)\n",
    "- **Risks catastrophic forgetting** of pre-trained knowledge\n",
    "\n",
    "### The Solution: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA enables efficient fine-tuning by:\n",
    "1. **Freezing pre-trained weights** (no updates to base model)\n",
    "2. **Injecting trainable low-rank matrices** into attention layers\n",
    "3. **Reducing trainable parameters** to <1% of original model (e.g., 52M ‚Üí 0.5M parameters)\n",
    "4. **Maintaining model quality** while using 3-4x less memory\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "W = W‚ÇÄ + ŒîW = W‚ÇÄ + BA\n",
    "```\n",
    "Where:\n",
    "- `W‚ÇÄ`: Frozen pre-trained weights\n",
    "- `B`: Low-rank matrix (d √ó r)\n",
    "- `A`: Low-rank matrix (r √ó k)\n",
    "- `r`: Rank (typically 8-32, much smaller than d,k)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Benchmark Suites Overview\n",
    "\n",
    "OmniGenBench provides 5 comprehensive benchmark suites:\n",
    "\n",
    "| Suite | Full Name | Focus | Tasks | Genome Type | Example Tasks |\n",
    "|-------|-----------|-------|-------|-------------|---------------|\n",
    "| **RGB** | RNA Genome Benchmark | RNA biology | 12 | RNA | Secondary structure, m6A modification |\n",
    "| **BEACON** | Broad Evaluation Across COmputational geNOmics | Multi-domain RNA | 13 | RNA | Translation efficiency, mRNA degradation |\n",
    "| **GUE** | Genomic Understanding Evaluation | DNA understanding | 36 | DNA | Promoter recognition, enhancer prediction |\n",
    "| **GB** | Genomics Benchmarks | Classic DNA tasks | 9 | DNA | Splice site detection, TF binding |\n",
    "| **PGB** | Plant Genome Benchmark | Plant genomics | 7+ | DNA (Plant) | Plant regulatory elements |\n",
    "\n",
    "**Task Types Covered:**\n",
    "- **Sequence Classification**: Binary/multi-class labels (e.g., \"Is this a promoter?\")\n",
    "- **Token Classification**: Per-nucleotide predictions (e.g., splice site positions)\n",
    "- **Regression**: Continuous values (e.g., expression levels)\n",
    "- **Multi-label**: Multiple simultaneous labels (e.g., binding sites for 919 TFs)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Tutorial Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[1. Environment Setup<br/>Install dependencies & verify GPU] --> B[2. Configuration<br/>Set benchmark, model, LoRA parameters]\n",
    "    B --> C[3. Model Loading<br/>Handle model-specific requirements]\n",
    "    C --> D[4. AutoBench Execution<br/>Automated training & evaluation]\n",
    "    D --> E[5. Results Analysis<br/>Interpret metrics & visualizations]\n",
    "    E --> F[6. Optional: Multi-Model Comparison<br/>Batch evaluation of multiple GFMs]\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#fff3e0\n",
    "    style D fill:#e8f5e9\n",
    "    style E fill:#fce4ec\n",
    "    style F fill:#f1f8e9\n",
    "```\n",
    "\n",
    "**Execution Strategy:**\n",
    "- **Quick Test**: 5 minutes (1 epoch, 1000 examples, 1 task)\n",
    "- **Full Evaluation**: 30-60 minutes per model (50 epochs, full datasets, all tasks)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f9512a24bdba8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \udce6 Step 1: Environment Setup and Verification\n",
    "\n",
    "**Purpose:** Install required packages and verify that your environment meets prerequisites.\n",
    "\n",
    "**What This Step Does:**\n",
    "- Installs `omnigenbench` and LoRA dependencies (`peft`, `bitsandbytes`)\n",
    "- Verifies Python version, CUDA availability, and GPU memory\n",
    "- Confirms successful installation\n",
    "\n",
    "**Expected Duration:** 2-3 minutes (depending on internet speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78b6c369b55ed7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.1: Install Required Packages\n",
    "# This cell installs omnigenbench and LoRA dependencies\n",
    "# Skip if already installed (check by running: pip show omnigenbench peft bitsandbytes)\n",
    "\n",
    "import sys\n",
    "print(f\"[INFO] Python Version: {sys.version}\")\n",
    "print(f\"[INFO] Installing packages...\")\n",
    "\n",
    "# Install with upgrade flag to ensure latest versions\n",
    "!pip install omnigenbench peft bitsandbytes transformers accelerate -U -q\n",
    "\n",
    "print(\"[SUCCESS] Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1d38d",
   "metadata": {},
   "source": [
    "### 1.2: Environment Verification\n",
    "\n",
    "**Critical Check:** Ensure GPU is available and has sufficient memory for LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import omnigenbench\n",
    "\n",
    "print(\"[INFO] Environment Verification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Get GPU memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "    \n",
    "    # Memory recommendation\n",
    "    if total_memory < 6:\n",
    "        print(\"[WARNING] GPU memory < 6GB. Consider using smaller models or reducing batch size.\")\n",
    "    elif total_memory < 12:\n",
    "        print(\"[INFO] GPU memory sufficient for models up to ~186M parameters with LoRA.\")\n",
    "    else:\n",
    "        print(\"[INFO] GPU memory excellent for large-scale benchmarking.\")\n",
    "else:\n",
    "    print(\"[WARNING] No GPU detected. Training will be slow on CPU.\")\n",
    "    print(\"          For LoRA fine-tuning, GPU is strongly recommended.\")\n",
    "\n",
    "# Check OmniGenBench installation\n",
    "print(f\"\\nOmniGenBench Version: {omnigenbench.__version__}\")\n",
    "\n",
    "# Check PEFT installation\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"PEFT Version: {peft.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] PEFT not installed. Run the installation cell above.\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"[SUCCESS] Environment verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a6643958f6d06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 2: Configuration - The Single Source of Truth\n",
    "\n",
    "**Purpose:** Define all experimental parameters in one centralized location following the **Single Source of Truth (SSoT)** principle.\n",
    "\n",
    "**What This Step Does:**\n",
    "- Configures benchmark selection (RGB, GUE, GB, PGB, or BEACON)\n",
    "- Selects the genomic foundation model to evaluate\n",
    "- Sets training hyperparameters (epochs, batch size, learning rate)\n",
    "- Defines model-specific LoRA configurations\n",
    "\n",
    "**Design Principle:** All configuration is declared upfront to enable reproducibility and easy experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb04a84df87efd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### 2.1: Import Core Libraries\n",
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from omnigenbench import AutoBench\n",
    "\n",
    "print(\"[SUCCESS] Core libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c46c392be2c3",
   "metadata": {},
   "source": [
    "### 2.2: Experimental Configuration (Single Source of Truth)\n",
    "\n",
    "**Modify the parameters below to customize your benchmarking experiment.**\n",
    "\n",
    "Key parameters explained:\n",
    "- **`BENCHMARK`**: Which benchmark suite to evaluate on (RGB, GUE, GB, PGB, BEACON)\n",
    "- **`GFM_TO_TUNE`**: HuggingFace model identifier or local path\n",
    "- **`MAX_EXAMPLES`**: Limit training examples per task (for quick testing; `None` = full dataset)\n",
    "- **`EPOCHS`**: Number of training epochs (RGB/BEACON typically use 50 for full evaluation)\n",
    "- **`SEED`**: Random seed for reproducibility (or randomize for variance estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8a7e16659a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENTAL CONFIGURATION - Modify these parameters for your experiment\n",
    "# ============================================================================\n",
    "\n",
    "# --- General Training Settings ---\n",
    "BENCHMARK = \"RGB\"                    # Options: \"RGB\", \"GUE\", \"GB\", \"PGB\", \"BEACON\"\n",
    "BATCH_SIZE = 8                       # Adjust based on GPU memory (8-16 typical)\n",
    "PATIENCE = 3                         # Early stopping patience (epochs without improvement)\n",
    "EPOCHS = 1                           # Quick test: 1, Full evaluation: 50\n",
    "MAX_EXAMPLES = 1000                  # Quick test: 1000, Full dataset: None\n",
    "SEED = 42                            # Fixed seed: 42, Random: random.randint(0, 1000)\n",
    "\n",
    "# --- Model Selection ---\n",
    "GFM_TO_TUNE = 'yangheng/OmniGenome-52M'   # Model to evaluate\n",
    "\n",
    "# Available models for testing (uncomment to try others):\n",
    "AVAILABLE_GFMS = [\n",
    "    'yangheng/OmniGenome-52M',              # 52M parameters, balanced performance\n",
    "    # 'yangheng/OmniGenome-186M',           # 186M parameters, higher capacity\n",
    "    # 'yangheng/OmniGenome-v1.5',           # Latest OmniGenome version\n",
    "    # 'zhihan1996/DNABERT-2-117M',          # DNA-specific BERT variant\n",
    "    # 'LongSafari/hyenadna-large-1m-seqlen-hf',  # Long-range DNA model\n",
    "    # 'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species',  # Multi-species DNA\n",
    "    # 'kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16',  # Caduceus architecture\n",
    "    # 'multimolecule/rnafm',                # RNA Foundation Model\n",
    "    \n",
    "    # Special models requiring additional setup:\n",
    "    # 'arcinstitute/evo-1-131k-base',       # Evo models (install `evo` package first)\n",
    "    # 'multimolecule/SpliceBERT-510nt',     # Splice-specific model\n",
    "]\n",
    "\n",
    "# Extract model name for configuration lookup\n",
    "GFM = GFM_TO_TUNE.split('/')[-1]\n",
    "\n",
    "# --- LoRA Hyperparameter Configurations ---\n",
    "# Each model has optimized LoRA settings based on its architecture\n",
    "# Key parameters:\n",
    "#   - r: Rank of low-rank matrices (higher = more capacity, more parameters)\n",
    "#   - lora_alpha: Scaling factor (typically 2-4x the rank)\n",
    "#   - lora_dropout: Regularization to prevent overfitting\n",
    "#   - target_modules: Which layers to apply LoRA (attention/projection layers)\n",
    "\n",
    "LORA_CONFIGS = {\n",
    "    # --- Transformer-based models (BERT/RoBERTa architecture) ---\n",
    "    \"OmniGenome-52M\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],  # Standard attention layers\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"OmniGenome-186M\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"DNABERT-2-117M\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"Wqkv\", \"dense\"],  # DNABERT uses fused QKV projection\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \n",
    "    # --- State Space Models (Mamba/Caduceus architecture) ---\n",
    "    \"caduceus-ph_seqlen-131k_d_model-256_n_layer-16\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"x_proj\", \"out_proj\"],  # SSM-specific projections\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \n",
    "    # --- RNA-specific models ---\n",
    "    \"rnamsm\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"out_proj\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnafm\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnabert\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"SpliceBERT-510nt\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \n",
    "    # --- Hyena models (long-range convolution) ---\n",
    "    \"hyenadna-large-1m-seqlen-hf\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"out_proj\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \n",
    "    # --- Nucleotide Transformer ---\n",
    "    \"nucleotide-transformer-v2-100m-multi-species\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \n",
    "    # --- Evo models (StripedHyena architecture) ---\n",
    "    \"evo-1-131k-base\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"mlp\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1.5-8k-base\": {\n",
    "        \"r\": 8, \n",
    "        \"lora_alpha\": 32, \n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Validation and Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"[INFO] Experimental Configuration Loaded\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Benchmark Suite:    {BENCHMARK}\")\n",
    "print(f\"Model to Evaluate:  {GFM_TO_TUNE}\")\n",
    "print(f\"Training Epochs:    {EPOCHS}\")\n",
    "print(f\"Batch Size:         {BATCH_SIZE}\")\n",
    "print(f\"Max Examples:       {MAX_EXAMPLES if MAX_EXAMPLES else 'Full Dataset'}\")\n",
    "print(f\"Random Seed:        {SEED}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify LoRA config exists for selected model\n",
    "if GFM in LORA_CONFIGS:\n",
    "    lora_cfg = LORA_CONFIGS[GFM]\n",
    "    print(f\"\\nLoRA Configuration for {GFM}:\")\n",
    "    print(f\"  Rank (r):          {lora_cfg['r']}\")\n",
    "    print(f\"  Alpha:             {lora_cfg['lora_alpha']}\")\n",
    "    print(f\"  Dropout:           {lora_cfg['lora_dropout']}\")\n",
    "    print(f\"  Target Modules:    {', '.join(lora_cfg['target_modules'])}\")\n",
    "    \n",
    "    # Estimate trainable parameters (rough approximation)\n",
    "    # For transformer: ~4 * d_model * r * num_layers * num_target_modules\n",
    "    # This is a rough estimate; actual depends on model architecture\n",
    "    print(f\"\\n[INFO] Estimated trainable parameters: <1% of base model\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING] No LoRA config found for '{GFM}'.\")\n",
    "    print(f\"          Available configs: {list(LORA_CONFIGS.keys())}\")\n",
    "    print(f\"          Using default OmniGenome-52M config as fallback.\")\n",
    "    lora_cfg = LORA_CONFIGS[\"OmniGenome-52M\"]\n",
    "\n",
    "print(\"\\n[SUCCESS] Configuration validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89951b937ccc2cc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Step 3: Model Loading and Preparation\n",
    "\n",
    "**Purpose:** Load the selected genomic foundation model and its tokenizer, handling architecture-specific requirements.\n",
    "\n",
    "**What This Step Does:**\n",
    "- Defines a flexible loading function for different model architectures\n",
    "- Handles special cases (multimolecule RNA models, Evo models)\n",
    "- Validates model loading before benchmarking\n",
    "\n",
    "**Architecture-Specific Handling:**\n",
    "- **Standard HuggingFace models**: Direct loading via model name\n",
    "- **Multimolecule models**: Custom tokenizer and base model extraction\n",
    "- **Evo models**: Special handling for StripedHyena architecture and pad token configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1e3f7c5cd7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1: Define Model Loading Function\n",
    "\n",
    "def load_gfm_and_tokenizer(gfm_name):\n",
    "    \"\"\"\n",
    "    Loads a genomic foundation model and its tokenizer with architecture-specific handling.\n",
    "    \n",
    "    This function abstracts away the complexity of loading different model architectures,\n",
    "    providing a unified interface for AutoBench.\n",
    "    \n",
    "    Args:\n",
    "        gfm_name (str): HuggingFace model identifier (e.g., 'yangheng/OmniGenome-52M')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "            - model: Model instance or model name (for standard HF models)\n",
    "            - tokenizer: Tokenizer instance or None (AutoBench will auto-load)\n",
    "    \n",
    "    Supported Architectures:\n",
    "        - Standard Transformers (BERT, RoBERTa): Pass model name, AutoBench handles loading\n",
    "        - Multimolecule models: Load with custom RnaTokenizer and extract base_model\n",
    "        - Evo models: Load with custom config, patch unembed layer, set pad tokens\n",
    "    \"\"\"\n",
    "    print(f\"\\n[INFO] Loading model and tokenizer: {gfm_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # --- Special Case 1: Multimolecule RNA Models ---\n",
    "    if 'multimolecule' in gfm_name:\n",
    "        try:\n",
    "            from multimolecule import RnaTokenizer, AutoModelForTokenPrediction\n",
    "            tokenizer = RnaTokenizer.from_pretrained(gfm_name)\n",
    "            model = AutoModelForTokenPrediction.from_pretrained(\n",
    "                gfm_name, \n",
    "                trust_remote_code=True\n",
    "            ).base_model\n",
    "            print(f\"[SUCCESS] Loaded multimolecule model with custom RnaTokenizer\")\n",
    "            print(f\"          Tokenizer vocab size: {len(tokenizer)}\")\n",
    "            return model, tokenizer\n",
    "        except ImportError:\n",
    "            print(\"[ERROR] 'multimolecule' package not installed.\")\n",
    "            print(\"        Install with: pip install multimolecule\")\n",
    "            raise\n",
    "    \n",
    "    # --- Special Case 2: Evo Models (StripedHyena Architecture) ---\n",
    "    elif 'evo-1' in gfm_name or 'evo2' in gfm_name:\n",
    "        try:\n",
    "            # Load config and model with trust_remote_code\n",
    "            config = AutoConfig.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                gfm_name, \n",
    "                config=config, \n",
    "                trust_remote_code=True\n",
    "            ).backbone  # Extract backbone for fine-tuning\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "            \n",
    "            # Fix pad token configuration (Evo-specific requirement)\n",
    "            tokenizer.pad_token_id = tokenizer.pad_token_type_id\n",
    "            model.config = config\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            \n",
    "            # Patch unembed layer (prevent output projection errors)\n",
    "            model.unembed.unembed = lambda x: x\n",
    "            \n",
    "            print(f\"[SUCCESS] Loaded Evo model with custom patching\")\n",
    "            print(f\"          Model layers: {config.num_hidden_layers}\")\n",
    "            print(f\"          Pad token ID: {tokenizer.pad_token_id}\")\n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Evo model: {e}\")\n",
    "            print(\"        Ensure 'evo' package is installed (if required)\")\n",
    "            print(\"        Refer to: https://github.com/evo-design/evo\")\n",
    "            raise\n",
    "    \n",
    "    # --- Default Case: Standard HuggingFace Models ---\n",
    "    else:\n",
    "        # Return model name; AutoBench will handle loading with proper task head\n",
    "        print(f\"[INFO] Using standard HuggingFace loading\")\n",
    "        print(f\"       AutoBench will auto-load model and tokenizer\")\n",
    "        return gfm_name, None\n",
    "\n",
    "# Test loading function with selected model\n",
    "try:\n",
    "    model, tokenizer = load_gfm_and_tokenizer(GFM_TO_TUNE)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[SUCCESS] Model loading function validated!\")\n",
    "    print(\"=\"*60)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Model loading failed: {e}\")\n",
    "    print(\"        Check model name and network connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43542679a9d3b7f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Step 4: Running Automated Benchmarking with LoRA\n",
    "\n",
    "**Purpose:** Execute the complete evaluation pipeline: data loading, LoRA fine-tuning, evaluation, and result saving.\n",
    "\n",
    "**What This Step Does:**\n",
    "1. **Initialize AutoBench**: Configure benchmark suite, model, and trainer\n",
    "2. **Apply LoRA Configuration**: Inject trainable low-rank adapters into the model\n",
    "3. **Execute Benchmark**: Run automated training and evaluation across all tasks\n",
    "4. **Save Results**: Store metrics, checkpoints, and visualizations\n",
    "\n",
    "**Expected Outputs:**\n",
    "- `autobench_logs/`: Training logs (loss curves, learning rates)\n",
    "- `autobench_evaluations/`: Evaluation results (.mv files for visualization)\n",
    "- Console output: Per-task metrics (accuracy, MCC, F1, etc.)\n",
    "\n",
    "**Duration:** \n",
    "- Quick test (MAX_EXAMPLES=1000, EPOCHS=1): ~5-10 minutes\n",
    "- Full evaluation (MAX_EXAMPLES=None, EPOCHS=50): ~30-60 minutes per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818720a64f31f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1: Initialize AutoBench and Run Evaluation\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"[INFO] Starting Automated Benchmarking with LoRA\")\n",
    "print(\"=\"*60)\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Step 4.1: Load Model and Tokenizer ---\n",
    "print(f\"\\n[Step 1/3] Loading model: {GFM_TO_TUNE}\")\n",
    "model, tokenizer = load_gfm_and_tokenizer(GFM_TO_TUNE)\n",
    "\n",
    "# --- Step 4.2: Initialize AutoBench ---\n",
    "print(f\"\\n[Step 2/3] Initializing AutoBench for benchmark: {BENCHMARK}\")\n",
    "bench = AutoBench(\n",
    "    benchmark=BENCHMARK,              # Benchmark suite (RGB, GUE, GB, PGB, BEACON)\n",
    "    config_or_model=model,            # Model instance or HF model name\n",
    "    tokenizer=tokenizer,              # Tokenizer (None for auto-loading)\n",
    "    overwrite=True,                   # Overwrite existing results\n",
    "    trainer='native',                 # Training backend: 'native', 'accelerate', 'hf_trainer'\n",
    "    autocast='fp16',                  # Mixed precision: 'fp16', 'bf16', 'fp32'\n",
    "    device='cuda',                    # Device: 'cuda' or 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"[INFO] AutoBench initialized successfully\")\n",
    "print(f\"       Benchmark: {BENCHMARK}\")\n",
    "print(f\"       Trainer: native (single GPU)\")\n",
    "print(f\"       Precision: fp16 (mixed precision)\")\n",
    "\n",
    "# --- Step 4.3: Get LoRA Configuration ---\n",
    "lora_config = LORA_CONFIGS.get(GFM, LORA_CONFIGS[\"OmniGenome-52M\"])\n",
    "print(f\"\\n[INFO] Applying LoRA configuration:\")\n",
    "print(f\"       Rank: {lora_config['r']}, Alpha: {lora_config['lora_alpha']}\")\n",
    "print(f\"       Target modules: {lora_config['target_modules']}\")\n",
    "\n",
    "# --- Step 4.4: Run Benchmark with LoRA Fine-Tuning ---\n",
    "print(f\"\\n[Step 3/3] Running benchmark evaluation...\")\n",
    "print(f\"           This may take several minutes...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "try:\n",
    "    bench.run(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=1,\n",
    "        patience=PATIENCE,\n",
    "        max_examples=MAX_EXAMPLES,\n",
    "        seeds=SEED,                   # Single seed or list: [0, 1, 2] for multi-seed\n",
    "        epochs=EPOCHS,\n",
    "        lora_config=lora_config,      # Enable LoRA fine-tuning\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"[SUCCESS] Benchmarking complete!\")\n",
    "    print(f\"          Total time: {elapsed_time/60:.2f} minutes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n[INFO] Results saved to:\")\n",
    "    print(f\"       Evaluations: ./autobench_evaluations/\")\n",
    "    print(f\"       Logs: ./autobench_logs/\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"  1. Check evaluation metrics in the output above\")\n",
    "    print(f\"  2. Visualize results using MetricVisualizer\")\n",
    "    print(f\"  3. Compare with other models (see Step 6)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Benchmarking failed: {e}\")\n",
    "    print(f\"        Check error messages above for details\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386249fc8fc35efa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Step 5: Results Analysis and Interpretation\n",
    "\n",
    "**Purpose:** Understand the evaluation results and interpret model performance.\n",
    "\n",
    "**What This Step Does:**\n",
    "- Explains key evaluation metrics used in genomic benchmarks\n",
    "- Shows how to access and visualize results\n",
    "- Provides guidance on interpreting performance\n",
    "\n",
    "**Key Metrics by Task Type:**\n",
    "\n",
    "| Metric | Task Type | Range | Interpretation |\n",
    "|--------|-----------|-------|----------------|\n",
    "| **MCC** (Matthews Correlation) | Classification | [-1, 1] | 1 = perfect, 0 = random, -1 = inverse |\n",
    "| **Accuracy** | Classification | [0, 1] | Proportion of correct predictions |\n",
    "| **F1 Score** | Classification | [0, 1] | Harmonic mean of precision & recall |\n",
    "| **AUPRC** | Classification | [0, 1] | Area under precision-recall curve |\n",
    "| **MSE** | Regression | [0, ‚àû) | Lower is better (squared error) |\n",
    "| **Spearman œÅ** | Regression | [-1, 1] | Rank correlation (1 = perfect) |\n",
    "\n",
    "**Accessing Results:**\n",
    "\n",
    "```python\n",
    "from metric_visualizer import MetricVisualizer\n",
    "\n",
    "# Load saved results\n",
    "mv = MetricVisualizer.load(\"./autobench_evaluations/<your_results>.mv\")\n",
    "\n",
    "# View summary\n",
    "mv.summary(round=4)\n",
    "\n",
    "# Get specific metrics\n",
    "metrics = mv.get_metrics()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1: View Evaluation Summary\n",
    "\n",
    "# Access the AutoBench MetricVisualizer for detailed results\n",
    "print(\"[INFO] Evaluation Results Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display summary with 4 decimal places\n",
    "bench.mv.summary(round=4)\n",
    "\n",
    "print(\"\\n[INFO] Interpreting Results:\")\n",
    "print(\"  - MCC (Matthews Correlation): Balanced metric for imbalanced datasets\")\n",
    "print(\"    Range: [-1, 1], where 1 = perfect, 0 = random, -1 = inverse\")\n",
    "print(\"  - Accuracy: Simple proportion of correct predictions\")\n",
    "print(\"  - F1 Score: Balance between precision and recall\")\n",
    "print(\"  - For multi-seed runs, results show mean ¬± std deviation\")\n",
    "print(\"\\n[INFO] Results are automatically saved to:\")\n",
    "print(f\"       {bench.mv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1432b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary and Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "Congratulations! You've completed the OmniGenBench LoRA benchmarking tutorial. You now know how to:\n",
    "\n",
    "‚úÖ **Configure automated benchmarking experiments** with reproducible parameters  \n",
    "‚úÖ **Apply LoRA for parameter-efficient fine-tuning** (reducing trainable parameters by 99%)  \n",
    "‚úÖ **Evaluate genomic foundation models** across standardized benchmark suites  \n",
    "‚úÖ **Interpret evaluation metrics** (MCC, F1, accuracy, Spearman correlation)  \n",
    "‚úÖ **Compare multiple models** systematically using consistent protocols  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LoRA enables efficient fine-tuning**: Train large GFMs with 3-4x less memory\n",
    "2. **AutoBench automates evaluation**: Consistent protocols across benchmarks eliminate manual errors\n",
    "3. **Multiple seeds improve reliability**: Running with seeds=[0, 1, 2] quantifies variance\n",
    "4. **Architecture matters**: Different models (Transformer, Mamba, Hyena) require specific LoRA configs\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps and Advanced Topics\n",
    "\n",
    "#### üìö Explore Other Tutorials\n",
    "\n",
    "- **[Attention Score Extraction](../attention_score_extraction/)**: Visualize model attention patterns\n",
    "- **[Genomic Embeddings](../genomic_embeddings/)**: Extract and analyze sequence representations\n",
    "- **[RNA Sequence Design](../rna_sequence_design/)**: Generate RNA sequences with desired structures\n",
    "- **[Variant Effect Prediction](../variant_effect_prediction/)**: Predict mutation impacts\n",
    "\n",
    "#### üî¨ Extend This Tutorial\n",
    "\n",
    "1. **Run Full Evaluations**: Set `MAX_EXAMPLES=None`, `EPOCHS=50`, `seeds=[0,1,2]`\n",
    "2. **Try Different Benchmarks**: Change `BENCHMARK` to \"GUE\", \"GB\", or \"PGB\"\n",
    "3. **Experiment with LoRA Parameters**: \n",
    "   - Increase `r` (rank) to 16 or 32 for more capacity\n",
    "   - Adjust `lora_alpha` to control scaling (typically 2-4x rank)\n",
    "   - Try different `target_modules` (e.g., add \"query\" projections)\n",
    "4. **Multi-Seed Evaluation**: Use `seeds=[0, 1, 2, 3, 4]` for robust statistics\n",
    "\n",
    "#### üìä Analyze Results Programmatically\n",
    "\n",
    "```python\n",
    "from metric_visualizer import MetricVisualizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "mv = MetricVisualizer.load(\"./autobench_evaluations/<your_file>.mv\")\n",
    "\n",
    "# Get all metrics as DataFrame\n",
    "metrics_df = mv.to_dataframe()\n",
    "\n",
    "# Compare specific tasks\n",
    "task_metrics = metrics_df[metrics_df['task'] == 'task_name']\n",
    "print(task_metrics[['accuracy', 'mcc', 'f1']])\n",
    "\n",
    "# Export to CSV for external analysis\n",
    "metrics_df.to_csv(\"benchmark_results.csv\", index=False)\n",
    "```\n",
    "\n",
    "#### ü§ù Contribute to OmniGenBench\n",
    "\n",
    "- **Add new benchmarks**: Create task configs following RGB/GUE structure\n",
    "- **Integrate new models**: Add LoRA configs for emerging GFM architectures\n",
    "- **Report issues**: [GitHub Issues](https://github.com/yangheng95/OmniGenBench/issues)\n",
    "- **Share results**: Submit PRs with benchmark results for new models\n",
    "\n",
    "---\n",
    "\n",
    "### Reproducibility Checklist\n",
    "\n",
    "When sharing results or writing papers, document:\n",
    "\n",
    "- [ ] OmniGenBench version (`omnigenbench.__version__`)\n",
    "- [ ] Model name and version (e.g., `yangheng/OmniGenome-52M`)\n",
    "- [ ] Benchmark suite and task list\n",
    "- [ ] LoRA configuration (r, alpha, target_modules)\n",
    "- [ ] Training hyperparameters (epochs, batch_size, learning_rate)\n",
    "- [ ] Random seeds used (for variance estimation)\n",
    "- [ ] Hardware specifications (GPU model, VRAM)\n",
    "- [ ] Evaluation metrics with standard deviations\n",
    "\n",
    "---\n",
    "\n",
    "### References and Resources\n",
    "\n",
    "**OmniGenBench Documentation:**\n",
    "- üìñ [Getting Started Guide](../../docs/GETTING_STARTED.md)\n",
    "- üîß [CLI Reference](../../docs/cli.rst)\n",
    "- üèóÔ∏è [Architecture Overview](../../framework_architecture_v2.md)\n",
    "- üêõ [Troubleshooting Guide](../../docs/troubleshooting.rst)\n",
    "\n",
    "**Key Papers:**\n",
    "- **LoRA**: Hu et al. (2021) \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "- **OmniGenBench**: [Cite the OmniGenBench paper when published]\n",
    "- **RGB Benchmark**: [Cite the RGB benchmark paper]\n",
    "- **GUE Benchmark**: [Cite the GUE benchmark paper]\n",
    "\n",
    "**Community:**\n",
    "- üí¨ GitHub Discussions: https://github.com/yangheng95/OmniGenBench/discussions\n",
    "- üêõ Bug Reports: https://github.com/yangheng95/OmniGenBench/issues\n",
    "- üìß Contact: YANG, HENG <hy345@exeter.ac.uk>\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: CUDA Out of Memory\n",
    "\n",
    "**Symptoms:** `RuntimeError: CUDA out of memory`\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# Reduce batch size\n",
    "BATCH_SIZE = 4  # or even 2\n",
    "\n",
    "# Enable gradient accumulation\n",
    "bench.run(\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 4*4 = 16\n",
    "    ...\n",
    ")\n",
    "\n",
    "# Use smaller model\n",
    "GFM_TO_TUNE = 'yangheng/OmniGenome-52M'  # Instead of 186M\n",
    "```\n",
    "\n",
    "#### Issue 2: Model Loading Fails\n",
    "\n",
    "**Symptoms:** `OSError: Can't load config for...` or `ImportError`\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# Check network connection\n",
    "!ping huggingface.co\n",
    "\n",
    "# Verify model exists on HuggingFace Hub\n",
    "# Visit: https://huggingface.co/[model_name]\n",
    "\n",
    "# For multimolecule models, install package\n",
    "!pip install multimolecule -U\n",
    "\n",
    "# For Evo models, check requirements\n",
    "# https://github.com/evo-design/evo\n",
    "```\n",
    "\n",
    "#### Issue 3: LoRA Config Not Found\n",
    "\n",
    "**Symptoms:** `[WARNING] No LoRA config found for '<model_name>'`\n",
    "\n",
    "**Solution:** Add custom config to LORA_CONFIGS:\n",
    "```python\n",
    "LORA_CONFIGS[\"your-model-name\"] = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"key\", \"value\", \"dense\"],  # Inspect model architecture\n",
    "    \"bias\": \"none\"\n",
    "}\n",
    "```\n",
    "\n",
    "To find target modules, inspect model:\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"model-name\")\n",
    "print(model)  # Look for attention/projection layers\n",
    "```\n",
    "\n",
    "#### Issue 4: Slow Training on CPU\n",
    "\n",
    "**Symptoms:** Training takes hours, no GPU utilization\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should be True\n",
    "\n",
    "# If False, check CUDA installation\n",
    "!nvidia-smi\n",
    "\n",
    "# Force CUDA device\n",
    "bench = AutoBench(\n",
    "    ...,\n",
    "    device='cuda:0',  # Explicitly specify GPU\n",
    ")\n",
    "```\n",
    "\n",
    "#### Issue 5: Benchmark Data Not Found\n",
    "\n",
    "**Symptoms:** `FileNotFoundError: Benchmark data not available`\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# AutoBench auto-downloads from HuggingFace Hub\n",
    "# Ensure internet connection and HuggingFace credentials\n",
    "\n",
    "# Manual download (if needed)\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\n",
    "    repo_id=\"OmniGenBench/RGB\",  # Example\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./benchmarks/RGB\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Issue 6: Windows Path Errors\n",
    "\n",
    "**Symptoms:** `OSError: [Errno 22] Invalid argument` on Windows\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Use forward slashes or Path objects\n",
    "from pathlib import Path\n",
    "output_dir = Path(\"./autobench_evaluations\")\n",
    "\n",
    "# Avoid Windows-specific paths like:\n",
    "# output_dir = \".\\\\autobench_evaluations\"  # DON'T USE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "If you encounter issues not covered here:\n",
    "\n",
    "1. **Check error messages carefully**: Most errors include helpful diagnostics\n",
    "2. **Search GitHub Issues**: https://github.com/yangheng95/OmniGenBench/issues\n",
    "3. **Review documentation**: Especially [troubleshooting.rst](../../docs/troubleshooting.rst)\n",
    "4. **Ask in Discussions**: https://github.com/yangheng95/OmniGenBench/discussions\n",
    "5. **Report bugs**: Include full error traceback and environment details\n",
    "\n",
    "**When reporting issues, include:**\n",
    "- OmniGenBench version\n",
    "- Python version\n",
    "- PyTorch/CUDA versions\n",
    "- Full error traceback\n",
    "- Minimal code to reproduce\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial** | **Thank you for using OmniGenBench!** üß¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
