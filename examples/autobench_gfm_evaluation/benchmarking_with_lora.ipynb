{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Benchmarking with OmniGenBench using LoRA Fine-tuning\n",
    "This notebook demonstrates how to efficiently fine-tune Genomic Foundation Models (GFMs) using Low-Rank Adaptation (LoRA) with the **OmniGenBench** framework. LoRA is a parameter-efficient fine-tuning technique that allows for quick adaptation of large models to specific tasks while minimizing computational costs.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning (PEFT) technique that significantly reduces the number of trainable parameters. Instead of fine-tuning the entire model, LoRA freezes the pre-trained model weights and injects trainable low-rank decomposition matrices into the layers of the Transformer architecture. This approach dramatically reduces memory footprint and training time, making it possible to fine-tune large models on consumer-grade hardware.\n",
    "\n",
    "### Advantages of LoRA:\n",
    "- **Reduced Computational Cost**: Fewer trainable parameters mean faster training and lower GPU memory requirements.\n",
    "- **Easy Task Switching**: The original pre-trained model remains unchanged. You can have multiple small LoRA adapters for different tasks and switch between them on the fly.\n",
    "- **Comparable Performance**: LoRA can achieve performance comparable to full fine-tuning while being much more efficient."
   ],
   "id": "5420f6ebf1395838"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1.  **Setup & Installation**: Ensures all required libraries are installed.\n",
    "2.  **Import Libraries**: Loads the necessary Python libraries.\n",
    "3.  **Configuration**: Defines all key parameters, including model selection, benchmark choice, training settings, and LoRA configurations.\n",
    "4.  **Model-Specific Loading**: Contains the logic for loading different types of GFMs and their tokenizers.\n",
    "5.  **Running LoRA Fine-tuning**: Demonstrates how to initialize `AutoBench` and run the fine-tuning process with a single command.\n",
    "6.  **Multi-Model LoRA Fine-tuning (Optional)**: Shows how to automate the fine-tuning process for a list of models."
   ],
   "id": "bb4f9512a24bdba8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's ensure all the required packages are installed. If you have already installed them, you can skip this cell."
   ],
   "id": "a740c7d05f60e109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install omnigenbench transformers peft accelerate bitsandbytes"
   ],
   "id": "cf78b6c369b55ed7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for the benchmark."
   ],
   "id": "776a6643958f6d06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from omnigenbench import AutoBench\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ],
   "id": "b3fb04a84df87efd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "This section contains all the settings for the LoRA fine-tuning experiment. You can easily modify these parameters to test different models, benchmarks, or LoRA settings."
   ],
   "id": "b1d6c46c392be2c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Settings ---\n",
    "BENCHMARK = \"GUE\"  # Benchmark suite to use, e.g., \"GUE\", \"RGB\"\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 3\n",
    "EPOCHS = 20\n",
    "MAX_EXAMPLES = 1000  # Use a smaller number for quick testing, set to None for all data\n",
    "SEED = random.randint(0, 1000)\n",
    "\n",
    "# --- Model Selection ---\n",
    "# Choose the Genomic Foundation Model (GFM) to fine-tune\n",
    "GFM_TO_TUNE = 'yangheng/OmniGenome-52M'\n",
    "\n",
    "# List of available GFMs for testing\n",
    "AVAILABLE_GFMS = [\n",
    "    'yangheng/OmniGenome-52M',\n",
    "    'yangheng/OmniGenome-186M',\n",
    "    'yangheng/OmniGenome-v1.5',\n",
    "    'zhihan1996/DNABERT-2-117M',\n",
    "    'LongSafari/hyenadna-large-1m-seqlen-hf',\n",
    "    'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species',\n",
    "    'kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16',\n",
    "    'multimolecule/rnafm',\n",
    "\n",
    "    # Evo models, you need to install the `evo` package according to the official documentation\n",
    "    # 'arcinstitute/evo-1-131k-base',\n",
    "    # 'SpliceBERT-510nt',\n",
    "]\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "# This dictionary contains LoRA settings for different models.\n",
    "# `r`: The rank of the update matrices.\n",
    "# `lora_alpha`: The scaling factor.\n",
    "# `lora_dropout`: The dropout probability for LoRA layers.\n",
    "# `target_modules`: The modules (e.g., attention blocks) to apply LoRA to.\n",
    "LORA_CONFIGS = {\n",
    "    \"OmniGenome-186M\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"caduceus-ph_seqlen-131k_d_model-256_n_layer-16\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"x_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnamsm\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnafm\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnabert\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"agro-nucleotide-transformer-1b\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"SpliceBERT-510nt\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"DNABERT-2-117M\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"Wqkv\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"3utrbert\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"hyenadna-large-1m-seqlen-hf\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"nucleotide-transformer-v2-100m-multi-species\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1-131k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"mlp\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1.5-8k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1-8k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo2_7b\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            # \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - GFM to Tune: {GFM_TO_TUNE}\")\n",
    "print(f\"  - Benchmark: {BENCHMARK}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - LoRA Config: {LORA_CONFIGS.get(GFM_TO_TUNE.split('/')[-1], LORA_CONFIGS['default'])}\")"
   ],
   "id": "d1e8a7e16659a8c9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model-Specific Loading\n",
    "\n",
    "Different GFMs may require specific loading procedures. This function handles these special cases, particularly for models like `multimolecule` or `evo` which might have custom tokenizers or model classes."
   ],
   "id": "89951b937ccc2cc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gfm_and_tokenizer(gfm_name):\n",
    "    \"\"\"Loads a GFM and its tokenizer, handling special cases.\"\"\"\n",
    "    print(f\"\\nLoading model and tokenizer for: {gfm_name}\")\n",
    "    \n",
    "    if 'multimolecule' in gfm_name:\n",
    "        from multimolecule import RnaTokenizer, AutoModelForTokenPrediction\n",
    "        tokenizer = RnaTokenizer.from_pretrained(gfm_name)\n",
    "        model = AutoModelForTokenPrediction.from_pretrained(gfm_name, trust_remote_code=True).base_model\n",
    "        print(\"Loaded multimolecule model with custom classes.\")\n",
    "        \n",
    "    elif 'evo-1' in gfm_name:\n",
    "        # Using transformers to load Evo models\n",
    "        config = AutoConfig.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(gfm_name, config=config, trust_remote_code=True).backbone\n",
    "        tokenizer = AutoTokenizer.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token_id = tokenizer.pad_token_type_id\n",
    "        \n",
    "        # Patch for the unembedding layer\n",
    "        model.config = config\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.unembed.unembed = lambda x: x\n",
    "        print(\"Loaded Evo model with custom patching.\")\n",
    "        \n",
    "    else:\n",
    "        # Default loading for most Hugging Face models\n",
    "        tokenizer = None  # Let AutoBench handle it\n",
    "        model = gfm_name\n",
    "        print(\"Using standard model name for AutoBench.\")\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "print(\"Model loading function defined.\")"
   ],
   "id": "d2e1e3f7c5cd7901"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running LoRA Fine-tuning\n",
    "\n",
    "Now, we'll execute the LoRA fine-tuning process for the selected model. `AutoBench` handles the entire workflow, from data loading and preprocessing to training and evaluation."
   ],
   "id": "43542679a9d3b7f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selected model and tokenizer\n",
    "model, tokenizer = load_gfm_and_tokenizer(GFM_TO_TUNE)\n",
    "\n",
    "# Initialize AutoBench\n",
    "print(f\"\\nInitializing AutoBench for benchmark: {BENCHMARK}\")\n",
    "bench = AutoBench(\n",
    "    benchmark=BENCHMARK,\n",
    "    model_name_or_path=model,\n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True,\n",
    "    trainer='native',  # 'native' or 'accelerate'\n",
    "    autocast='fp16',  # 'fp16', 'bf16', or 'fp32'\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# Get the appropriate LoRA config\n",
    "lora_config = LORA_CONFIGS.get(GFM_TO_TUNE.split('/')[-1], LORA_CONFIGS['default'])\n",
    "\n",
    "# Run the benchmark with LoRA fine-tuning\n",
    "print(f\"\\nStarting LoRA fine-tuning for {GFM_TO_TUNE}...\")\n",
    "bench.run(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    patience=PATIENCE,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    seeds=SEED,\n",
    "    epochs=EPOCHS,\n",
    "    lora_config=lora_config, # Pass the LoRA config here\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ LoRA fine-tuning complete!\")\n",
    "print(\"Check the 'autobench_logs' and 'autobench_evaluations' directories for results.\")"
   ],
   "id": "818720a64f31f294"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Model LoRA Fine-tuning (Optional)\n",
    "\n",
    "The following section demonstrates how to automate the LoRA fine-tuning process for a list of GFMs. Uncomment and run this cell to compare the performance of multiple models with LoRA."
   ],
   "id": "386249fc8fc35efa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this cell to run LoRA fine-tuning for multiple models\n",
    "\n",
    "# print(\"Starting multi-model LoRA fine-tuning...\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# for gfm in AVAILABLE_GFMS:\n",
    "#     try:\n",
    "#         # Load model and tokenizer\n",
    "#         model, tokenizer = load_gfm_and_tokenizer(gfm)\n",
    "\n",
    "#         # Initialize AutoBench\n",
    "#         print(f\"\\nInitializing AutoBench for {gfm} on {BENCHMARK}\")\n",
    "#         bench = AutoBench(\n",
    "#             benchmark=BENCHMARK,\n",
    "#             model_name_or_path=model,\n",
    "#             tokenizer=tokenizer,\n",
    "#             overwrite=True,\n",
    "#             trainer='native',\n",
    "#             autocast='fp16',\n",
    "#             device='cuda',\n",
    "#         )\n",
    "\n",
    "#         # Get the appropriate LoRA config\n",
    "#         lora_config = LORA_CONFIGS.get(gfm.split('/')[-1], LORA_CONFIGS['default'])\n",
    "\n",
    "#         # Run the benchmark\n",
    "#         print(f\"\\nStarting LoRA fine-tuning for {gfm}...\")\n",
    "#         bench.run(\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             patience=PATIENCE,\n",
    "#             max_examples=MAX_EXAMPLES,\n",
    "#             seeds=SEED,\n",
    "#             epochs=EPOCHS,\n",
    "#             lora_config=lora_config,\n",
    "#         )\n",
    "#         print(f\"\\n‚úÖ Finished fine-tuning for {gfm}.\")\n",
    "#         print(\"=\"*50)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå An error occurred while processing {gfm}: {e}\")\n",
    "#         print(\"=\"*50)\n",
    "#         continue\n",
    "\n",
    "# print(\"\\nüéâ All models have been processed!\")"
   ],
   "id": "7f89dd9373b6a3a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
