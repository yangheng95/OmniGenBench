{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5420f6ebf1395838",
   "metadata": {},
   "source": [
    "# 🧬 Automated Genomic Foundation Model Benchmarking with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to **systematically evaluate genomic foundation models** using **OmniGenBench's AutoBench framework**. This guide will walk you through automated benchmarking, parameter-efficient fine-tuning with LoRA, and comprehensive performance analysis across multiple genomic tasks.\n",
    "\n",
    "### 1. The Evaluation Challenge: Why Benchmark Genomic Foundation Models?\n",
    "\n",
    "**Genomic Foundation Model (GFM) benchmarking** is crucial for understanding model capabilities and selecting the best models for specific applications. Systematic evaluation addresses several critical needs:\n",
    "\n",
    "- **Model Selection**: Comparing different foundation models to choose the optimal one for your task\n",
    "- **Performance Validation**: Ensuring models perform reliably across diverse genomic tasks\n",
    "- **Efficiency Analysis**: Understanding computational trade-offs between model size and performance\n",
    "- **Research Advancement**: Contributing to the scientific understanding of genomic AI capabilities\n",
    "\n",
    "The challenge lies in the diversity of genomic tasks and the computational cost of comprehensive evaluation. This is where automated benchmarking becomes essential.\n",
    "\n",
    "### 2. The Solution: AutoBench Framework\n",
    "\n",
    "**AutoBench** is OmniGenBench's automated evaluation system that provides:\n",
    "\n",
    "- **Standardized Benchmarks**: Curated datasets covering diverse genomic tasks (GUE, RGB suites)\n",
    "- **Parameter-Efficient Fine-tuning**: LoRA integration for efficient model adaptation\n",
    "- **Automated Pipeline**: End-to-end evaluation with minimal manual intervention\n",
    "- **Comprehensive Metrics**: Multiple evaluation metrics and statistical analysis\n",
    "- **Scalable Evaluation**: Support for evaluating multiple models across multiple tasks\n",
    "\n",
    "**Benchmark Suites:**\n",
    "\n",
    "| Benchmark Suite | Tasks | Focus Area |\n",
    "|----------------|--------|------------|\n",
    "| **GUE** (Genomic Understanding Evaluation) | DNA/Protein tasks | Basic genomic understanding |\n",
    "| **RGB** (RNA Genome Benchmark) | RNA-specific tasks | RNA biology and function |\n",
    "\n",
    "### 3. The Tool: LoRA + AutoBench Integration\n",
    "\n",
    "#### Parameter-Efficient Fine-Tuning with LoRA\n",
    "**Low-Rank Adaptation (LoRA)** enables efficient fine-tuning of large genomic foundation models by:\n",
    "\n",
    "1. **Freezing Base Model**: Original model weights remain unchanged\n",
    "2. **Low-Rank Decomposition**: Adding small trainable matrices to transformer layers\n",
    "3. **Reduced Parameters**: Dramatically fewer trainable parameters (~0.1-1% of original)\n",
    "4. **Task Switching**: Easy switching between different task-specific adapters\n",
    "\n",
    "#### AutoBench Integration\n",
    "The integration provides:\n",
    "- **Automated LoRA Configuration**: Optimal LoRA parameters for genomic tasks\n",
    "- **Batch Processing**: Evaluate multiple models and tasks simultaneously\n",
    "- **Resource Management**: Efficient GPU memory usage and training optimization\n",
    "- **Result Aggregation**: Comprehensive performance reporting and analysis\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Automated Benchmarking\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for Automated GFM Benchmarking\"\n",
    "        A[\"📥 Step 1: Setup and Configuration<br/>Configure benchmarks, models, and LoRA parameters\"] --> B[\"🔧 Step 2: Model Loading and Preparation<br/>Load foundation models and initialize AutoBench\"]\n",
    "        B --> C[\"🎓 Step 3: Automated Benchmarking<br/>Run systematic evaluation across tasks and models\"]\n",
    "        C --> D[\"🔮 Step 4: Analysis and Interpretation<br/>Analyze results and derive insights\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "Let's start systematically evaluating genomic foundation models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f9512a24bdba8",
   "metadata": {},
   "source": [
    "## 🚀 Step 1: Setup and Configuration\n",
    "\n",
    "This first step focuses on setting up our automated benchmarking environment and configuring the evaluation parameters.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required packages for automated benchmarking with LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740c7d05f60e109",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's ensure all the required packages are installed. If you have already installed them, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78b6c369b55ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omnigenbench transformers peft accelerate bitsandbytes -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a6643958f6d06",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb04a84df87efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from omnigenbench import AutoBench\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c46c392be2c3",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "This section contains all the settings for the LoRA fine-tuning experiment. You can easily modify these parameters to test different models, benchmarks, or LoRA settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8a7e16659a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Settings ---\n",
    "BENCHMARK = \"RGB\"  # Benchmark suite to use, e.g., \"GUE\", \"RGB\"\n",
    "BATCH_SIZE = 8\n",
    "PATIENCE = 3\n",
    "EPOCHS = 1\n",
    "MAX_EXAMPLES = 1000  # Use a smaller number for quick testing, set to None for all data\n",
    "SEED = random.randint(0, 1000)\n",
    "\n",
    "# --- Model Selection ---\n",
    "# Choose the Genomic Foundation Model (GFM) to fine-tune\n",
    "GFM_TO_TUNE = 'yangheng/OmniGenome-52M'\n",
    "\n",
    "# List of available GFMs for testing\n",
    "AVAILABLE_GFMS = [\n",
    "    'yangheng/OmniGenome-52M',\n",
    "    # 'yangheng/OmniGenome-186M',\n",
    "    # 'yangheng/OmniGenome-v1.5',\n",
    "    # 'zhihan1996/DNABERT-2-117M',\n",
    "    # 'LongSafari/hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species',\n",
    "    # 'kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16',\n",
    "    # 'multimolecule/rnafm',\n",
    "\n",
    "    # Evo models, you need to install the `evo` package according to the official documentation\n",
    "    # 'arcinstitute/evo-1-131k-base',\n",
    "    # 'SpliceBERT-510nt',\n",
    "]\n",
    "GFM = GFM_TO_TUNE.split('/')[-1]\n",
    "# --- LoRA Configuration ---\n",
    "# This dictionary contains LoRA settings for different models.\n",
    "# `r`: The rank of the update matrices.\n",
    "# `lora_alpha`: The scaling factor.\n",
    "# `lora_dropout`: The dropout probability for LoRA layers.\n",
    "# `target_modules`: The modules (e.g., attention blocks) to apply LoRA to.\n",
    "LORA_CONFIGS = {\n",
    "    \"OmniGenome-52M\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"OmniGenome-186M\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"caduceus-ph_seqlen-131k_d_model-256_n_layer-16\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"x_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnamsm\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnafm\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"rnabert\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"agro-nucleotide-transformer-1b\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"SpliceBERT-510nt\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"DNABERT-2-117M\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"Wqkv\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"3utrbert\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"hyenadna-large-1m-seqlen-hf\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"in_proj\", \"out_proj\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"nucleotide-transformer-v2-100m-multi-species\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"key\", \"value\", \"dense\"], \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1-131k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"mlp\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1.5-8k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo-1-8k-base\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"evo2_7b\": {\n",
    "        \"r\": 8, \"lora_alpha\": 32, \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"Wqkv\", \"out_proj\",\n",
    "            \"l1\", \"l2\", \"l3\",\n",
    "            # \"projections\",\n",
    "            \"out_filter_dense\"\n",
    "        ],\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - GFM to Tune: {GFM_TO_TUNE}\")\n",
    "print(f\"  - Benchmark: {BENCHMARK}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - LoRA Config: {LORA_CONFIGS.get(GFM, LORA_CONFIGS[GFM])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89951b937ccc2cc2",
   "metadata": {},
   "source": [
    "## 4. Model-Specific Loading\n",
    "\n",
    "Different GFMs may require specific loading procedures. This function handles these special cases, particularly for models like `multimolecule` or `evo` which might have custom tokenizers or model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1e3f7c5cd7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gfm_and_tokenizer(gfm_name):\n",
    "    \"\"\"Loads a GFM and its tokenizer, handling special cases.\"\"\"\n",
    "    print(f\"\\nLoading model and tokenizer for: {gfm_name}\")\n",
    "    \n",
    "    if 'multimolecule' in gfm_name:\n",
    "        from multimolecule import RnaTokenizer, AutoModelForTokenPrediction\n",
    "        tokenizer = RnaTokenizer.from_pretrained(gfm_name)\n",
    "        model = AutoModelForTokenPrediction.from_pretrained(gfm_name, trust_remote_code=True).base_model\n",
    "        print(\"Loaded multimolecule model with custom classes.\")\n",
    "        \n",
    "    elif 'evo-1' in gfm_name:\n",
    "        # Using transformers to load Evo models\n",
    "        config = AutoConfig.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(gfm_name, config=config, trust_remote_code=True).backbone\n",
    "        tokenizer = AutoTokenizer.from_pretrained(gfm_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token_id = tokenizer.pad_token_type_id\n",
    "        \n",
    "        # Patch for the unembedding layer\n",
    "        model.config = config\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.unembed.unembed = lambda x: x\n",
    "        print(\"Loaded Evo model with custom patching.\")\n",
    "        \n",
    "    else:\n",
    "        # Default loading for most Hugging Face models\n",
    "        tokenizer = None  # Let AutoBench handle it\n",
    "        model = gfm_name\n",
    "        print(\"Using standard model name for AutoBench.\")\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "print(\"Model loading function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43542679a9d3b7f9",
   "metadata": {},
   "source": [
    "## 5. Running LoRA Fine-tuning\n",
    "\n",
    "Now, we'll execute the LoRA fine-tuning process for the selected model. `AutoBench` handles the entire workflow, from data loading and preprocessing to training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818720a64f31f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selected model and tokenizer\n",
    "model, tokenizer = load_gfm_and_tokenizer(GFM_TO_TUNE)\n",
    "\n",
    "# Initialize AutoBench\n",
    "print(f\"\\nInitializing AutoBench for benchmark: {BENCHMARK}\")\n",
    "bench = AutoBench(\n",
    "    benchmark=BENCHMARK,\n",
    "    config_or_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True,\n",
    "    trainer='native',  # 'native' or 'accelerate'\n",
    "    autocast='fp16',  # 'fp16', 'bf16', or 'fp32'\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# Get the appropriate LoRA config\n",
    "lora_config = LORA_CONFIGS.get(GFM, LORA_CONFIGS[GFM])\n",
    "\n",
    "# Run the benchmark with LoRA fine-tuning\n",
    "print(f\"\\nStarting LoRA fine-tuning for {GFM_TO_TUNE}...\")\n",
    "bench.run(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    patience=PATIENCE,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    seeds=SEED,\n",
    "    epochs=EPOCHS,\n",
    "    lora_config=lora_config, # Pass the LoRA config here\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 LoRA fine-tuning complete!\")\n",
    "print(\"Check the 'autobench_logs' and 'autobench_evaluations' directories for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386249fc8fc35efa",
   "metadata": {},
   "source": [
    "## 6. Multi-Model LoRA Fine-tuning (Optional)\n",
    "\n",
    "The following section demonstrates how to automate the LoRA fine-tuning process for a list of GFMs. Uncomment and run this cell to compare the performance of multiple models with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89dd9373b6a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this cell to run LoRA fine-tuning for multiple models\n",
    "\n",
    "# print(\"Starting multi-model LoRA fine-tuning...\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# for gfm in AVAILABLE_GFMS:\n",
    "#     try:\n",
    "#         # Load model and tokenizer\n",
    "#         model, tokenizer = load_gfm_and_tokenizer(gfm)\n",
    "\n",
    "#         # Initialize AutoBench\n",
    "#         print(f\"\\nInitializing AutoBench for {gfm} on {BENCHMARK}\")\n",
    "#         bench = AutoBench(\n",
    "#             benchmark=BENCHMARK,\n",
    "#             config_or_model=model,\n",
    "#             tokenizer=tokenizer,\n",
    "#             overwrite=True,\n",
    "#             trainer='native',\n",
    "#             autocast='fp16',\n",
    "#             device='cuda',\n",
    "#         )\n",
    "\n",
    "#         # Get the appropriate LoRA config\n",
    "#         lora_config = LORA_CONFIGS.get(gfm.split('/')[-1], LORA_CONFIGS['default'])\n",
    "\n",
    "#         # Run the benchmark\n",
    "#         print(f\"\\nStarting LoRA fine-tuning for {gfm}...\")\n",
    "#         bench.run(\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             patience=PATIENCE,\n",
    "#             max_examples=MAX_EXAMPLES,\n",
    "#             seeds=SEED,\n",
    "#             epochs=EPOCHS,\n",
    "#             lora_config=lora_config,\n",
    "#         )\n",
    "#         print(f\"\\n✅ Finished fine-tuning for {gfm}.\")\n",
    "#         print(\"=\"*50)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n❌ An error occurred while processing {gfm}: {e}\")\n",
    "#         print(\"=\"*50)\n",
    "#         continue\n",
    "\n",
    "# print(\"\\n🎉 All models have been processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
