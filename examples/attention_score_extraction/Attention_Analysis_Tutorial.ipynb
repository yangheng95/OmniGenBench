{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d33370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env set: TRANSFORMERS_NO_TORCHVISION= 1\n"
     ]
    }
   ],
   "source": [
    "# Environment safety for Transformers/torchvision integration issues\n",
    "import os\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TORCHVISION\", \"1\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"DISABLE_TELEMETRY\", \"1\")\n",
    "print(\"Env set: TRANSFORMERS_NO_TORCHVISION=\", os.environ.get(\"TRANSFORMERS_NO_TORCHVISION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cc335",
   "metadata": {},
   "source": [
    "# 沐 Attention Analysis Tutorial: Exploring Genomic Attention Patterns\n",
    "\n",
    "## 沁ｯ Key Feature: ALL OmniModel Types Support Attention Extraction!\n",
    "\n",
    "This tutorial demonstrates attention extraction capabilities that are now available in **ALL OmniGenBench models** through the `EmbeddingMixin`.\n",
    "\n",
    "### Supported Model Types\n",
    "笨 `OmniModelForEmbedding` - Dedicated embedding extraction  \n",
    "笨 `OmniModelForSequenceClassification` - Classification + Attention  \n",
    "笨 `OmniModelForSequenceRegression` - Regression + Attention  \n",
    "笨 `OmniModelForTokenClassification` - Token classification + Attention  \n",
    "笨 `OmniModelForMLM` - Masked language modeling + Attention  \n",
    "笨 **All other OmniModel variants** - Task-specific + Attention\n",
    "\n",
    "### What You'll Learn\n",
    "1. 洫ｬ Extract attention scores from genomic sequences\n",
    "2. 沒 Analyze attention patterns and statistics\n",
    "3. 沁ｨ Visualize attention heatmaps\n",
    "4. 沐ｬ Compare attention patterns across sequences\n",
    "5. 汳｡ Use attention extraction with any model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b94775",
   "metadata": {},
   "source": [
    "## 泅 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6771630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: omnigenbench in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (0.3.22a0)\n",
      "Collecting omnigenbench\n",
      "  Downloading omnigenbench-0.3.24a0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (2.7.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: transformers in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (4.54.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: matplotlib in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (3.10.6)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: seaborn in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: findfile>=2.0.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (2.1.1)\n",
      "Requirement already satisfied: autocuda>=0.16 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (0.16)\n",
      "Requirement already satisfied: metric-visualizer>=0.9.6 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (0.9.17)\n",
      "Requirement already satisfied: termcolor in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (3.1.0)\n",
      "Requirement already satisfied: gitpython in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (3.1.45)\n",
      "Requirement already satisfied: pandas in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (2.3.1)\n",
      "Requirement already satisfied: viennarna in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (1.7.2)\n",
      "Requirement already satisfied: accelerate in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (1.9.0)\n",
      "Requirement already satisfied: packaging in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (25.0)\n",
      "Requirement already satisfied: peft in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (0.16.0)\n",
      "Requirement already satisfied: dill in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (0.3.8)\n",
      "Requirement already satisfied: plotly in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (6.3.1)\n",
      "Requirement already satisfied: logomaker in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from omnigenbench) (0.8.7)\n",
      "Requirement already satisfied: filelock in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: tikzplotlib in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (0.10.1)\n",
      "Requirement already satisfied: scipy in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (1.16.1)\n",
      "Requirement already satisfied: tabulate in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (0.9.0)\n",
      "Requirement already satisfied: natsort in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (8.4.0)\n",
      "Requirement already satisfied: update-checker in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (0.18.0)\n",
      "Requirement already satisfied: click in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (8.2.1)\n",
      "Requirement already satisfied: openpyxl in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (3.1.5)\n",
      "Requirement already satisfied: xlsxwriter in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (3.2.9)\n",
      "Requirement already satisfied: colorama in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from metric-visualizer>=0.9.6->omnigenbench) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from pandas->omnigenbench) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from pandas->omnigenbench) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from accelerate->omnigenbench) (7.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from gitpython->omnigenbench) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython->omnigenbench) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from openpyxl->metric-visualizer>=0.9.6->omnigenbench) (2.0.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from plotly->omnigenbench) (2.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from scikit-learn->omnigenbench) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from scikit-learn->omnigenbench) (3.6.0)\n",
      "Requirement already satisfied: webcolors in /home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages (from tikzplotlib->metric-visualizer>=0.9.6->omnigenbench) (24.11.1)\n",
      "Downloading omnigenbench-0.3.24a0-py3-none-any.whl (263 kB)\n",
      "Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, matplotlib, tokenizers, nvidia-cusolver-cu12, transformers, torch, omnigenbench\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "\u001b[2K  Attempting uninstall: triton笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 0/21\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: triton 3.3.1[0m \u001b[32m 0/21\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling triton-3.3.1:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 0/21\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1笏―u001b[0m \u001b[32m 0/21\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 1/21\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77笏≫煤笏≫煤笏―u001b[0m \u001b[32m 1/21\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 1/21\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 2/21\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85笏≫煤\u001b[0m \u001b[32m 3/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 4/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77笏―u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77笏≫煤笏―u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.11.1.6笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.11.1.6:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77笏―u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80笏―u001b[0m \u001b[32m 9/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1笏≫煤\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1笏≫煤笏≫煤\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu1290m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2笏≫煤\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4笏≫煤笏―u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4笏≫煤笏≫煤笏―u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m13/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17笏≫煤笏―u001b[0m \u001b[32m13/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m13/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17笏≫煤笏≫煤笏―u001b[0m \u001b[32m13/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: matplotlib\u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m14/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: matplotlib 3.10.6笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m14/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling matplotlib-3.10.6:笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]12]\n",
      "\u001b[2K      Successfully uninstalled matplotlib-3.10.60m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]\n",
      "\u001b[2K  Attempting uninstall: tokenizers笏≫煤\u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.2m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.2:m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.290m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m15/21\u001b[0m [matplotlib]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu1290m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m16/21\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\u001b[0m \u001b[32m16/21\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m16/21\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2笏≫煤\u001b[0m \u001b[32m16/21\u001b[0m [tokenizers]\n",
      "\u001b[2K  Attempting uninstall: transformers笏≫煤笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: transformers 4.54.190m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling transformers-4.54.1:笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/21\u001b[0m [transformers]u12]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.54.1m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/21\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torch笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/21\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: torch 2.7.190m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/21\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏―u001b[0m \u001b[32m19/21\u001b[0m [torch]rs]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏―u001b[0m \u001b[32m19/21\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: omnigenbench笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏―u001b[0m \u001b[32m19/21\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: omnigenbench 0.3.22a0m笊ｺ\u001b[0m\u001b[90m笏―u001b[0m \u001b[32m20/21\u001b[0m [omnigenbench]\n",
      "\u001b[2K    Uninstalling omnigenbench-0.3.22a0:笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏―u001b[0m \u001b[32m20/21\u001b[0m [omnigenbench]\n",
      "\u001b[2K      Successfully uninstalled omnigenbench-0.3.22a0\u001b[0m\u001b[90m笏―u001b[0m \u001b[32m20/21\u001b[0m [omnigenbench]\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21/21\u001b[0m [omnigenbench][0m [omnigenbench]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.10.0 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "torchvision 0.22.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "xformers 0.0.31 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.10.7 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 omnigenbench-0.3.24a0 tokenizers-0.22.1 torch-2.9.0 transformers-4.57.1 triton-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install omnigenbench matplotlib seaborn -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62511985",
   "metadata": {},
   "source": [
    "## 沒 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80008fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Import various model types - ALL support attention extraction!\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01momnigenbench\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     OmniModelForEmbedding,\n\u001b[32m      9\u001b[39m     OmniModelForSequenceClassification,\n\u001b[32m     10\u001b[39m     OmniModelForSequenceRegression,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Set up plotting\u001b[39;00m\n\u001b[32m     14\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mseaborn-v0_8\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/omnigenbench/__init__.py:94\u001b[39m\n\u001b[32m     91\u001b[39m __license__ = \u001b[33m\"\u001b[39m\u001b[33mApache-2.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Import core auto components\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_bench\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_bench\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoBench\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbench_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbench_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BenchHub\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/omnigenbench/auto/auto_bench/auto_bench.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmetric_visualizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetricVisualizer\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer \u001b[38;5;28;01mas\u001b[39;00m HFTrainer\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabstract_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OmniTokenizer\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OmniLoraModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/trainer.py:42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.getenv(\u001b[33m\"\u001b[39m\u001b[33mWANDB_MODE\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33moffline\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m笞呻ｸ  Running in WANDB offline mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TrainingArguments\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     47\u001b[39m     PushToHubMixin,\n\u001b[32m     48\u001b[39m     flatten_dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     logging,\n\u001b[32m     54\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PreTrainedModel' from 'transformers' (/home/yangheng/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Import various model types - ALL support attention extraction!\n",
    "from omnigenbench import (\n",
    "    OmniModelForEmbedding,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForSequenceRegression,\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"笨 Imports successful!\")\n",
    "print(\"汳｡ All OmniModel types support attention extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef397b99",
   "metadata": {},
   "source": [
    "## 沐ｧ Load Model\n",
    "\n",
    "### Important: You Can Use ANY OmniModel Type!\n",
    "\n",
    "The attention extraction functionality is available in all model types. Choose the one that fits your use case:\n",
    "- Use `OmniModelForEmbedding` for dedicated embedding/attention extraction\n",
    "- Use `OmniModelForSequenceClassification` if you also need classification\n",
    "- Use `OmniModelForSequenceRegression` if you also need regression\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"anonymous8/OmniGenome-186M\"  # Change to your preferred model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"沐ｧ Loading model: {model_name}\")\n",
    "print(f\"沒ｱ Device: {device}\")\n",
    "\n",
    "# Option 1: Use dedicated embedding model\n",
    "model = OmniModelForEmbedding(model_name, trust_remote_code=True)\n",
    "\n",
    "# Option 2: Use classification model (also supports attention extraction!)\n",
    "# model = OmniModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Option 3: Use regression model (also supports attention extraction!)\n",
    "# model = OmniModelForSequenceRegression.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"笨 Model loaded: {type(model).__name__}\")\n",
    "print(f\"汳｡ This model supports both task-specific operations AND attention extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fa5ca",
   "metadata": {},
   "source": [
    "## 洫ｬ Prepare Test Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example genomic sequences with different characteristics\n",
    "test_sequences = [\n",
    "    \"ATCGATCGATCGTAGCTAGCTAGCT\",  # Regular sequence\n",
    "    \"GGCCTTAACCGGTTAACCGGTTAA\",   # GC-rich sequence\n",
    "    \"TTTTAAAACCCCGGGGTTTTAAAA\",   # Repeat pattern\n",
    "    \"AUGCGAUCUCGAGCUACGUCGAUGCUAGCUCGAUGGCAUCCGAUUCGAGCUACGUCGAUGCUAG\",  # Longer sequence\n",
    "]\n",
    "\n",
    "print(\"洫ｬ Test sequences prepared:\")\n",
    "for i, seq in enumerate(test_sequences, 1):\n",
    "    print(f\"  {i}. {seq[:40]}{'...' if len(seq) > 40 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7291392",
   "metadata": {},
   "source": [
    "## 1ｸ鞘Ε Extract Attention Scores from Single Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention from the first sequence\n",
    "sequence = test_sequences[0]\n",
    "\n",
    "print(f\"沐 Analyzing sequence: {sequence}\")\n",
    "print(\"竢ｳ Extracting attention scores...\")\n",
    "\n",
    "attention_result = model.extract_attention_scores(\n",
    "    sequence=sequence,\n",
    "    max_length=128,\n",
    "    layer_indices=None,  # Extract all layers (or specify [0, 5, 11] for specific layers)\n",
    "    head_indices=None,   # Extract all heads (or specify [0, 1, 2] for specific heads)\n",
    "    return_on_cpu=True\n",
    ")\n",
    "\n",
    "print(f\"\\n笨 Attention extraction successful!\")\n",
    "print(f\"沒 Attention tensor shape: {attention_result['attentions'].shape}\")\n",
    "print(f\"   Format: (layers, heads, seq_len, seq_len)\")\n",
    "print(f\"沐､ Number of tokens: {len(attention_result['tokens'])}\")\n",
    "print(f\"沁ｯ First 10 tokens: {attention_result['tokens'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aedf1",
   "metadata": {},
   "source": [
    "## 2ｸ鞘Ε Compute Attention Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f297324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute comprehensive attention statistics\n",
    "stats = model.get_attention_statistics(\n",
    "    attention_result['attentions'],\n",
    "    attention_result['attention_mask'],\n",
    "    layer_aggregation=\"mean\",  # Options: mean, max, sum, first, last\n",
    "    head_aggregation=\"mean\"    # Options: mean, max, sum\n",
    ")\n",
    "\n",
    "print(\"沒 Attention Statistics:\")\n",
    "print(f\"  Attention matrix shape: {stats['attention_matrix'].shape}\")\n",
    "print(f\"  Average attention entropy: {stats['attention_entropy'].mean():.4f}\")\n",
    "print(f\"  Max attention concentration: {stats['attention_concentration'].max():.4f}\")\n",
    "print(f\"  Average self-attention score: {stats['self_attention_scores'].mean():.4f}\")\n",
    "print(f\"  Max attention per position (top 5): {stats['max_attention_per_position'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2e7cc",
   "metadata": {},
   "source": [
    "## 3ｸ鞘Ε Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention pattern for a specific layer and head\n",
    "fig = model.visualize_attention_pattern(\n",
    "    attention_result=attention_result,\n",
    "    layer_idx=0,   # First layer\n",
    "    head_idx=0,    # First attention head\n",
    "    save_path=\"attention_heatmap.png\",\n",
    "    figsize=(12, 10)\n",
    ")\n",
    "\n",
    "if fig is not None:\n",
    "    print(\"笨 Attention heatmap generated and saved!\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"笞ｸ  Visualization skipped (matplotlib not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003db7",
   "metadata": {},
   "source": [
    "## 4ｸ鞘Ε Batch Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention from multiple sequences efficiently\n",
    "print(\"竢ｳ Extracting attention from batch of sequences...\")\n",
    "\n",
    "batch_results = model.batch_extract_attention_scores(\n",
    "    sequences=test_sequences[:3],  # First 3 sequences\n",
    "    batch_size=2,\n",
    "    max_length=128,\n",
    "    layer_indices=[0, -1],  # First and last layer only\n",
    "    head_indices=[0, 1, 2], # First 3 heads only\n",
    "    return_on_cpu=True\n",
    ")\n",
    "\n",
    "print(f\"笨 Batch attention extraction successful!\")\n",
    "print(f\"沒 Processed {len(batch_results)} sequences\")\n",
    "\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"  Sequence {i} attention shape: {result['attentions'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a648a",
   "metadata": {},
   "source": [
    "## 5ｸ鞘Ε Compare Attention Patterns Across Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns between different sequences\n",
    "print(\"沐ｬ Comparing attention patterns across sequences...\\n\")\n",
    "\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    stats = model.get_attention_statistics(\n",
    "        result['attentions'],\n",
    "        result['attention_mask']\n",
    "    )\n",
    "    \n",
    "    seq_preview = test_sequences[i-1][:30] + \"...\"\n",
    "    print(f\"Sequence {i}: {seq_preview}\")\n",
    "    print(f\"  Attention entropy: {stats['attention_entropy'].mean():.4f}\")\n",
    "    print(f\"  Self-attention: {stats['self_attention_scores'].mean():.4f}\")\n",
    "    print(f\"  Concentration: {stats['attention_concentration'].mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b81ae",
   "metadata": {},
   "source": [
    "## 6ｸ鞘Ε Advanced: Embedding Extraction\n",
    "\n",
    "### Bonus: Extract Embeddings Too!\n",
    "\n",
    "Since all OmniModel types support both attention AND embedding extraction, you can easily get both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80add529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from the same model\n",
    "print(\"沁ｯ Extracting embeddings from the same model...\")\n",
    "\n",
    "# Single sequence\n",
    "embedding = model.encode(test_sequences[0], agg=\"mean\")\n",
    "print(f\"Single embedding shape: {embedding.shape}\")\n",
    "\n",
    "# Batch encoding\n",
    "embeddings = model.batch_encode(test_sequences, batch_size=4, agg=\"mean\")\n",
    "print(f\"Batch embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Compute similarity\n",
    "similarity = model.compute_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"\\nSimilarity between sequences 1 and 2: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n笨 Both attention and embeddings extracted from the same model!\")\n",
    "print(\"汳｡ This works with ALL OmniModel types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9163e4e",
   "metadata": {},
   "source": [
    "## 沁 Summary\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Universal Support**: ALL OmniModel types support attention and embedding extraction\n",
    "2. **Flexible API**: Same API works across all model types\n",
    "3. **Rich Features**:\n",
    "   - `extract_attention_scores()` - Single sequence attention\n",
    "   - `batch_extract_attention_scores()` - Batch processing\n",
    "   - `get_attention_statistics()` - Attention analysis\n",
    "   - `visualize_attention_pattern()` - Visualization\n",
    "   - `encode()` / `batch_encode()` - Embedding extraction\n",
    "   - `compute_similarity()` - Similarity computation\n",
    "\n",
    "### Supported Model Types\n",
    "笨 OmniModelForEmbedding  \n",
    "笨 OmniModelForSequenceClassification  \n",
    "笨 OmniModelForSequenceRegression  \n",
    "笨 OmniModelForTokenClassification  \n",
    "笨 OmniModelForMLM  \n",
    "笨 All other OmniModel variants!\n",
    "\n",
    "### Key Benefits\n",
    "- 沁ｯ Use task-specific models for their intended purpose\n",
    "- 沐 Extract attention and embeddings from the same model\n",
    "- 汳ｪ No need for separate embedding models\n",
    "- 泅 Efficient and unified API\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Try using attention extraction with your fine-tuned models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
