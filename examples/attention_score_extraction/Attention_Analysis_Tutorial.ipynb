{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65523a6a",
   "metadata": {},
   "source": [
    "<!-- \n",
    "Tutorial Information\n",
    "====================\n",
    "File: Attention_Analysis_Tutorial.ipynb\n",
    "Last Updated: November 2, 2025\n",
    "Author: YANG, HENG <hy345@exeter.ac.uk>\n",
    "Version: 2.0 (Comprehensive Review & Enhancement)\n",
    "\n",
    "Prerequisites:\n",
    "- Python 3.8+\n",
    "- OmniGenBench library (pip install omnigenbench)\n",
    "- Basic understanding of genomic sequences and PyTorch\n",
    "- Recommended: CUDA-capable GPU (CPU works but slower)\n",
    "\n",
    "Estimated Time: 30-45 minutes\n",
    "Difficulty Level: Intermediate\n",
    "\n",
    "Learning Path:\n",
    "1. Complete this tutorial first\n",
    "2. Then explore: Genomic Embeddings Tutorial\n",
    "3. Advanced: RNA Secondary Structure Prediction\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cc335",
   "metadata": {},
   "source": [
    "# Attention Score Extraction from Genomic Foundation Models\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates how to extract and analyze attention patterns from genomic foundation models (GFMs) to understand sequence representations learned by transformer architectures.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By completing this tutorial, you will be able to:\n",
    "\n",
    "1. **Extract** attention scores from genomic sequences using OmniGenBench models\n",
    "2. **Analyze** attention patterns using statistical metrics\n",
    "3. **Visualize** attention heatmaps to interpret model focus\n",
    "4. **Compare** attention patterns across different sequences\n",
    "5. **Apply** attention extraction to any OmniModel type (embedding, classification, regression, etc.)\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "Attention mechanisms in transformer models assign importance weights to different positions in a sequence, enabling the model to \"focus\" on relevant features. In genomic contexts, attention patterns reveal:\n",
    "\n",
    "- **Motif recognition**: Which nucleotide positions interact\n",
    "- **Structural dependencies**: Long-range relationships in sequences\n",
    "- **Feature importance**: What the model considers relevant for predictions\n",
    "\n",
    "### Why Attention Extraction Matters\n",
    "\n",
    "- **Model interpretability**: Understand what the model \"sees\" in genomic sequences\n",
    "- **Biological insights**: Discover sequence patterns the model associates with functions\n",
    "- **Model debugging**: Identify potential biases or unexpected attention patterns\n",
    "- **Transfer learning**: Use attention patterns to guide feature engineering\n",
    "\n",
    "### Key Feature: Universal Attention Support\n",
    "\n",
    "**ALL OmniModel types support attention extraction** through the `EmbeddingMixin` base class:\n",
    "\n",
    "| Model Type | Primary Purpose | Attention Support |\n",
    "|------------|----------------|-------------------|\n",
    "| `OmniModelForEmbedding` | Embedding extraction | ✓ |\n",
    "| `OmniModelForSequenceClassification` | Sequence-level classification | ✓ |\n",
    "| `OmniModelForSequenceRegression` | Sequence-level regression | ✓ |\n",
    "| `OmniModelForTokenClassification` | Token-level prediction | ✓ |\n",
    "| `OmniModelForMLM` | Masked language modeling | ✓ |\n",
    "\n",
    "You don't need a separate \"embedding model\" – any task-specific model provides attention extraction!\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Python**: 3.8+\n",
    "- **Environment**: CPU or CUDA-capable GPU (GPU recommended for large models)\n",
    "- **Knowledge**: Basic understanding of Python, PyTorch tensors, and genomic sequences\n",
    "- **Packages**: `omnigenbench`, `torch`, `matplotlib`, `seaborn`, `numpy`\n",
    "\n",
    "### References\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n",
    "- [BERTology: What does BERT learn?](https://arxiv.org/abs/2002.12327) - Attention analysis in language models\n",
    "- [OmniGenBench Documentation](https://github.com/yangheng95/OmniGenBench) - Framework documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b94775",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration\n",
    "\n",
    "### 1.1 Installation\n",
    "\n",
    "First, ensure OmniGenBench is installed. This may take a few minutes on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6771630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OmniGenBench if not already available\n",
    "!pip install omnigenbench -U -q\n",
    "\n",
    "# Verify installation\n",
    "import omnigenbench\n",
    "print(f\"OmniGenBench version: {omnigenbench.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62511985",
   "metadata": {},
   "source": [
    "### 1.2 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80008fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Import various model types - ALL support attention extraction through EmbeddingMixin\n",
    "from omnigenbench import (\n",
    "    OmniModelForEmbedding,\n",
    "    OmniModelForSequenceClassification,\n",
    "    OmniModelForSequenceRegression,\n",
    "    OmniTokenizer,\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"[SUCCESS] Imports successful!\")\n",
    "print(\"[INFO] All OmniModel types support attention extraction via EmbeddingMixin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df50fc",
   "metadata": {},
   "source": [
    "### 1.3 Configuration (Single Source of Truth)\n",
    "\n",
    "Define all configurable parameters in one place for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab203405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Configuration ====================\n",
    "# Single source of truth for all parameters - modify here as needed\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"yangheng/OmniGenome-186M\"  # Pre-trained genomic foundation model\n",
    "TRUST_REMOTE_CODE = True  # Required for custom model architectures\n",
    "\n",
    "# Sequence Processing\n",
    "MAX_LENGTH = 128  # Maximum sequence length for tokenization\n",
    "BATCH_SIZE = 4    # Number of sequences to process in parallel\n",
    "\n",
    "# Attention Extraction\n",
    "EXTRACT_ALL_LAYERS = None  # None = all layers, or specify list like [0, 5, 11]\n",
    "EXTRACT_ALL_HEADS = None   # None = all heads, or specify list like [0, 1, 2]\n",
    "\n",
    "# Device Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = Path(\"attention_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==================== Environment Info ====================\n",
    "print(\"[INFO] Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(\"[SUCCESS] Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef397b99",
   "metadata": {},
   "source": [
    "## 2. Model Loading\n",
    "\n",
    "### Choosing the Right Model Type\n",
    "\n",
    "The attention extraction functionality is available in **all OmniModel types** through the `EmbeddingMixin` base class. Choose based on your use case:\n",
    "\n",
    "- **`OmniModelForEmbedding`**: Use when you only need embeddings and attention (no task-specific head)\n",
    "- **`OmniModelForSequenceClassification`**: Use when you need classification + attention analysis\n",
    "- **`OmniModelForSequenceRegression`**: Use when you need regression + attention analysis\n",
    "- **`OmniModelForTokenClassification`**: Use when you need token-level predictions + attention\n",
    "\n",
    "For this tutorial, we'll use `OmniModelForEmbedding` as it's the simplest option focused on representation learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Loading model: {MODEL_NAME}\")\n",
    "print(f\"[INFO] Target device: {DEVICE}\")\n",
    "\n",
    "try:\n",
    "    # Load the embedding model\n",
    "    # OmniModelForEmbedding automatically loads both the model and tokenizer\n",
    "    model = OmniModelForEmbedding(\n",
    "        config_or_model=MODEL_NAME,\n",
    "        trust_remote_code=TRUST_REMOTE_CODE\n",
    "    )\n",
    "    \n",
    "    # Move to device and set to evaluation mode\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Verify model capabilities\n",
    "    assert hasattr(model, 'extract_attention_scores'), \\\n",
    "        \"Model must have extract_attention_scores method\"\n",
    "    assert hasattr(model, 'encode'), \\\n",
    "        \"Model must have encode method\"\n",
    "    \n",
    "    print(f\"[SUCCESS] Model loaded: {type(model).__name__}\")\n",
    "    print(f\"[INFO] Model supports:\")\n",
    "    print(\"  - Attention extraction (extract_attention_scores)\")\n",
    "    print(\"  - Embedding generation (encode, batch_encode)\")\n",
    "    print(\"  - Similarity computation (compute_similarity)\")\n",
    "    print(\"  - Attention visualization (visualize_attention_pattern)\")\n",
    "    print(\"  - Attention statistics (get_attention_statistics)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Alternative: Load task-specific models (these also support attention extraction)\n",
    "# \n",
    "# Option 2: Classification model\n",
    "# tokenizer = OmniTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=TRUST_REMOTE_CODE)\n",
    "# model = OmniModelForSequenceClassification(\n",
    "#     config_or_model=MODEL_NAME,\n",
    "#     tokenizer=tokenizer,\n",
    "#     num_labels=2,\n",
    "#     trust_remote_code=TRUST_REMOTE_CODE\n",
    "# ).to(DEVICE).eval()\n",
    "#\n",
    "# Option 3: Regression model\n",
    "# tokenizer = OmniTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=TRUST_REMOTE_CODE)\n",
    "# model = OmniModelForSequenceRegression(\n",
    "#     config_or_model=MODEL_NAME,\n",
    "#     tokenizer=tokenizer,\n",
    "#     num_labels=1,\n",
    "#     trust_remote_code=TRUST_REMOTE_CODE\n",
    "# ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fa5ca",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Sequences\n",
    "\n",
    "We'll use diverse genomic sequences to demonstrate different attention patterns:\n",
    "\n",
    "1. **Regular sequence**: Balanced nucleotide composition\n",
    "2. **GC-rich sequence**: High G+C content (common in promoters)\n",
    "3. **Repeat pattern**: Tandem repeats (microsatellites)\n",
    "4. **Long sequence**: Extended sequence to test attention across distances\n",
    "\n",
    "These sequences represent common patterns in genomics and will help illustrate how attention mechanisms capture different sequence features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sequences with different characteristics\n",
    "test_sequences = [\n",
    "    \"ATCGATCGATCGTAGCTAGCTAGCT\",     # Regular: Mixed nucleotides (25 bp)\n",
    "    \"GGCCTTAACCGGTTAACCGGTTAA\",      # GC-rich: 68% GC content (24 bp)\n",
    "    \"TTTTAAAACCCCGGGGTTTTAAAA\",      # Repeat: Simple tandem repeats (24 bp)\n",
    "    \"AUGCGAUCUCGAGCUACGUCGAUGCUAGCUCGAUGGCAUCCGAUUCGAGCUACGUCGAUGCUAG\",  # RNA: Longer sequence (64 bp)\n",
    "]\n",
    "\n",
    "# Analyze sequence properties\n",
    "print(\"[INFO] Test sequence properties:\")\n",
    "print(\"=\" * 70)\n",
    "for i, seq in enumerate(test_sequences, 1):\n",
    "    length = len(seq)\n",
    "    gc_content = (seq.count('G') + seq.count('C') + seq.count('g') + seq.count('c')) / length * 100\n",
    "    seq_type = \"RNA\" if 'U' in seq.upper() else \"DNA\"\n",
    "    \n",
    "    print(f\"Sequence {i} ({seq_type}):\")\n",
    "    print(f\"  Length: {length} bp\")\n",
    "    print(f\"  GC%: {gc_content:.1f}%\")\n",
    "    print(f\"  Preview: {seq[:40]}{'...' if length > 40 else ''}\")\n",
    "    print()\n",
    "\n",
    "print(f\"[SUCCESS] Prepared {len(test_sequences)} test sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7291392",
   "metadata": {},
   "source": [
    "## 4. Single Sequence Attention Extraction\n",
    "\n",
    "### 4.1 Extract Attention Scores\n",
    "\n",
    "The `extract_attention_scores()` method returns attention weights for all transformer layers and heads. The output format is:\n",
    "\n",
    "```\n",
    "attention_tensor: (num_layers, num_heads, seq_len, seq_len)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **num_layers**: Number of transformer layers in the model\n",
    "- **num_heads**: Number of attention heads per layer\n",
    "- **seq_len**: Tokenized sequence length (including special tokens)\n",
    "- **Attention values**: Range [0, 1], sum to 1 across the last dimension (softmax normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first sequence for detailed analysis\n",
    "sequence = test_sequences[0]\n",
    "\n",
    "print(f\"[INFO] Analyzing sequence: {sequence}\")\n",
    "print(f\"[INFO] Sequence length: {len(sequence)} bp\")\n",
    "print(\"[INFO] Extracting attention scores...\")\n",
    "\n",
    "try:\n",
    "    attention_result = model.extract_attention_scores(\n",
    "        sequence=sequence,\n",
    "        max_length=MAX_LENGTH,\n",
    "        layer_indices=EXTRACT_ALL_LAYERS,  # None = all layers\n",
    "        head_indices=EXTRACT_ALL_HEADS,     # None = all heads\n",
    "        return_on_cpu=True  # Transfer to CPU to save GPU memory\n",
    "    )\n",
    "    \n",
    "    # Validate result structure\n",
    "    assert 'attentions' in attention_result, \"Missing 'attentions' key\"\n",
    "    assert 'tokens' in attention_result, \"Missing 'tokens' key\"\n",
    "    assert 'attention_mask' in attention_result, \"Missing 'attention_mask' key\"\n",
    "    \n",
    "    attentions = attention_result['attentions']\n",
    "    tokens = attention_result['tokens']\n",
    "    attention_mask = attention_result['attention_mask']\n",
    "    \n",
    "    # Verify attention tensor properties\n",
    "    assert attentions.ndim == 4, f\"Expected 4D tensor, got {attentions.ndim}D\"\n",
    "    assert attentions.shape[2] == attentions.shape[3], \"Attention matrix must be square\"\n",
    "    \n",
    "    # Extract dimensions\n",
    "    num_layers, num_heads, seq_len, _ = attentions.shape\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    print(f\"[SUCCESS] Attention extraction complete!\")\n",
    "    print(f\"[INFO] Attention tensor shape: {attentions.shape}\")\n",
    "    print(f\"       Format: (layers={num_layers}, heads={num_heads}, seq_len={seq_len}, seq_len={seq_len})\")\n",
    "    print(f\"[INFO] Tokenized into {num_tokens} tokens\")\n",
    "    print(f\"[INFO] First 10 tokens: {tokens[:10]}\")\n",
    "    print(f\"[INFO] Attention value range: [{attentions.min():.4f}, {attentions.max():.4f}]\")\n",
    "    \n",
    "    # Verify softmax normalization (rows sum to 1)\n",
    "    row_sums = attentions[0, 0, :, :].sum(dim=-1)\n",
    "    print(f\"[INFO] Attention row sums (should be ~1.0): min={row_sums.min():.4f}, max={row_sums.max():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Attention extraction failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aedf1",
   "metadata": {},
   "source": [
    "### 4.2 Compute Attention Statistics\n",
    "\n",
    "Attention statistics help quantify attention behavior:\n",
    "\n",
    "- **Attention matrix**: Aggregated attention across layers and heads\n",
    "- **Attention entropy**: Measures how \"spread out\" attention is (higher = more uniform)\n",
    "- **Attention concentration**: Measures how \"focused\" attention is (higher = more peaked)\n",
    "- **Self-attention scores**: Diagonal values showing how much each token attends to itself\n",
    "- **Max attention per position**: Identifies which tokens receive the most attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f297324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute comprehensive attention statistics\n",
    "print(\"[INFO] Computing attention statistics...\")\n",
    "\n",
    "try:\n",
    "    stats = model.get_attention_statistics(\n",
    "        attention_result['attentions'],\n",
    "        attention_result['attention_mask'],\n",
    "        layer_aggregation=\"mean\",  # Options: mean, max, sum, first, last\n",
    "        head_aggregation=\"mean\"    # Options: mean, max, sum\n",
    "    )\n",
    "    \n",
    "    # Validate statistics\n",
    "    assert 'attention_matrix' in stats, \"Missing attention_matrix\"\n",
    "    assert 'attention_entropy' in stats, \"Missing attention_entropy\"\n",
    "    assert 'attention_concentration' in stats, \"Missing attention_concentration\"\n",
    "    assert 'self_attention_scores' in stats, \"Missing self_attention_scores\"\n",
    "    assert 'max_attention_per_position' in stats, \"Missing max_attention_per_position\"\n",
    "    \n",
    "    print(\"[SUCCESS] Statistics computed!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Attention Matrix Shape: {stats['attention_matrix'].shape}\")\n",
    "    print(f\"  Expected: (seq_len, seq_len) aggregated across layers and heads\")\n",
    "    print()\n",
    "    print(f\"Attention Entropy:\")\n",
    "    print(f\"  Mean: {stats['attention_entropy'].mean():.4f}\")\n",
    "    print(f\"  Std:  {stats['attention_entropy'].std():.4f}\")\n",
    "    print(f\"  Range: [{stats['attention_entropy'].min():.4f}, {stats['attention_entropy'].max():.4f}]\")\n",
    "    print(f\"  Interpretation: Higher entropy = attention more spread out\")\n",
    "    print()\n",
    "    print(f\"Attention Concentration:\")\n",
    "    print(f\"  Mean: {stats['attention_concentration'].mean():.4f}\")\n",
    "    print(f\"  Max:  {stats['attention_concentration'].max():.4f}\")\n",
    "    print(f\"  Interpretation: Higher concentration = attention more focused\")\n",
    "    print()\n",
    "    print(f\"Self-Attention Scores:\")\n",
    "    print(f\"  Mean: {stats['self_attention_scores'].mean():.4f}\")\n",
    "    print(f\"  Interpretation: How much each token attends to itself\")\n",
    "    print()\n",
    "    print(f\"Max Attention Per Position (first 5):\")\n",
    "    for i, val in enumerate(stats['max_attention_per_position'][:5]):\n",
    "        token = tokens[i] if i < len(tokens) else \"N/A\"\n",
    "        print(f\"  Position {i} ({token}): {val:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Statistics computation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2e7cc",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Attention Patterns\n",
    "\n",
    "Attention heatmaps visualize which positions attend to which. Key observations:\n",
    "\n",
    "- **Diagonal patterns**: Strong diagonal indicates local attention (each token attends to nearby tokens)\n",
    "- **Vertical/horizontal lines**: Specific tokens receiving/giving attention to many others\n",
    "- **Block patterns**: Related regions attending to each other (e.g., motif recognition)\n",
    "- **Special tokens**: [CLS], [SEP], [PAD] tokens often show distinct patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention pattern for a specific layer and head\n",
    "print(\"[INFO] Generating attention heatmap...\")\n",
    "\n",
    "try:\n",
    "    layer_idx = 0   # First layer (change to -1 for last layer)\n",
    "    head_idx = 0    # First attention head\n",
    "    save_path = OUTPUT_DIR / \"attention_heatmap_layer0_head0.png\"\n",
    "    \n",
    "    fig = model.visualize_attention_pattern(\n",
    "        attention_result=attention_result,\n",
    "        layer_idx=layer_idx,\n",
    "        head_idx=head_idx,\n",
    "        save_path=str(save_path),\n",
    "        figsize=(14, 12)\n",
    "    )\n",
    "    \n",
    "    if fig is not None:\n",
    "        print(f\"[SUCCESS] Attention heatmap generated!\")\n",
    "        print(f\"[INFO] Saved to: {save_path}\")\n",
    "        print(f\"[INFO] Visualizing Layer {layer_idx}, Head {head_idx}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"[WARNING] Visualization skipped (matplotlib not available or error occurred)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Visualization failed: {e}\")\n",
    "    # Continue execution even if visualization fails\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n[INFO] Heatmap Interpretation Guide:\")\n",
    "print(\"  - Bright colors = high attention weights (strong relationship)\")\n",
    "print(\"  - Dark colors = low attention weights (weak relationship)\")\n",
    "print(\"  - Diagonal = tokens attending to themselves or nearby positions\")\n",
    "print(\"  - Vertical lines = tokens that many others attend to (important positions)\")\n",
    "print(\"  - Horizontal lines = tokens that attend to many others (query-rich positions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003db7",
   "metadata": {},
   "source": [
    "## 5. Batch Attention Extraction\n",
    "\n",
    "### 5.1 Efficient Batch Processing\n",
    "\n",
    "When analyzing multiple sequences, batch processing is more efficient than individual extraction:\n",
    "\n",
    "- **Memory efficiency**: Processes sequences in batches to avoid OOM errors\n",
    "- **Computational efficiency**: Leverages GPU parallelism\n",
    "- **Layer/head selection**: Can limit extraction to specific layers/heads to reduce memory\n",
    "\n",
    "**Best Practices:**\n",
    "- Use smaller batch sizes if encountering memory issues\n",
    "- Extract only needed layers (e.g., `[0, -1]` for first and last)\n",
    "- Enable `return_on_cpu=True` to free GPU memory immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention from multiple sequences efficiently\n",
    "print(\"[INFO] Extracting attention from batch of sequences...\")\n",
    "print(f\"[INFO] Processing {len(test_sequences[:3])} sequences\")\n",
    "print(f\"[INFO] Batch size: {BATCH_SIZE // 2}\")  # Use smaller batch for demo\n",
    "\n",
    "try:\n",
    "    batch_results = model.batch_extract_attention_scores(\n",
    "        sequences=test_sequences[:3],  # First 3 sequences\n",
    "        batch_size=BATCH_SIZE // 2,    # Smaller batch size for memory efficiency\n",
    "        max_length=MAX_LENGTH,\n",
    "        layer_indices=[0, -1],  # First and last layer only (reduce memory)\n",
    "        head_indices=[0, 1, 2], # First 3 heads only (reduce memory)\n",
    "        return_on_cpu=True      # Free GPU memory immediately\n",
    "    )\n",
    "    \n",
    "    # Validate batch results\n",
    "    assert len(batch_results) == 3, f\"Expected 3 results, got {len(batch_results)}\"\n",
    "    \n",
    "    print(f\"[SUCCESS] Batch attention extraction complete!\")\n",
    "    print(f\"[INFO] Processed {len(batch_results)} sequences\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(batch_results, 1):\n",
    "        assert 'attentions' in result, f\"Result {i} missing attentions\"\n",
    "        assert 'tokens' in result, f\"Result {i} missing tokens\"\n",
    "        \n",
    "        attn_shape = result['attentions'].shape\n",
    "        num_tokens = len(result['tokens'])\n",
    "        \n",
    "        print(f\"Sequence {i}:\")\n",
    "        print(f\"  Attention shape: {attn_shape}\")\n",
    "        print(f\"  Format: (layers={attn_shape[0]}, heads={attn_shape[1]}, seq_len={attn_shape[2]}, seq_len={attn_shape[3]})\")\n",
    "        print(f\"  Number of tokens: {num_tokens}\")\n",
    "        print(f\"  First 5 tokens: {result['tokens'][:5]}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Batch extraction failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a648a",
   "metadata": {},
   "source": [
    "### 5.2 Compare Attention Patterns Across Sequences\n",
    "\n",
    "Comparing statistics across sequences helps identify:\n",
    "\n",
    "- **Sequence complexity**: More complex sequences often have higher entropy\n",
    "- **Structural patterns**: Repeat sequences may show lower entropy (more predictable)\n",
    "- **Model confidence**: High concentration suggests the model is \"confident\" about relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns between different sequences\n",
    "print(\"[INFO] Comparing attention patterns across sequences...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for i, result in enumerate(batch_results, 1):\n",
    "        stats = model.get_attention_statistics(\n",
    "            result['attentions'],\n",
    "            result['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Extract key metrics\n",
    "        entropy_mean = stats['attention_entropy'].mean().item()\n",
    "        concentration_mean = stats['attention_concentration'].mean().item()\n",
    "        self_attn_mean = stats['self_attention_scores'].mean().item()\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'seq_idx': i,\n",
    "            'entropy': entropy_mean,\n",
    "            'concentration': concentration_mean,\n",
    "            'self_attention': self_attn_mean\n",
    "        })\n",
    "        \n",
    "        seq_preview = test_sequences[i-1][:30] + (\"...\" if len(test_sequences[i-1]) > 30 else \"\")\n",
    "        print(f\"Sequence {i}: {seq_preview}\")\n",
    "        print(f\"  Attention entropy:       {entropy_mean:.4f}\")\n",
    "        print(f\"  Attention concentration: {concentration_mean:.4f}\")\n",
    "        print(f\"  Self-attention:          {self_attn_mean:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(\"[INFO] Summary:\")\n",
    "    entropy_vals = [d['entropy'] for d in comparison_data]\n",
    "    print(f\"  Entropy range: [{min(entropy_vals):.4f}, {max(entropy_vals):.4f}]\")\n",
    "    print(f\"  Highest entropy: Sequence {entropy_vals.index(max(entropy_vals)) + 1} (more dispersed attention)\")\n",
    "    print(f\"  Lowest entropy:  Sequence {entropy_vals.index(min(entropy_vals)) + 1} (more focused attention)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Comparison failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b81ae",
   "metadata": {},
   "source": [
    "## 6. Combined Attention and Embedding Extraction\n",
    "\n",
    "### Unified Interface for Multi-Modal Analysis\n",
    "\n",
    "Since all OmniModel types inherit from `EmbeddingMixin`, you can extract **both** attention and embeddings from the same model:\n",
    "\n",
    "- **Attention**: Shows *how* the model processes sequences (relationships between positions)\n",
    "- **Embeddings**: Shows *what* the model learned (fixed-length representations)\n",
    "\n",
    "This is useful for:\n",
    "- **Visualization**: Combine attention with embedding-based clustering/UMAP\n",
    "- **Transfer learning**: Use both attention patterns and embeddings as features\n",
    "- **Model debugging**: Compare attention behavior with embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80add529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from the same model used for attention\n",
    "print(\"[INFO] Extracting embeddings from the same model...\")\n",
    "\n",
    "try:\n",
    "    # Single sequence embedding\n",
    "    single_embedding = model.encode(\n",
    "        test_sequences[0],\n",
    "        max_length=MAX_LENGTH,\n",
    "        agg=\"mean\"  # Options: mean, head, tail\n",
    "    )\n",
    "    print(f\"[SUCCESS] Single sequence embedding shape: {single_embedding.shape}\")\n",
    "    \n",
    "    # Batch embedding extraction\n",
    "    batch_embeddings = model.batch_encode(\n",
    "        test_sequences[:3],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=MAX_LENGTH,\n",
    "        agg=\"mean\"\n",
    "    )\n",
    "    print(f\"[SUCCESS] Batch embeddings shape: {batch_embeddings.shape}\")\n",
    "    print(f\"           Format: (num_sequences={batch_embeddings.shape[0]}, embedding_dim={batch_embeddings.shape[1]})\")\n",
    "    \n",
    "    # Compute pairwise similarity\n",
    "    print(\"\\n[INFO] Computing pairwise sequence similarities:\")\n",
    "    print(\"=\" * 70)\n",
    "    for i in range(len(batch_embeddings)):\n",
    "        for j in range(i + 1, len(batch_embeddings)):\n",
    "            similarity = model.compute_similarity(\n",
    "                batch_embeddings[i],\n",
    "                batch_embeddings[j]\n",
    "            )\n",
    "            print(f\"  Sequence {i+1} vs Sequence {j+1}: {similarity:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] Both attention and embeddings extracted from the same model!\")\n",
    "    print(\"[INFO] This unified interface works with ALL OmniModel types:\")\n",
    "    print(\"       - OmniModelForEmbedding\")\n",
    "    print(\"       - OmniModelForSequenceClassification\")\n",
    "    print(\"       - OmniModelForSequenceRegression\")\n",
    "    print(\"       - OmniModelForTokenClassification\")\n",
    "    print(\"       - And all other OmniModel variants!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Embedding extraction failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f8011",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics and Best Practices\n",
    "\n",
    "### 7.1 Memory Management\n",
    "\n",
    "**GPU Memory Considerations:**\n",
    "- Each attention tensor can be large: `(layers × heads × seq_len × seq_len) × 4 bytes`\n",
    "- Example: 12 layers, 12 heads, 512 seq_len = ~150 MB per sequence\n",
    "- Use `return_on_cpu=True` to immediately transfer results to CPU\n",
    "- Extract specific layers/heads to reduce memory usage\n",
    "- Reduce batch size if encountering OOM errors\n",
    "\n",
    "**CPU Memory Considerations:**\n",
    "- Batch processing on CPU is slower but uses less memory\n",
    "- Consider processing sequences sequentially for very large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate memory-efficient extraction strategies\n",
    "\n",
    "print(\"[INFO] Memory Management Strategies:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Strategy 1: Extract only specific layers\n",
    "print(\"Strategy 1: Extract specific layers only\")\n",
    "specific_layers = [0, -1]  # First and last layer\n",
    "result_specific = model.extract_attention_scores(\n",
    "    sequence=test_sequences[0],\n",
    "    max_length=MAX_LENGTH,\n",
    "    layer_indices=specific_layers,\n",
    "    return_on_cpu=True\n",
    ")\n",
    "print(f\"  Full extraction would be: (all_layers, all_heads, seq_len, seq_len)\")\n",
    "print(f\"  Optimized extraction: {result_specific['attentions'].shape}\")\n",
    "print(f\"  Memory saved: {((1 - len(specific_layers)/num_layers) * 100):.1f}%\")\n",
    "print()\n",
    "\n",
    "# Strategy 2: Extract specific heads\n",
    "print(\"Strategy 2: Extract specific heads only\")\n",
    "specific_heads = [0, 1]  # First 2 heads\n",
    "result_heads = model.extract_attention_scores(\n",
    "    sequence=test_sequences[0],\n",
    "    max_length=MAX_LENGTH,\n",
    "    head_indices=specific_heads,\n",
    "    return_on_cpu=True\n",
    ")\n",
    "print(f\"  Optimized extraction: {result_heads['attentions'].shape}\")\n",
    "print(f\"  Memory saved: {((1 - len(specific_heads)/num_heads) * 100):.1f}%\")\n",
    "print()\n",
    "\n",
    "# Strategy 3: Return on CPU\n",
    "print(\"Strategy 3: Use return_on_cpu=True\")\n",
    "print(\"  [INFO] Immediately transfers results to CPU, freeing GPU memory\")\n",
    "print(\"  [INFO] Essential for processing large batches\")\n",
    "print()\n",
    "\n",
    "print(\"[SUCCESS] Memory management strategies demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db78841",
   "metadata": {},
   "source": [
    "### 7.2 Limitations and Boundary Conditions\n",
    "\n",
    "**Sequence Length Constraints:**\n",
    "- **max_length parameter**: Must match model's maximum context window\n",
    "- OmniGenome-186M supports up to 1024 tokens (check model card for specifics)\n",
    "- Sequences exceeding max_length are **truncated**, not split\n",
    "- Attention to truncated regions is lost\n",
    "\n",
    "**Attention Extraction Limitations:**\n",
    "- **Not all models support attention extraction**: Model must output attention weights\n",
    "- **Computational cost**: Scales as O(L × H × N²) where L=layers, H=heads, N=sequence length\n",
    "- **Memory requirements**: Full attention matrices can be very large\n",
    "- **Interpretation challenges**: Attention ≠ importance (see [Attention is not Explanation](https://arxiv.org/abs/1902.10186))\n",
    "\n",
    "**When Attention Analysis is Useful:**\n",
    "- Debugging unexpected model predictions\n",
    "- Understanding learned sequence motifs\n",
    "- Comparing attention patterns across model variants\n",
    "- Identifying position-specific biases\n",
    "\n",
    "**When Attention Analysis May NOT Be Useful:**\n",
    "- Directly determining feature importance (use gradient-based methods instead)\n",
    "- Explaining black-box predictions (attention is one of many factors)\n",
    "- Sequences with extreme length (computational constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687668",
   "metadata": {},
   "source": [
    "### 7.3 Aggregation Strategies\n",
    "\n",
    "Different aggregation methods provide different insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970624df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different aggregation strategies\n",
    "print(\"[INFO] Comparing aggregation strategies:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aggregation_strategies = [\n",
    "    (\"mean\", \"mean\"),  # Average across layers and heads\n",
    "    (\"first\", \"mean\"), # First layer, average heads\n",
    "    (\"last\", \"mean\"),  # Last layer, average heads\n",
    "    (\"max\", \"max\"),    # Maximum across layers and heads\n",
    "]\n",
    "\n",
    "for layer_agg, head_agg in aggregation_strategies:\n",
    "    try:\n",
    "        stats_agg = model.get_attention_statistics(\n",
    "            attention_result['attentions'],\n",
    "            attention_result['attention_mask'],\n",
    "            layer_aggregation=layer_agg,\n",
    "            head_aggregation=head_agg\n",
    "        )\n",
    "        \n",
    "        entropy = stats_agg['attention_entropy'].mean().item()\n",
    "        concentration = stats_agg['attention_concentration'].mean().item()\n",
    "        \n",
    "        print(f\"Strategy: layer={layer_agg:5s}, head={head_agg:4s}\")\n",
    "        print(f\"  Entropy: {entropy:.4f}, Concentration: {concentration:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Strategy: layer={layer_agg}, head={head_agg} - Error: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"[INFO] Interpretation:\")\n",
    "print(\"  - 'mean' aggregation: Smoother, averaged patterns across layers/heads\")\n",
    "print(\"  - 'first' layer: Early representations (closer to input)\")\n",
    "print(\"  - 'last' layer: Late representations (task-specific features)\")\n",
    "print(\"  - 'max' aggregation: Emphasizes strongest attention signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9163e4e",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### 8.1 What We Learned\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "**Core Skills:**\n",
    "- [x] Extract attention scores from genomic sequences\n",
    "- [x] Compute attention statistics (entropy, concentration, self-attention)\n",
    "- [x] Visualize attention patterns as heatmaps\n",
    "- [x] Process multiple sequences in batches efficiently\n",
    "- [x] Compare attention patterns across different sequences\n",
    "- [x] Extract both attention and embeddings from the same model\n",
    "\n",
    "**Key Concepts:**\n",
    "- [x] Attention mechanisms in transformer models for genomics\n",
    "- [x] Universal attention support across all OmniModel types\n",
    "- [x] Memory management strategies for large-scale analysis\n",
    "- [x] Aggregation strategies (layer-wise and head-wise)\n",
    "- [x] Limitations and appropriate use cases\n",
    "\n",
    "### 8.2 API Methods Reference\n",
    "\n",
    "All methods are available through `EmbeddingMixin` (inherited by all OmniModel types):\n",
    "\n",
    "| Method | Purpose | Returns |\n",
    "|--------|---------|---------|\n",
    "| `extract_attention_scores()` | Single sequence attention | Dict with 'attentions', 'tokens', 'attention_mask' |\n",
    "| `batch_extract_attention_scores()` | Multiple sequences attention | List of dicts |\n",
    "| `get_attention_statistics()` | Compute attention metrics | Dict with statistical measures |\n",
    "| `visualize_attention_pattern()` | Create attention heatmap | matplotlib Figure |\n",
    "| `encode()` | Single sequence embedding | Tensor (embedding_dim,) |\n",
    "| `batch_encode()` | Multiple sequences embeddings | Tensor (num_seqs, embedding_dim) |\n",
    "| `compute_similarity()` | Embedding similarity | Float (cosine similarity) |\n",
    "\n",
    "### 8.3 Supported Model Types\n",
    "\n",
    "**All these models support attention extraction:**\n",
    "\n",
    "| Model Type | Primary Purpose | Task Head |\n",
    "|------------|----------------|-----------|\n",
    "| `OmniModelForEmbedding` | Representation learning | None |\n",
    "| `OmniModelForSequenceClassification` | Sequence-level classification | Linear classifier |\n",
    "| `OmniModelForSequenceRegression` | Sequence-level regression | Linear regressor |\n",
    "| `OmniModelForTokenClassification` | Token-level prediction | Token classifier |\n",
    "| `OmniModelForMLM` | Masked language modeling | MLM head |\n",
    "\n",
    "### 8.4 Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "| Issue | Possible Cause | Solution |\n",
    "|-------|---------------|----------|\n",
    "| `CUDA out of memory` | Sequence too long / batch too large | Reduce batch_size, extract specific layers/heads, use return_on_cpu=True |\n",
    "| `AttributeError: 'Model' object has no attribute 'extract_attention_scores'` | Old OmniGenBench version | Update: `pip install omnigenbench -U` |\n",
    "| `Attention values not summing to 1` | Using wrong aggregation | Check layer/head aggregation, use raw attention from specific head |\n",
    "| `Heatmap not showing` | matplotlib not available | Install: `pip install matplotlib seaborn` |\n",
    "| `Sequence truncated` | max_length too small | Increase MAX_LENGTH (check model's max context window) |\n",
    "\n",
    "### 8.5 Next Steps\n",
    "\n",
    "**Explore More:**\n",
    "\n",
    "1. **Fine-tune on your data**: Train task-specific models and analyze attention changes\n",
    "2. **Compare models**: Extract attention from different GFMs and compare patterns\n",
    "3. **Biological validation**: Correlate attention patterns with known biological motifs\n",
    "4. **Advanced visualization**: Create attention flow diagrams across layers\n",
    "5. **Integration**: Combine attention with other interpretability methods (SHAP, integrated gradients)\n",
    "\n",
    "**Related Tutorials:**\n",
    "- [Genomic Embeddings Tutorial](../genomic_embeddings/) - Deep dive into embedding extraction\n",
    "- [RNA Secondary Structure Prediction](../rna_secondary_structure_prediction/) - Task-specific attention analysis\n",
    "- [Variant Effect Prediction](../variant_effect_prediction/) - Attention for variant analysis\n",
    "\n",
    "**Further Reading:**\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper\n",
    "- [Attention is not Explanation](https://arxiv.org/abs/1902.10186) - Critical perspective on attention\n",
    "- [Analyzing Attention in Genomics](https://www.nature.com/articles/s41592-021-01252-x) - Domain-specific analysis\n",
    "\n",
    "### 8.6 Citation\n",
    "\n",
    "If you use OmniGenBench in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@software{omnigenbench2025,\n",
    "  author = {Yang, Heng},\n",
    "  title = {OmniGenBench: A Unified Framework for Genomic Foundation Models},\n",
    "  year = {2025},\n",
    "  publisher = {GitHub},\n",
    "  url = {https://github.com/yangheng95/OmniGenBench}\n",
    "}\n",
    "```\n",
    "\n",
    "### 8.7 Feedback and Contributions\n",
    "\n",
    "Found an issue or have suggestions? Please:\n",
    "- Open an issue on [GitHub](https://github.com/yangheng95/OmniGenBench/issues)\n",
    "- Submit a pull request with improvements\n",
    "- Join discussions in the community forum\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial!** You now have the tools to analyze attention patterns in genomic foundation models. Happy exploring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
