{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9118cec958f69f2f",
   "metadata": {},
   "source": [
    "# ðŸ§¬ mRNA Degradation Rate Prediction with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to predict **mRNA degradation rates** from RNA sequences using **OmniGenBench**. This guide will walk you through a complete genomic deep learning project, from understanding the fundamental biological concepts to deploying a trained model for real-world applications.\n",
    "\n",
    "### 1. The Biological Challenge: What is mRNA Degradation?\n",
    "\n",
    "**mRNA degradation** is a crucial regulatory mechanism in gene expression that controls the stability and lifespan of mRNA molecules in cells. The rate at which mRNA degrades directly impacts:\n",
    "\n",
    "- **Protein production levels**: Stable mRNAs produce more proteins over time\n",
    "- **Gene expression dynamics**: Rapid degradation enables quick response to cellular signals  \n",
    "- **Cellular homeostasis**: Proper mRNA turnover maintains balanced protein levels\n",
    "- **Disease mechanisms**: Dysregulated mRNA stability contributes to various disorders\n",
    "\n",
    "Understanding and predicting mRNA degradation rates has profound implications across multiple domains:\n",
    "- **Therapeutic Design**: Engineering mRNA-based therapeutics (like COVID-19 vaccines) with optimal stability\n",
    "- **Synthetic Biology**: Designing gene circuits with precise temporal control\n",
    "- **Disease Research**: Understanding how mutations affect mRNA stability in genetic disorders\n",
    "- **Biotechnology**: Optimizing protein production in industrial applications\n",
    "\n",
    "However, experimentally measuring degradation rates across thousands of mRNA sequences is time-consuming and costly. This is where computational methods, particularly deep learning with Genomic Foundation Models, provide transformative solutions.\n",
    "\n",
    "### 2. The Data: mRNA Degradation Dataset\n",
    "\n",
    "To train our predictive model, we utilize a carefully curated dataset containing mRNA sequences with experimentally determined degradation parameters.\n",
    "\n",
    "- **What it contains**: mRNA sequences with multiple degradation rate measurements under different conditions\n",
    "- **What it labels**: Each sequence has three continuous values representing degradation rates:\n",
    "  - `reactivity`: General degradation reactivity\n",
    "  - `deg_Mg_pH10`: Degradation under Mg2+ and pH 10 conditions  \n",
    "  - `deg_Mg_50C`: Degradation under Mg2+ and 50Â°C conditions\n",
    "- **Our Goal**: Train a model that can accurately predict degradation rates for each nucleotide position in mRNA sequences\n",
    "\n",
    "**Dataset Structure:**\n",
    "\n",
    "| sequence | reactivity | deg_Mg_pH10 | deg_Mg_50C |\n",
    "|---------|------------|-------------|------------|\n",
    "| AUGCCAU... | [0.1, 0.2, ...] | [0.15, 0.25, ...] | [0.3, 0.4, ...] |\n",
    "| AUGCUA... | [0.05, 0.1, ...] | [0.2, 0.3, ...] | [0.25, 0.35, ...] |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "### 3. The Tool: From Language Models to Genomic Foundation Models\n",
    "\n",
    "#### The Rise of Language Models\n",
    "In recent years, **Language Models (LMs)** like BERT have revolutionized Natural Language Processing (NLP). Trained on vast amounts of text, they learn the underlying patterns of languageâ€”grammar, context, and even semantics. This allows them to be \"fine-tuned\" for a wide range of specific tasks.\n",
    "\n",
    "#### A New Paradigm in Genomics: Genomic Foundation Models (GFMs)\n",
    "The same principles can be applied to biology. The \"language of life\" is written in RNA and DNA sequences using nucleotides (A, C, G, U/T). **Genomic Foundation Models (GFMs)**, like **OmniGenome** (Yang et al., 2025), are large-scale models pre-trained on massive amounts of genomic sequences.\n",
    "\n",
    "In this tutorial, we will follow a standard 4-step fine-tuning pipeline for **token-level regression**, where we predict continuous values for each nucleotide position in the sequence.\n",
    "\n",
    "### 4. The Workflow: A 4-Step Guide to Fine-Tuning\n",
    "\n",
    "We will follow a standard 4-step fine-tuning pipeline, which is a common practice in machine learning.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for mRNA Degradation Prediction\"\n",
    "        A[\"ðŸ“¥ Step 1: Data Preparation<br/>Download and process the mRNA degradation dataset\"] --> B[\"ðŸ”§ Step 2: Model Initialization<br/>Load the pre-trained OmniGenome model\"]\n",
    "        B --> C[\"ðŸŽ“ Step 3: Model Training<br/>Fine-tune the model on the degradation dataset\"]\n",
    "        C --> D[\"ðŸ”® Step 4: Model Inference<br/>Use the trained model to predict degradation rates\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "In a nutshell, we will see the following steps:\n",
    "1. **Data Preparation**: Download and preprocess the mRNA degradation dataset\n",
    "2. **Model Initialization**: Load the pre-trained OmniGenome model and set it up for token regression\n",
    "3. **Training Implementation**: Fine-tune the model using our dataset and validate its performance\n",
    "4. **Inference: Make Predictions**: Use the trained model to predict degradation rates on new sequences\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e0efa47756ae3",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Data Preparation\n",
    "\n",
    "This first step is all about getting our data ready for analysis. It involves four key parts:\n",
    "1. **Environment Setup**: Installing and importing the necessary libraries\n",
    "2. **Configuration**: Defining all our important parameters in one place\n",
    "3. **Data Acquisition**: Loading and preparing the mRNA degradation dataset\n",
    "4. **Data Pipeline**: Creating an efficient pipeline to feed data to the model\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library that provides state-of-the-art genomic foundation models and training utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f9343bc0350cc",
   "metadata": {},
   "source": [
    "!pip install omnigenbench -U  # Install the latest version of omnigenbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12cd82625ee12b",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and token-level regression modeling.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce7dd5e0999f65",
   "metadata": {},
   "source": [
    "### 1.2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4e37aa5ceefe10e",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from omnigenbench import (\n",
    "    RegressionMetric,\n",
    "    AccelerateTrainer,\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForTokenRegression,\n",
    "    OmniModelForTokenRegression,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18039cd8bdd9824b",
   "metadata": {},
   "source": [
    "### 1.3: Global Configuration\n",
    "\n",
    "To make our tutorial easy to modify and understand, we'll centralize all important parameters in this section. This is a best practice in software development that makes experiments more reproducible.\n",
    "\n",
    "#### Key Parameters\n",
    "- **Dataset**: We define the dataset name for automatic downloading from our curated collection\n",
    "- **Model**: We select which pre-trained OmniGenome model to use. For this tutorial, we'll use `OmniGenome-52M` because it's efficient and perfect for learning\n",
    "\n",
    "This centralized approach allows you to easily experiment with different settings without hunting through the code."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb3410dd88458683",
   "metadata": {},
   "source": [
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"mrna_degradation\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc3f458cbdeac083",
   "metadata": {},
   "source": [
    "### 1.4: Data Acquisition and Loading\n",
    "\n",
    "With our environment configured, it's time to load the mRNA degradation dataset. The enhanced OmniDataset framework automates this process by:\n",
    "1. **Automatic downloading** from our curated dataset collection\n",
    "2. **Processing sequences** with proper tokenization for token-level regression\n",
    "3. **Handling multi-target labels** for the three degradation measurements\n",
    "4. **Creating efficient data pipelines** ready for training\n",
    "\n",
    "This ensures we have properly formatted data with train/validation/test splits ready for the next stage."
   ]
  },
  {
   "cell_type": "code",
   "id": "601a6067622c6f6",
   "metadata": {},
   "source": [
    "# Model and Tokenizer initialization\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "print(f\"âœ… Tokenizer loaded: {model_name_or_path}\")\n",
    "\n",
    "class Dataset(OmniDatasetForTokenRegression):\n",
    "    def __init__(self, dataset_name_or_path, tokenizer, max_length, **kwargs):\n",
    "        super().__init__(dataset_name_or_path, tokenizer, max_length, **kwargs)\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "        instance[\"sequence\"] = f'{instance[\"sequence\"]}'\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            instance[\"sequence\"],\n",
    "            padding=kwargs.get(\"padding\", \"do_not_pad\"),\n",
    "            truncation=kwargs.get(\"truncation\", True),\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = [instance[target_col] for target_col in target_cols]\n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                np.array(labels),\n",
    "                np.array(\n",
    "                    [\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).T\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Load datasets using the enhanced OmniDataset framework for token regression\n",
    "print(\"ðŸ—ï¸ Loading datasets with automatic download...\")\n",
    "datasets = Dataset.from_hub(\n",
    "    dataset_name_or_path=\"RNA-mRNA\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128,\n",
    "    target_columns=[\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"],  # Three regression targets\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67718155cb3bde05",
   "metadata": {},
   "source": [
    "### 1.5: Dataset Loading with OmniGenBench\n",
    "\n",
    "With OmniGenBench, data loading for token regression is significantly simplified! The framework automatically handles:\n",
    "\n",
    "#### A. Automatic Data Processing\n",
    "The `OmniDatasetForTokenRegression` class automatically:\n",
    "1. **Downloads and processes** the dataset from our curated collection\n",
    "2. **Handles sequence preprocessing** including tokenization and proper padding\n",
    "3. **Manages multi-target regression formatting** for position-wise degradation prediction  \n",
    "4. **Creates train/validation/test splits** ready for training\n",
    "\n",
    "#### B. Built-in Optimizations\n",
    "The framework includes several optimizations:\n",
    "1. **Efficient batching** for variable-length sequences\n",
    "2. **Memory management** for large genomic datasets\n",
    "3. **Automatic label alignment** with tokenized sequences\n",
    "4. **Proper masking** for padded positions using -100 labels\n",
    "\n",
    "This streamlined approach eliminates the need for complex custom dataset classes while maintaining full flexibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "77bb3c26423dadcb",
   "metadata": {},
   "source": [
    "print(\"ðŸ“ Data loading completed! Using modern OmniDataset framework.\")\n",
    "print(f\"ðŸ“Š Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")\n",
    "    \n",
    "# Inspect a sample to understand the data structure\n",
    "if len(datasets[\"train\"]) > 0:\n",
    "    sample = datasets[\"train\"][0]\n",
    "    print(f\"\\nðŸ” Sample data structure:\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31240209d1705e0c",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 2: Model Initialization\n",
    "\n",
    "With our data pipeline in place, it's time to set up the model. This is where the power of Genomic Foundation Models (GFMs) comes into play. Instead of building a model from scratch, we will load the pre-trained **OmniGenome** model and adapt it for token-level regression.\n",
    "\n",
    "This process involves three key components:\n",
    "1. **The Tokenizer**: We use the same tokenizer from data preparation that converts RNA sequences into numerical format\n",
    "2. **The Base Model**: The core OmniGenome model that has learned fundamental genomic patterns from pretraining\n",
    "3. **The Regression Head**: A neural network layer that maps sequence representations to continuous degradation values for each token position\n",
    "\n",
    "The `OmniModelForTokenRegression` class handles this seamlessly, combining the base model with the appropriate regression head for our multi-target prediction task."
   ]
  },
  {
   "cell_type": "code",
   "id": "8f70cba9af5b9ca1",
   "metadata": {},
   "source": [
    "# === Model Initialization ===\n",
    "# We support all genomic foundation models from Hugging Face Hub.\n",
    "\n",
    "model = OmniModelForTokenRegression(\n",
    "    model_name_or_path,\n",
    "    tokenizer,\n",
    "    num_labels=3,  # Three regression targets: reactivity, deg_Mg_pH10, deg_Mg_50C\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {model_name_or_path}\")\n",
    "print(f\"ðŸ“Š Model configuration:\")\n",
    "print(f\"  - Architecture: Token-level regression\")\n",
    "print(f\"  - Number of targets: 3 (reactivity, deg_Mg_pH10, deg_Mg_50C)\")\n",
    "print(f\"  - Max sequence length: 128\")\n",
    "print(f\"  - Model parameters: ~52M\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ced577fd79468ea0",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 3: Model Training\n",
    "\n",
    "This is the most exciting part! With our data and model ready, we can now begin the **fine-tuning** process. During training, the model will learn to associate specific patterns in RNA sequences with degradation rates at each nucleotide position.\n",
    "\n",
    "### Our Training Strategy\n",
    "\n",
    "We use a sophisticated strategy to ensure the best possible outcome:\n",
    "\n",
    "1. **Evaluation Metrics**: For token-level regression tasks, we use:\n",
    "   - **Root Mean Squared Error (RMSE)**: Measures average prediction error magnitude\n",
    "   - **RÂ² Score**: Indicates how well the model explains variance in degradation rates\n",
    "   - **OmniGenBench supports 60+ ML metrics** and customized metrics for different tasks\n",
    "\n",
    "2. **Advanced Training Features**:\n",
    "   - **Automatic mixed precision** for faster training and memory efficiency\n",
    "   - **Gradient accumulation** for effective large batch training\n",
    "   - **Learning rate scheduling** with warmup for stable convergence\n",
    "   - **Early stopping** based on validation performance to prevent overfitting\n",
    "\n",
    "The `AccelerateTrainer` from `omnigenbench` wraps all this logic into a simple interface, leveraging Hugging Face Accelerate for distributed training support."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b3009d3a4a082fe",
   "metadata": {},
   "source": [
    "# Define evaluation metrics for token-level regression\n",
    "metric_functions = [\n",
    "    RegressionMetric(ignore_y=-100).root_mean_squared_error,\n",
    "    RegressionMetric(ignore_y=-100).r2_score,\n",
    "]\n",
    "\n",
    "# Initialize the modern AccelerateTrainer\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"], \n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ“ Starting training...\")\n",
    "print(\"âš¡ Using AccelerateTrainer with automatic optimizations:\")\n",
    "print(\"  - Mixed precision training for speed and memory efficiency\")\n",
    "print(\"  - Automatic gradient accumulation\")\n",
    "print(\"  - Learning rate scheduling with warmup\")\n",
    "print(\"  - Early stopping based on validation metrics\")\n",
    "\n",
    "# Train the model\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_mrna_degradation_finetuned\")\n",
    "\n",
    "print(\"âœ… Training completed!\")\n",
    "print(\"ðŸ“Š Final metrics:\")\n",
    "print(metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6ec39ba9f645f8b",
   "metadata": {},
   "source": [
    "## ðŸ”® Step 4: Model Inference and Interpretation\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting mRNA degradation rates on new RNA sequences. This process is called **inference**.\n",
    "\n",
    "### The Inference Pipeline\n",
    "\n",
    "Our inference pipeline consists of several key steps:\n",
    "1. **Load the Model**: We load the best-performing model saved during training using ModelHub\n",
    "2. **Process Input**: We take new RNA sequences and apply the same preprocessing steps\n",
    "3. **Run Prediction**: We feed the processed sequence to the model and get degradation predictions for each nucleotide\n",
    "4. **Interpret Results**: We analyze the position-wise degradation rates and identify key patterns\n",
    "\n",
    "To demonstrate, we'll test our model on sample sequences with different characteristics and analyze the predicted degradation patterns."
   ]
  },
  {
   "cell_type": "code",
   "id": "724ecda60e901a6a",
   "metadata": {},
   "source": [
    "# Load the fine-tuned model for inference\n",
    "inference_model = ModelHub.load(\"ogb_mrna_degradation_finetuned\")\n",
    "\n",
    "# Test sequences with different characteristics\n",
    "sample_sequences = {\n",
    "    \"Structured RNA\": \"AUGCCGUGCUAAUCGCGGUAGCGCUAGGCUGCAUCGCGGUAGCGCUAGGCUGCAU\",\n",
    "    \"AU-rich sequence\": \"AUGUAUAUAUGUAUAUGUAUAUGUAUAUGUAUAUGUAUAUGUAUAUGUAUAU\",\n",
    "    \"GC-rich sequence\": \"AUGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC\",\n",
    "    \"Random sequence\": \"AUGCAGUCCGAUUCGAGCUACGUCGAUGCUAGCUCGAUGGCAUCCGAUUCGAG\",\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"ðŸ”® Running inference on sample sequences...\\n\")\n",
    "    \n",
    "    for seq_name, sequence in sample_sequences.items():\n",
    "        print(f\"ðŸ“Š Analysis for {seq_name}:\")\n",
    "        print(f\"  ðŸ“ Sequence: {sequence[:50]}{'...' if len(sequence) > 50 else ''}\")\n",
    "        print(f\"  ðŸ“ Length: {len(sequence)} nucleotides\")\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = inference_model.inference(sequence)\n",
    "        predictions = outputs.get('predictions', None)\n",
    "        \n",
    "        if predictions is not None:\n",
    "            predictions = np.array(predictions)\n",
    "            print(f\"  ðŸŽ¯ Prediction shape: {predictions.shape}\")\n",
    "            \n",
    "            # Analyze each degradation target\n",
    "            target_names = [\"Reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "            for i, target_name in enumerate(target_names):\n",
    "                target_values = predictions[:, i] if len(predictions.shape) > 1 else predictions\n",
    "                valid_predictions = target_values[target_values != -100]  # Remove padding\n",
    "                \n",
    "                if len(valid_predictions) > 0:\n",
    "                    mean_val = np.mean(valid_predictions)\n",
    "                    std_val = np.std(valid_predictions) \n",
    "                    max_val = np.max(valid_predictions)\n",
    "                    min_val = np.min(valid_predictions)\n",
    "                    \n",
    "                    print(f\"  ðŸ“ˆ {target_name}:\")\n",
    "                    print(f\"    Mean: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "                    print(f\"    Range: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "                    \n",
    "                    # Interpretation based on degradation levels\n",
    "                    if mean_val > 0.3:\n",
    "                        stability = \"ðŸ”´ High degradation (unstable)\"\n",
    "                    elif mean_val > 0.15:\n",
    "                        stability = \"ðŸŸ¡ Moderate degradation\"\n",
    "                    else:\n",
    "                        stability = \"ðŸŸ¢ Low degradation (stable)\"\n",
    "                    print(f\"    {stability}\")\n",
    "        \n",
    "        print(\"â”€\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d74afeac7d05d563",
   "metadata": {},
   "source": [
    "### Advanced Analysis: Position-wise Degradation Patterns\n",
    "\n",
    "Let's perform a more detailed analysis to understand how degradation varies along the sequence length and identify potential structural motifs that influence stability."
   ]
  },
  {
   "cell_type": "code",
   "id": "aff30ee45d4b58ce",
   "metadata": {},
   "source": [
    "# Advanced analysis: Position-wise degradation pattern analysis\n",
    "test_sequence = \"AUGCCGUGCUAAUCGCGGUAGCGCUAGGCUGCAUCGCGGUAGCGCUAGGCUGCAU\"\n",
    "\n",
    "print(\"ðŸ”¬ Advanced Analysis: Position-wise Degradation Patterns\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analyzing sequence: {test_sequence}\")\n",
    "print(f\"Length: {len(test_sequence)} nucleotides\\n\")\n",
    "\n",
    "# Get detailed predictions\n",
    "outputs = inference_model.inference(test_sequence)\n",
    "predictions = outputs.get('predictions', None)\n",
    "\n",
    "if predictions is not None:\n",
    "    predictions = np.array(predictions)\n",
    "    target_names = [\"Reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "    \n",
    "    print(\"ðŸ“Š Position-wise Analysis:\")\n",
    "    print(\"Pos\\tNuc\\tReactivity\\tdeg_Mg_pH10\\tdeg_Mg_50C\\tStability\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for pos in range(min(20, len(test_sequence))):  # Show first 20 positions\n",
    "        nucleotide = test_sequence[pos]\n",
    "        \n",
    "        if len(predictions.shape) > 1 and pos < predictions.shape[0]:\n",
    "            reactivity = predictions[pos, 0]\n",
    "            deg_ph10 = predictions[pos, 1] \n",
    "            deg_50c = predictions[pos, 2]\n",
    "            \n",
    "            # Skip padded positions\n",
    "            if reactivity == -100:\n",
    "                continue\n",
    "                \n",
    "            # Determine stability based on average degradation\n",
    "            avg_deg = (reactivity + deg_ph10 + deg_50c) / 3\n",
    "            if avg_deg > 0.25:\n",
    "                stability = \"Unstable\"\n",
    "            elif avg_deg > 0.15:\n",
    "                stability = \"Moderate\"\n",
    "            else:\n",
    "                stability = \"Stable\"\n",
    "                \n",
    "            print(f\"{pos+1:2d}\\t{nucleotide}\\t{reactivity:.4f}\\t\\t{deg_ph10:.4f}\\t\\t{deg_50c:.4f}\\t\\t{stability}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Summary Insights:\")\n",
    "    print(\"â€¢ Positions with high degradation may indicate structural vulnerability\")\n",
    "    print(\"â€¢ GC-rich regions often show different degradation patterns than AU-rich regions\")\n",
    "    print(\"â€¢ The model captures position-specific degradation propensities\")\n",
    "    print(\"â€¢ These predictions can guide RNA engineering for stability optimization\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Tutorial completed successfully!\")\n",
    "print(\"ðŸš€ Your model is ready for:\")\n",
    "print(\"  - Predicting mRNA stability in therapeutic design\")\n",
    "print(\"  - Optimizing RNA sequences for biotechnology applications\")\n",
    "print(\"  - Understanding sequence-structure-stability relationships\")\n",
    "print(\"  - Advancing synthetic biology and gene therapy research\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae818ac2",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on mRNA degradation rate prediction with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end MLOps workflow for token-level regression, a critical skill in computational biology. Specifically, you have:\n",
    "\n",
    "1. **Understood the \"Why\"**: Gained appreciation for the biological problem of mRNA stability and how Genomic Foundation Models provide powerful solutions for therapeutic and biotechnology applications.\n",
    "\n",
    "2. **Mastered the 4-Step Workflow**:\n",
    "   - **Step 1: Data Preparation**: You learned how to acquire, process, and efficiently load genomic datasets using the enhanced OmniDataset framework for token regression tasks.\n",
    "   - **Step 2: Model Initialization**: You saw how to leverage pre-trained models and adapt them for multi-target token-level regression.\n",
    "   - **Step 3: Model Training**: You implemented robust training strategies using AccelerateTrainer with proper evaluation metrics and modern optimizations.\n",
    "   - **Step 4: Model Inference**: You used your fine-tuned model to make position-wise predictions and interpreted the biological significance of degradation patterns.\n",
    "\n",
    "3. **Advanced Capabilities**: You explored:\n",
    "   - Token-level regression for position-specific predictions\n",
    "   - Multi-target modeling for multiple degradation conditions\n",
    "   - Pattern analysis for understanding sequence-stability relationships\n",
    "   - Real-world applications in RNA engineering and therapeutic design\n",
    "\n",
    "### Next Steps and Applications\n",
    "\n",
    "Your trained model can now be applied to:\n",
    "- **mRNA Therapeutics**: Design stable mRNA vaccines and therapeutics\n",
    "- **Synthetic Biology**: Engineer RNA circuits with predictable degradation kinetics  \n",
    "- **Biotechnology**: Optimize protein expression systems\n",
    "- **Research**: Study sequence-structure-function relationships in RNA biology\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "Explore our other tutorials to expand your genomic AI toolkit:\n",
    "- **[Translation Efficiency Prediction](../translation_efficiency_prediction/)**: Predict protein production rates\n",
    "- **[RNA Secondary Structure Prediction](../rna_secondary_structure_prediction/)**: Model RNA folding patterns\n",
    "- **[Transcription Factor Binding](../tfb_prediction/)**: Understand gene regulation\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. The future of computational biology is in your hands!\n",
    "\n",
    "**Happy coding and discovering! ðŸ§¬âœ¨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
