# -*- coding: utf-8 -*-
# file: omnigenome_model.py
# time: 18:36 06/04/2024
# author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
# github: https://github.com/yangheng95
# huggingface: https://huggingface.co/yangheng
# google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
# Copyright (C) 2019-2024. All Rights Reserved.
import json
import multiprocessing
import os
import shutil
import warnings

import findfile
import torch

from transformers import AutoModel, AutoConfig, AutoTokenizer, BatchEncoding

from ..misc.utils import fprint, env_meta_info


from ..misc.utils import RNA2StructureCache

rna2structure = RNA2StructureCache()


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def last_hidden_state_forward(model, inputs, ss=None, tokenizer=None):
    """

    :param model: The model to extract the last hidden state from
    :param inputs: The inputs to the model
    :param ss: We can use the secondary structure information to help the model to learn better representations,
                  if ss is not None, the model will NOT use the secondary structure information. If ss is 'viennarna',
                    the model will use the secondary structure information generated by the ViennaRNA package. If ss is
                    'model', the model will use the secondary structure information generated by the model itself.
    :param tokenizer: Required while ss='viennarna'. The tokenizer to tokenize the secondary structure information
    :return: The last hidden state of the model and the secondary structure information if ss is not None
    """
    assert ss in [
        None,
        "viennarna",
        "model",
    ], f'ss should be one of [None, "viennarna", "model"], got {ss}'
    if isinstance(inputs, tuple):
        input_ids = inputs[0]
        attention_mask = inputs[1] if len(inputs) > 1 else None
    elif isinstance(inputs, BatchEncoding) or isinstance(inputs, dict):
        input_ids = inputs["input_ids"]
        attention_mask = (
            inputs["attention_mask"] if "attention_mask" in inputs else None
        )
    else:
        raise ValueError(
            f"The inputs should be a tuple, BatchEncoding or a dictionary-like object, got {type(inputs)}."
        )

    try:
        outputs = model(
            input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
        )

    except Exception as e:
        # For autoregressive models, the attention_mask is not required
        outputs = model(
            input_ids,
            output_hidden_states=True,
        )

    if not hasattr(outputs, "last_hidden_state"):
        warnings.warn(
            f"last_hidden_state not found in the outputs from the {model.__class__.__name__} model."
        )

    if hasattr(outputs, "last_hidden_state"):
        last_hidden_state = outputs.last_hidden_state
    elif hasattr(outputs, "hidden_states"):
        last_hidden_state = outputs.hidden_states[-1]
    elif (
        isinstance(outputs, list)
        or isinstance(outputs, tuple)
        or isinstance(outputs, torch.Tensor)
    ):
        # For some models like DNABERT-2, the outputs is a list, tuple of tensors
        last_hidden_state = outputs[-1] if len(outputs[-1].shape) == 3 else outputs[0]
    else:
        raise ValueError(
            f"Cannot find the last hidden state in the outputs from the {model.__class__.__name__} \
            model, please check the model architecture."
        )

    if ss == "viennarna":
        if hasattr(tokenizer, "base_tokenizer"):
            tokenizer = tokenizer.base_tokenizer
        sequences = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
        structures = rna2structure.fold([seq.replace(" ", "") for seq in sequences])
        tokenized_struct = tokenizer(
            structures,
            padding="max_length",
            max_length=input_ids.shape[1],
            truncation=True,
            return_tensors="pt",
            add_special_tokens=True,
        )
        tokenized_struct.to(input_ids.device)
        ss_last_hidden_state = model(
            **tokenized_struct,
            output_hidden_states=True,
        )["last_hidden_state"]
    elif ss == "model":
        raise NotImplementedError(
            "The model-based secondary structure information is not implemented yet."
        )
        # tokenized_struct = last_hidden_state.argmax(-1)
        # ss_last_hidden_state = model(
        #     tokenized_struct,
        #     attention_mask=attention_mask,
        #     output_hidden_states=True,
        # )
    else:
        return last_hidden_state

    return last_hidden_state, ss_last_hidden_state


class OmniGenomeModel(torch.nn.Module):
    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
        self.loss_fn = None
        if isinstance(config_or_model_model, str):
            base_model = AutoModel.from_pretrained(
                config_or_model_model, trust_remote_code=True
            )
            config = AutoConfig.from_pretrained(
                config_or_model_model, trust_remote_code=True
            )
        elif isinstance(config_or_model_model, torch.nn.Module):
            base_model = config_or_model_model
            config = base_model.config
        elif isinstance(config_or_model_model, AutoConfig):
            config = config_or_model_model
            base_model = AutoModel.from_config(config)
        else:
            raise ValueError(
                "The config_or_model_model should be either a string, a torch.nn.Module or a AutoConfig object."
            )
        label2id = kwargs.pop("label2id", None)
        trust_remote_code = kwargs.pop("trust_remote_code", True)
        num_labels = kwargs.pop("num_labels", None)

        # The metadata of the model
        self.metadata = env_meta_info()
        self.metadata["model_cls"] = self.__class__.__name__

        # do not change the order of the following lines
        super().__init__(*args, **kwargs)

        # The config of the model
        if label2id is not None:
            config.label2id = label2id
            config.id2label = {v: k for k, v in config.label2id.items()}
            config.num_labels = (
                len(config.label2id) if num_labels is None else num_labels
            )
        else:
            config.num_labels = num_labels
            warnings.warn(
                "The number of labels is not provided, the model will use the default value 1."
            )
        if hasattr(config, "n_embd"):
            config.hidden_size = config.n_embd
        elif hasattr(config, "d_model"):
            config.hidden_size = config.d_model
        elif hasattr(config, "hidden_size"):
            config.hidden_size = config.hidden_size
        else:
            raise RuntimeError(
                "The hidden size of the model is not found in the config."
            )

        # The tokenizer of the model
        self.tokenizer = tokenizer
        if hasattr(self.tokenizer, "base_tokenizer"):
            self.pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
        else:
            self.pad_token_id = self.tokenizer.pad_token_id

        self.dropout = torch.nn.Dropout(kwargs.get("dropout", 0.0))
        self.activation = torch.nn.Tanh()

        if isinstance(base_model, torch.nn.Module):
            self.model = base_model
        else:
            self.model = AutoModel.from_pretrained(
                base_model, trust_remote_code=trust_remote_code
            )

        # Update the config
        self.config = config
        self.model.config = config

        fprint(
            f"The trainable parameters of the {self.model.__class__.__name__} model are: {count_parameters(self.model) / 1e6:.2f} Millions"
        )

    def loss_function(self, logits, labels):
        raise NotImplementedError(
            "The loss_function() function should be implemented for your model."
        )

    def set_loss_fn(self, loss_function):
        self.loss_fn = loss_function

    def predict(self, inputs, **kwargs):
        raise NotImplementedError(
            "The predict() function should be implemented for your model."
        )

    def inference(self, inputs, **kwargs):
        raise NotImplementedError(
            "The inference() function should be implemented for your model."
        )

    def forward(self, inputs):
        last_hidden_state = last_hidden_state_forward(self.model, inputs)
        last_hidden_state = self.dropout(last_hidden_state)
        last_hidden_state = self.activation(last_hidden_state)
        outputs = {"last_hidden_state": last_hidden_state}
        return outputs

    def __call__(self, inputs, labels=None, *args, **kwargs):
        if isinstance(inputs, dict):
            labels = inputs.get("labels", None)
            label = inputs.get("label", None)
            labels = labels if labels is not None else label
            if labels is None:
                warnings.warn(
                    "No labels are provided in the inputs, the model will not calculate the loss."
                )
        elif isinstance(inputs, tuple):
            labels = inputs[1]
            inputs = inputs[0]
        elif labels is not None:
            labels = labels

        outputs = self.forward(inputs)

        if labels is not None:
            outputs["loss"] = self._calculate_loss(outputs, labels)
        else:
            outputs["loss"] = None
        return outputs

    def _calculate_loss(self, outputs, labels):
        loss = outputs.get("loss", None)
        if loss is not None:
            return outputs

        logits = outputs["logits"]
        if logits is not None or labels is not None:
            loss = self.loss_function(logits, labels)
            return loss
        else:
            raise RuntimeError(
                "The output of the forward() function should be a dictionary-like objective"
                " and have either 'loss', or 'logits' and 'labels' attribute."
            )

    def save(self, path, overwrite=False, dtype=torch.float16, **kwargs):
        self.eval()
        import dill

        if os.path.exists(path) and not overwrite:
            raise FileExistsError(
                f"The path {path} already exists, please set overwrite=True to overwrite it."
            )

        if not os.path.exists(path):
            os.makedirs(path)

        for file in findfile.find_files(
            self.config.name_or_path,
            and_key=[],
            exclude_key=["pytorch_model", "model", "safetensors"],
        ):
            shutil.copyfile(file, f"{path}/{os.path.basename(file)}")

        _device = self.model.device
        _dtype = self.model.dtype
        self.model.to(dtype).to("cpu")
        with open(f"{path}/tokenizer.pkl", "wb") as f:
            dill.dump(self.tokenizer, f)
        with open(f"{path}/metadata.json", "w", encoding="utf8") as f:
            json.dump(self.metadata, f)
        self.model.save_pretrained(
            f"{path}", safe_serialization=False
        )  # do not remove this line, used to save customed model scripts
        with open(f"{path}/pytorch_model.bin", "wb") as f:
            torch.save(self.state_dict(), f)

        self.model.to(_dtype).to(_device)
        fprint(f"The model is saved to {path}.")

    def load(self, path, **kwargs):
        with open(f"{path}/metadata.json", "r", encoding="utf8") as f:
            metadata = json.load(f)

        if metadata["model_cls"] != self.__class__.__name__:  # Check the model class
            raise ValueError(
                f"The model class in the loaded model is {metadata['model_cls']}, "
                f"but the current model class is {self.__class__.__name__}."
            )
        config = AutoConfig.from_pretrained(path, trust_remote_code=True, **kwargs)

        for key, value in config.__dict__.items():
            if key not in self.config.__dict__ or self.config.__dict__[key] != value:
                fprint(
                    f"Warning: The value of the key {key} in the loaded model is {value}, "
                    f"but the current value is {self.config.__dict__.get(key, None)}."
                )

        with open(f"{path}/pytorch_model.bin", "rb") as f:
            self.load_state_dict(
                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
            )
        return self

    def _is_causal_lm(self):
        if (
            hasattr(self.config, "architectures")
            and "CausalLM" in str(self.config.architectures)
        ) or (
            hasattr(self.config, "auto_map") and "CausalLM" in str(self.config.auto_map)
        ):
            return True
        else:
            return False

    @staticmethod
    def from_pretrained(model_name_or_path, tokenizer, *args, **kwargs):
        config = kwargs.pop("config", None)
        if config is None:
            config = AutoConfig.from_pretrained(model_name_or_path, **kwargs)
        base_model = AutoModel.from_pretrained(model_name_or_path, **kwargs)
        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(base_model, **kwargs)
        return OmniGenomeModel(config, base_model, tokenizer, *args, **kwargs)
